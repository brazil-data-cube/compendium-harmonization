{"config":{"indexing":"full","lang":["en","pt"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Evaluating Landsat-8 and Sentinel-2 Nadir BRDF Adjusted Reflectance (NBAR) on South of Brazil through a Reproducible and Replicable Workflow \u00b6 Essa \u00e9 a documenta\u00e7\u00e3o oficial do Research Compendium ( RC ), com todos os materiais (C\u00f3digos, dados, e ambientes computacionais) necess\u00e1rios para a reprodu\u00e7\u00e3o, replica\u00e7\u00e3o e avalia\u00e7\u00e3o dos resultados apresentados no artigo: Marujo et al (2022). Evaluating Landsat-8 and Sentinel-2 Nadir BRDF Adjusted Reflectance (NBAR) on South of Brazil through a Reproducible and Replicable Workflow . Paper ser\u00e1 submetido em Junho de 2022. Organiza\u00e7\u00e3o do Research Compendium \u00b6 A organiza\u00e7\u00e3o definida para este RC , visa facilitar a utiliza\u00e7\u00e3o dos c\u00f3digos implementados para a gera\u00e7\u00e3o dos resultados apresentados no artigo. Para isso, os c\u00f3digos de processamento s\u00e3o disponibilizados em uma estrutura de exemplos que permitem a execu\u00e7\u00e3o sem dificuldades, fazendo com que outros possam reproduzir e replicar o estudo realizado. Esses c\u00f3digos, s\u00e3o armazenados no diret\u00f3rio analysis , o qual possui tr\u00eas subdiret\u00f3rios: :file_folder: analysis/notebook : Diret\u00f3rio com a vers\u00e3o Jupyter Notebook do fluxo de processamento implementado no artigo associado a este RC . Para mais informa\u00e7\u00f5es, consulte a Se\u00e7\u00e3o de refer\u00eancia Scripts de processamento ; :file_folder: analysis/pipeline : Diret\u00f3rio com a vers\u00e3o Dagster do fluxo de processamento implementado no artigo associado a este RC . Para mais informa\u00e7\u00f5es, consulte a Se\u00e7\u00e3o de refer\u00eancia Scripts de processamento ; :file_folder: analysis/data : Diret\u00f3rio para armazenar os dados de entrada e sa\u00edda gerados. Cont\u00e9m os seguintes subdiret\u00f3rios: :file_folder: examples : Diret\u00f3rio com os dados (Entrada/Sa\u00edda) dos exemplos disponibilizados neste RC . Ao todo, s\u00e3o dois exemplos. Para mais informa\u00e7\u00f5es sobre os exemplos, consulte o Cap\u00edtulo Processamento de dados ; :file_folder: original_scene_ids : Diret\u00f3rio para armazenar os arquivos de \u00edndices de cenas originais utilizados na produ\u00e7\u00e3o dos resultados dos artigos. Esses dados podem ser aplicados nos c\u00f3digos disponibilizados nos diret\u00f3rios analysis/notebook e analysis/pipeline para a reprodu\u00e7\u00e3o do resultados do artigo. Por padr\u00e3o, os dados de entrada, por conta do tamanho dos arquivos, n\u00e3o s\u00e3o armazenados diretamente no diret\u00f3rio de dados ( analysis/data/ ). Ao contr\u00e1rio disso, como \u00e9 descrito em detalhes na Se\u00e7\u00e3o de refer\u00eancia Scripts auxiliares , eles s\u00e3o disponibilizados no GitHub Release Assets do reposit\u00f3rio do RC . Para a constru\u00e7\u00e3o dos scripts de processamento dispon\u00edveis no diret\u00f3rio analysis , fez-se a cria\u00e7\u00e3o de diversas bibliotecas de software e scripts auxiliares . O c\u00f3digo fonte de parte dessas ferramentas fica dispon\u00edvel no diret\u00f3rio tools deste RC . Nesse diret\u00f3rio, tem-se quatro subdiret\u00f3rios, sendo eles: :file_folder: tools/auxiliary-library : C\u00f3digo fonte da biblioteca research-processing , a qual fornece as opera\u00e7\u00f5es de alto n\u00edvel para o processamento de dados deste RC ; :file_folder: tools/calculate-checksum : C\u00f3digo fonte do script calculate-checksum , criado para calcular o checksum dos arquivos deste RC antes do compartilhamento; :file_folder: tools/example-toolkit : C\u00f3digo fonte do script example-toolkit , criado para facilitar o download e valida\u00e7\u00e3o dos dados de exemplo do GitHub Release Assets; :file_folder: tools/github-asset-upload : C\u00f3digo fonte do script github-asset-upload , criado para facilitar o upload dos dados de exemplo para o GitHub Release Assets. Outro diret\u00f3rio dispon\u00edvel neste RC \u00e9 o composes . Nesse diret\u00f3rio, est\u00e3o arquivos de configura\u00e7\u00e3o do Docker Compose dos ambientes computacionais necess\u00e1rios para a execu\u00e7\u00e3o dos exemplos dispon\u00edveis neste RC . Para mais informa\u00e7\u00f5es sobre os ambientes computacionais do RC , consulte a Se\u00e7\u00e3o de refer\u00eancia Ambientes computacionais . No diret\u00f3rio composes , tem-se dois subdiret\u00f3rios: :file_folder: composes/minimal : Diret\u00f3rio com os Docker Composes para a execu\u00e7\u00e3o do Exemplo m\u00ednimo fornecido neste RC ; :file_folder: composes/replication : Diret\u00f3rio com os Docker Composes para a execu\u00e7\u00e3o do Exemplo de replica\u00e7\u00e3o fornecido neste RC . Para mais informa\u00e7\u00f5es sobre os exemplos, consulte a Se\u00e7\u00e3o Processamento de dados . De forma complementar ao diret\u00f3rio composes , tem-se o diret\u00f3rio docker . Nesse diret\u00f3rio, est\u00e3o armazenados os arquivos Dockerfile utilizados para a constru\u00e7\u00e3o dos ambientes utilizados nos Docker Composes. Esse diret\u00f3rio possui dois subdiret\u00f3rios: :file_folder: docker/notebook : Diret\u00f3rio com os Dockerfile do ambiente necess\u00e1rio para a execu\u00e7\u00e3o da vers\u00e3o Jupyter Notebook do fluxo de processamento deste RC . :file_folder: docker/pipeline : Diret\u00f3rio com os Dockerfile do ambiente necess\u00e1rio para a execu\u00e7\u00e3o da vers\u00e3o Dagster do fluxo de processamento deste RC . Al\u00e9m desses diret\u00f3rios, h\u00e1 tamb\u00e9m alguns arquivos fundamentais para o uso dos materiais deste RC , s\u00e3o eles: Vagrantfile e bootstrap.sh : Arquivos Vagrant utilizados para a constru\u00e7\u00e3o de uma m\u00e1quina virtual com o ambiente completo para a execu\u00e7\u00e3o dos Scripts de processamento dispon\u00edveis no diret\u00f3rio analysis . Para mais informa\u00e7\u00f5es, consulte a Se\u00e7\u00e3o de refer\u00eancia Ambientes computacionais - M\u00e1quina virtual com Vagrant ; Makefile : Arquivo de defini\u00e7\u00e3o GNU Make que facilita a utiliza\u00e7\u00e3o dos materiais dispon\u00edveis nos diret\u00f3rios analysis e composes . O arquivo setenv.sh \u00e9 utilizado pelo Makefile para a defini\u00e7\u00e3o do usu\u00e1rio que far\u00e1 a execu\u00e7\u00e3o do Jupyter Notebook. Mais informa\u00e7\u00f5es s\u00e3o fornecidos na Se\u00e7\u00e3o Processamento de dados .","title":"Home"},{"location":"#evaluating-landsat-8-and-sentinel-2-nadir-brdf-adjusted-reflectance-nbar-on-south-of-brazil-through-a-reproducible-and-replicable-workflow","text":"Essa \u00e9 a documenta\u00e7\u00e3o oficial do Research Compendium ( RC ), com todos os materiais (C\u00f3digos, dados, e ambientes computacionais) necess\u00e1rios para a reprodu\u00e7\u00e3o, replica\u00e7\u00e3o e avalia\u00e7\u00e3o dos resultados apresentados no artigo: Marujo et al (2022). Evaluating Landsat-8 and Sentinel-2 Nadir BRDF Adjusted Reflectance (NBAR) on South of Brazil through a Reproducible and Replicable Workflow . Paper ser\u00e1 submetido em Junho de 2022.","title":"Evaluating Landsat-8 and Sentinel-2 Nadir BRDF Adjusted Reflectance (NBAR) on South of Brazil through a Reproducible and Replicable Workflow"},{"location":"#organizacao-do-research-compendium","text":"A organiza\u00e7\u00e3o definida para este RC , visa facilitar a utiliza\u00e7\u00e3o dos c\u00f3digos implementados para a gera\u00e7\u00e3o dos resultados apresentados no artigo. Para isso, os c\u00f3digos de processamento s\u00e3o disponibilizados em uma estrutura de exemplos que permitem a execu\u00e7\u00e3o sem dificuldades, fazendo com que outros possam reproduzir e replicar o estudo realizado. Esses c\u00f3digos, s\u00e3o armazenados no diret\u00f3rio analysis , o qual possui tr\u00eas subdiret\u00f3rios: :file_folder: analysis/notebook : Diret\u00f3rio com a vers\u00e3o Jupyter Notebook do fluxo de processamento implementado no artigo associado a este RC . Para mais informa\u00e7\u00f5es, consulte a Se\u00e7\u00e3o de refer\u00eancia Scripts de processamento ; :file_folder: analysis/pipeline : Diret\u00f3rio com a vers\u00e3o Dagster do fluxo de processamento implementado no artigo associado a este RC . Para mais informa\u00e7\u00f5es, consulte a Se\u00e7\u00e3o de refer\u00eancia Scripts de processamento ; :file_folder: analysis/data : Diret\u00f3rio para armazenar os dados de entrada e sa\u00edda gerados. Cont\u00e9m os seguintes subdiret\u00f3rios: :file_folder: examples : Diret\u00f3rio com os dados (Entrada/Sa\u00edda) dos exemplos disponibilizados neste RC . Ao todo, s\u00e3o dois exemplos. Para mais informa\u00e7\u00f5es sobre os exemplos, consulte o Cap\u00edtulo Processamento de dados ; :file_folder: original_scene_ids : Diret\u00f3rio para armazenar os arquivos de \u00edndices de cenas originais utilizados na produ\u00e7\u00e3o dos resultados dos artigos. Esses dados podem ser aplicados nos c\u00f3digos disponibilizados nos diret\u00f3rios analysis/notebook e analysis/pipeline para a reprodu\u00e7\u00e3o do resultados do artigo. Por padr\u00e3o, os dados de entrada, por conta do tamanho dos arquivos, n\u00e3o s\u00e3o armazenados diretamente no diret\u00f3rio de dados ( analysis/data/ ). Ao contr\u00e1rio disso, como \u00e9 descrito em detalhes na Se\u00e7\u00e3o de refer\u00eancia Scripts auxiliares , eles s\u00e3o disponibilizados no GitHub Release Assets do reposit\u00f3rio do RC . Para a constru\u00e7\u00e3o dos scripts de processamento dispon\u00edveis no diret\u00f3rio analysis , fez-se a cria\u00e7\u00e3o de diversas bibliotecas de software e scripts auxiliares . O c\u00f3digo fonte de parte dessas ferramentas fica dispon\u00edvel no diret\u00f3rio tools deste RC . Nesse diret\u00f3rio, tem-se quatro subdiret\u00f3rios, sendo eles: :file_folder: tools/auxiliary-library : C\u00f3digo fonte da biblioteca research-processing , a qual fornece as opera\u00e7\u00f5es de alto n\u00edvel para o processamento de dados deste RC ; :file_folder: tools/calculate-checksum : C\u00f3digo fonte do script calculate-checksum , criado para calcular o checksum dos arquivos deste RC antes do compartilhamento; :file_folder: tools/example-toolkit : C\u00f3digo fonte do script example-toolkit , criado para facilitar o download e valida\u00e7\u00e3o dos dados de exemplo do GitHub Release Assets; :file_folder: tools/github-asset-upload : C\u00f3digo fonte do script github-asset-upload , criado para facilitar o upload dos dados de exemplo para o GitHub Release Assets. Outro diret\u00f3rio dispon\u00edvel neste RC \u00e9 o composes . Nesse diret\u00f3rio, est\u00e3o arquivos de configura\u00e7\u00e3o do Docker Compose dos ambientes computacionais necess\u00e1rios para a execu\u00e7\u00e3o dos exemplos dispon\u00edveis neste RC . Para mais informa\u00e7\u00f5es sobre os ambientes computacionais do RC , consulte a Se\u00e7\u00e3o de refer\u00eancia Ambientes computacionais . No diret\u00f3rio composes , tem-se dois subdiret\u00f3rios: :file_folder: composes/minimal : Diret\u00f3rio com os Docker Composes para a execu\u00e7\u00e3o do Exemplo m\u00ednimo fornecido neste RC ; :file_folder: composes/replication : Diret\u00f3rio com os Docker Composes para a execu\u00e7\u00e3o do Exemplo de replica\u00e7\u00e3o fornecido neste RC . Para mais informa\u00e7\u00f5es sobre os exemplos, consulte a Se\u00e7\u00e3o Processamento de dados . De forma complementar ao diret\u00f3rio composes , tem-se o diret\u00f3rio docker . Nesse diret\u00f3rio, est\u00e3o armazenados os arquivos Dockerfile utilizados para a constru\u00e7\u00e3o dos ambientes utilizados nos Docker Composes. Esse diret\u00f3rio possui dois subdiret\u00f3rios: :file_folder: docker/notebook : Diret\u00f3rio com os Dockerfile do ambiente necess\u00e1rio para a execu\u00e7\u00e3o da vers\u00e3o Jupyter Notebook do fluxo de processamento deste RC . :file_folder: docker/pipeline : Diret\u00f3rio com os Dockerfile do ambiente necess\u00e1rio para a execu\u00e7\u00e3o da vers\u00e3o Dagster do fluxo de processamento deste RC . Al\u00e9m desses diret\u00f3rios, h\u00e1 tamb\u00e9m alguns arquivos fundamentais para o uso dos materiais deste RC , s\u00e3o eles: Vagrantfile e bootstrap.sh : Arquivos Vagrant utilizados para a constru\u00e7\u00e3o de uma m\u00e1quina virtual com o ambiente completo para a execu\u00e7\u00e3o dos Scripts de processamento dispon\u00edveis no diret\u00f3rio analysis . Para mais informa\u00e7\u00f5es, consulte a Se\u00e7\u00e3o de refer\u00eancia Ambientes computacionais - M\u00e1quina virtual com Vagrant ; Makefile : Arquivo de defini\u00e7\u00e3o GNU Make que facilita a utiliza\u00e7\u00e3o dos materiais dispon\u00edveis nos diret\u00f3rios analysis e composes . O arquivo setenv.sh \u00e9 utilizado pelo Makefile para a defini\u00e7\u00e3o do usu\u00e1rio que far\u00e1 a execu\u00e7\u00e3o do Jupyter Notebook. Mais informa\u00e7\u00f5es s\u00e3o fornecidos na Se\u00e7\u00e3o Processamento de dados .","title":"Organiza\u00e7\u00e3o do Research Compendium"},{"location":"reproducible-research/","text":"Processamento de dados \u00b6 Info Este \u00e9 um Cap\u00edtulo de aplica\u00e7\u00e3o do material dispon\u00edvel neste RC . Caso voc\u00ea deseje explorar os conceitos por tr\u00e1s dos materiais e as formas de uso dos mesmos, por favor, consulte o Cap\u00edtulo de Refer\u00eancia do Compendium . A produ\u00e7\u00e3o dos resultados apresentados no artigo associado a este RC , fez o uso de uma grande cole\u00e7\u00e3o de dados de Observa\u00e7\u00e3o da terra ( ~2TB ). Com esse grande volume de dados, atividades de gerenciamento e processamento desses dados pode exigir recursos que nem sempre est\u00e3o dispon\u00edveis aos pesquisadores e interessados. De forma direta e indireta, isso faz com que a\u00e7\u00f5es de verifica\u00e7\u00e3o , valida\u00e7\u00e3o , reprodu\u00e7\u00e3o e replica\u00e7\u00e3o dos materiais disponibilizados sejam problem\u00e1ticas e trabalhosas de serem realizadas. Para resolver esse problema e permitir que todos possam explorar os materiais que desenvolvemos, de modo a entender a implementa\u00e7\u00e3o, organiza\u00e7\u00e3o e t\u00e9cnicas utilizadas, fez-se a cria\u00e7\u00e3o de exemplos de utiliza\u00e7\u00e3o. Nesses exemplos, todos os materiais disponibilizados no RC s\u00e3o utilizados em conjunto para a constru\u00e7\u00e3o do fluxo de processamento do trabalho. Os dados utilizados nesses exemplos, s\u00e3o subsets do conjunto de dados original utilizado na produ\u00e7\u00e3o dos resultados do artigo. Tamb\u00e9m s\u00e3o utilizados conjuntos de dados n\u00e3o aplicados no artigo, para testar a possibilidade de replica\u00e7\u00e3o dos materiais deste RC . Os exemplos dispon\u00edveis, s\u00e3o generaliz\u00e1veis e podem ser utilizados como base para a reprodu\u00e7\u00e3o do artigo e tamb\u00e9m a replica\u00e7\u00e3o , sendo necess\u00e1rio para essas a\u00e7\u00f5es, a troca do conjunto de dados de entrada. Exemplos generaliz\u00e1veis e customiz\u00e1veis Essas caracter\u00edsticas foram atribu\u00eddas aos exemplos disponibilizados j\u00e1 que, durante o desenvolvimento do artigo, eles foram os primeiros componentes a serem desenvolvidos. Em seguida, com as ferramentas prontas e testadas, fez-se a gera\u00e7\u00e3o dos resultados finais. Para essa gera\u00e7\u00e3o dos resultados finais, a \u00fanica a\u00e7\u00e3o necess\u00e1ria foi a troca do conjunto de dados de entrada dos exemplos para o conjunto completo de dados do artigo. Com isso, \u00e9 poss\u00edvel dizer que os exemplos s\u00e3o generaliz\u00e1beis em seu uso e customiz\u00e1veis o suficiente para permitir que outros conjuntos de dados e regi\u00f5es sejam processados. Fluxo de opera\u00e7\u00e3o dos exemplos \u00b6 Ao todo, est\u00e3o dispon\u00edveis dois exemplos com o uso dos materiais deste RC : Exemplo m\u00ednimo Exemplo de processamento completo, com todas as etapas da metodologia apresentada no artigo. Para esse exemplo, faz-se o uso de um pequeno subset de dados, retirados do conjunto de dados completo utilizado no artigo. Exemplo de replica\u00e7\u00e3o Seguindo exatamente os mesmos passos dispon\u00edveis no Exemplo m\u00ednimo , neste exemplo, faz-se o uso de um conjunto de dados de uma regi\u00e3o n\u00e3o trabalhada no artigo original, para mostrar a possibilidade de replica\u00e7\u00e3o do fluxo de processamento implementado. A realiza\u00e7\u00e3o de ambos os exemplos \u00e9 feita seguindo os mesmos passos. Esses, conforme ilustrado na figura abaixo s\u00e3o divididos em tr\u00eas partes: Download dos dados ; Processamento (Jupyter) ; Processamento (Dagster) Fluxo de opera\u00e7\u00e3o dos exemplos No Download de dados , faz-se o download dos dados necess\u00e1rio para a execu\u00e7\u00e3o do exemplo. Esses dados s\u00e3o disponibilizados atrav\u00e9s do GitHub Release Assets . Em seguida, com os dados dispon\u00edveis, eles podem ser utilizados como entrada para o fluxo de processamento. Esse fluxo de processamento, est\u00e1 implementado em duas tecnologias diferentes , Jupyter Notebook e Dagster . Qualquer uma das implementa\u00e7\u00f5es leva aos mesmos resultados. A diferen\u00e7a est\u00e1 no tipo de ambiente disponibilizado pelas ferramentas. Com o Jupyter Notebook, tem-se uma experi\u00eancia interativa na execu\u00e7\u00e3o das etapas de processamento, enquanto que no Dagster a opera\u00e7\u00e3o ocorre em modo batch . A realiza\u00e7\u00e3o dessas etapas, conforme ser\u00e1 mostrado nas se\u00e7\u00f5es a seguir, \u00e9 feita atrav\u00e9s de Docker Composes. Cada parte apresentada possui um Docker Compose espec\u00edfico, para que, essas, sejam realizadas de forma independente e num ambiente isolado. Neste caso, apenas os dados s\u00e3o compartilhado entre as partes. Automa\u00e7\u00e3o \u00b6 Conforme apresentado na Subse\u00e7\u00e3o anterior, para cada etapa realizada nos exemplos, tem-se um Docker Compose exclusivo. No entanto, ao utilizar essa estrat\u00e9gia, est\u00e1 sendo controlado apenas a recria\u00e7\u00e3o dos ambientes e as execu\u00e7\u00f5es. O uso desses Composes ainda \u00e9 algo que deve ser realizado pelos usu\u00e1rios. A realiza\u00e7\u00e3o dessas execu\u00e7\u00f5es pelo usu\u00e1rio n\u00e3o \u00e9 um problema. Pode ser feito sem grandes dificuldades. No entanto, quando testes est\u00e3o sendo feitos, tem-se opera\u00e7\u00f5es de configura\u00e7\u00e3o de cada etapa que podem ser muito repetitivas. Para evitar essas repeti\u00e7\u00f5es e os poss\u00edveis erros que essa a\u00e7\u00e3o pode causar, toda a l\u00f3gica necess\u00e1ria para a execu\u00e7\u00e3o de cada etapa, atrav\u00e9s do Docker Compose, foi colocada em um Makefile . Com esse Makefile , os conjuntos de comandos necess\u00e1rios para a execu\u00e7\u00e3o de cada etapa s\u00e3o abstra\u00eddos a simples execu\u00e7\u00f5es do GNU Make . Make e pesquisa reprodut\u00edvel A ideia da utiliza\u00e7\u00e3o do Make foi retirada do magn\u00edfico The Turing Way handbook to reproducible, ethical and collaborative data science . Para mais sobre o GNU Make e pesquisa reprodut\u00edvel, consulte o material Reproducibility with Make . Ao fazer o uso do GNU Make , conforme ilustrado na Figura abaixo, a intera\u00e7\u00e3o com o Docker Compose e poss\u00edveis configura\u00e7\u00f5es necess\u00e1rias nessas etapas, s\u00e3o realizadas atrav\u00e9s de c\u00f3digos prontos e testados. Com isso, evitamos que erros nos comandos impessam o uso de nosso material. Al\u00e9m disso, por se tratar de um documento de texto simples, para aqueles que desejam obter cada etapa realizada, basta abrir o arquivo e verificar o que \u00e9 realizado. Fluxo de opera\u00e7\u00e3o dos exemplos com Make Sendo assim, nos exemplos, ser\u00e1 feito o uso do GNU Make junto ao Makefile para que as etapas de configura\u00e7\u00e3o sejam automatizadas, tornando o uso dos materiais mais simples e diretos. Refer\u00eancia dos comandos dispon\u00edveis no Makefile \u00b6 Para facilitar a utiliza\u00e7\u00e3o dos comandos dispon\u00edveis no Makefile , essa Subse\u00e7\u00e3o possui uma refer\u00eancia para cada um dos comandos dispon\u00edveis. Esses comandos, est\u00e3o divididos em dois grupos: Example e Replication . Os comandos Example , s\u00e3o aqueles que facilitam as opera\u00e7\u00f5es para o Exemplo m\u00ednimo . J\u00e1 os comandos Replication facilitam o exemplo de replica\u00e7\u00e3o. Desses grupos, deve-se notar que eles executam os mesmos comandos, os mesmos ambientes, trocando apenas os dados de entrada. Sendo assim, caso voc\u00ea deseje adaptar os c\u00f3digos para seus dados, esses comados podem ser utilizados. Alternativamente, os arquivos Docker Compose utilizados pelo Makefile tamb\u00e9m est\u00e3o dispon\u00edveis e podem ser modificados. Nos t\u00f3picos abaixo, ser\u00e3o apresentados os comandos que est\u00e3o dispon\u00edveis para cada um desses grupos: Example Comando Descri\u00e7\u00e3o example_cleanup_data Remove todos os dados (Entrada e sa\u00edda) utilizados no Exemplo m\u00ednimo example_download_data Realiza o download dos dados utilizados no Exemplo m\u00ednimo. example_pipeline Cria Container para a execu\u00e7\u00e3o do Exemplo m\u00ednimo com Dagster example_notebook Cria Container para execu\u00e7\u00e3o do Exemplo m\u00ednimo com Jupyter Notebook Replication Comando Descri\u00e7\u00e3o replication_cleanup_data Remove todos os dados (Entrada e sa\u00edda) utilizados no Exemplo de replica\u00e7\u00e3o replication_ download_data Realiza o download dos dados utilizados no Exemplo de replica\u00e7\u00e3o replication_pipeline Cria Container para a execu\u00e7\u00e3o do Exemplo de replica\u00e7\u00e3o com Dagster replication_notebook Cria Container para execu\u00e7\u00e3o do Exemplo de replica\u00e7\u00e3o com Jupyter Notebook Como pode-se notar, os comandos para ambos os exemplos s\u00e3o os mesmos. A mudan\u00e7a ocorre apenas na nomenclatura. Com rela\u00e7\u00e3o ao funcionamento, como mencionado anteriormente, as opera\u00e7\u00f5es s\u00e3o as mesmas, variando apenas nos dados de entrada que s\u00e3o utilizados. Pr\u00e9-requisitos \u00b6 Antes de iniciar os exemplos, certique-se que de que possui todas as ferramentas necess\u00e1rias instaladas e configuradas em seu ambiente de trabalho. Abaixo, essas ferramentas s\u00e3o listadas e descritas: Git Sistema de controle de vers\u00f5es (Documenta\u00e7\u00e3o criada com git vers\u00e3o 2.25.1 . Vers\u00f5es posteriores devem suportar os comandos utilizados); Docker Software de virtualiza\u00e7\u00e3o baseado em Containers (Documenta\u00e7\u00e3o criada com Docker vers\u00e3o 0.10.12 . Vers\u00f5es posteriores devem suportar os comandos utilizados); Docker Compose Ferramenta para a orquestra\u00e7\u00e3o e gerenciamento de Docker Containers (Documenta\u00e7\u00e3o criada com Docker Compose vers\u00e3o 1.29.2 . Vers\u00f5es posteriores devem suportar os comandos utilizados). GNU Make Ferramenta de automatiza\u00e7\u00e3o e controle de fluxos de build e execu\u00e7\u00e3o (Documenta\u00e7\u00e3o criada com GNU Make vers\u00e3o 4.2.1 . Vers\u00f5es posteriores devem suportar os comandos utilizados). Caso voc\u00ea n\u00e3o tenha alguma dessas ferramentas, utilize os links acima para acessar as respectivas documenta\u00e7\u00f5es oficiais e realizar as instala\u00e7\u00f5es necess\u00e1rias. Sistema operacional O sistema operacional utilizado para a cria\u00e7\u00e3o de todos os materiais deste RC , foi o Ubuntu 20.04. Assim, \u00e9 esperado que os passos apresentados, possam ser realizados em outras distros ou sistemas equivalentes (e.g., MacOS , FreeBSD , openSUSE ). Para Windows, pode ser que adapta\u00e7\u00f5es sejam necess\u00e1rias. Caso voc\u00ea esteja utilizando o Windows e n\u00e3o deseja fazer modifica\u00e7\u00f5es para utilizar o conte\u00fado deste RC , n\u00f3s disponibilizados uma M\u00e1quina Virtual que pode ser utilizada. Para saber mais, por favor, consulte a Se\u00e7\u00e3o Ambientes computacionais - M\u00e1quina virtual com Vagrant . Ap\u00f3s a instala\u00e7\u00e3o e configura\u00e7\u00e3o de todas as ferramentas listadas acima, voc\u00ea est\u00e1 pronto para iniciar os exemplos.","title":"Introdu\u00e7\u00e3o"},{"location":"reproducible-research/#processamento-de-dados","text":"Info Este \u00e9 um Cap\u00edtulo de aplica\u00e7\u00e3o do material dispon\u00edvel neste RC . Caso voc\u00ea deseje explorar os conceitos por tr\u00e1s dos materiais e as formas de uso dos mesmos, por favor, consulte o Cap\u00edtulo de Refer\u00eancia do Compendium . A produ\u00e7\u00e3o dos resultados apresentados no artigo associado a este RC , fez o uso de uma grande cole\u00e7\u00e3o de dados de Observa\u00e7\u00e3o da terra ( ~2TB ). Com esse grande volume de dados, atividades de gerenciamento e processamento desses dados pode exigir recursos que nem sempre est\u00e3o dispon\u00edveis aos pesquisadores e interessados. De forma direta e indireta, isso faz com que a\u00e7\u00f5es de verifica\u00e7\u00e3o , valida\u00e7\u00e3o , reprodu\u00e7\u00e3o e replica\u00e7\u00e3o dos materiais disponibilizados sejam problem\u00e1ticas e trabalhosas de serem realizadas. Para resolver esse problema e permitir que todos possam explorar os materiais que desenvolvemos, de modo a entender a implementa\u00e7\u00e3o, organiza\u00e7\u00e3o e t\u00e9cnicas utilizadas, fez-se a cria\u00e7\u00e3o de exemplos de utiliza\u00e7\u00e3o. Nesses exemplos, todos os materiais disponibilizados no RC s\u00e3o utilizados em conjunto para a constru\u00e7\u00e3o do fluxo de processamento do trabalho. Os dados utilizados nesses exemplos, s\u00e3o subsets do conjunto de dados original utilizado na produ\u00e7\u00e3o dos resultados do artigo. Tamb\u00e9m s\u00e3o utilizados conjuntos de dados n\u00e3o aplicados no artigo, para testar a possibilidade de replica\u00e7\u00e3o dos materiais deste RC . Os exemplos dispon\u00edveis, s\u00e3o generaliz\u00e1veis e podem ser utilizados como base para a reprodu\u00e7\u00e3o do artigo e tamb\u00e9m a replica\u00e7\u00e3o , sendo necess\u00e1rio para essas a\u00e7\u00f5es, a troca do conjunto de dados de entrada. Exemplos generaliz\u00e1veis e customiz\u00e1veis Essas caracter\u00edsticas foram atribu\u00eddas aos exemplos disponibilizados j\u00e1 que, durante o desenvolvimento do artigo, eles foram os primeiros componentes a serem desenvolvidos. Em seguida, com as ferramentas prontas e testadas, fez-se a gera\u00e7\u00e3o dos resultados finais. Para essa gera\u00e7\u00e3o dos resultados finais, a \u00fanica a\u00e7\u00e3o necess\u00e1ria foi a troca do conjunto de dados de entrada dos exemplos para o conjunto completo de dados do artigo. Com isso, \u00e9 poss\u00edvel dizer que os exemplos s\u00e3o generaliz\u00e1beis em seu uso e customiz\u00e1veis o suficiente para permitir que outros conjuntos de dados e regi\u00f5es sejam processados.","title":"Processamento de dados"},{"location":"reproducible-research/#fluxo-de-operacao-dos-exemplos","text":"Ao todo, est\u00e3o dispon\u00edveis dois exemplos com o uso dos materiais deste RC : Exemplo m\u00ednimo Exemplo de processamento completo, com todas as etapas da metodologia apresentada no artigo. Para esse exemplo, faz-se o uso de um pequeno subset de dados, retirados do conjunto de dados completo utilizado no artigo. Exemplo de replica\u00e7\u00e3o Seguindo exatamente os mesmos passos dispon\u00edveis no Exemplo m\u00ednimo , neste exemplo, faz-se o uso de um conjunto de dados de uma regi\u00e3o n\u00e3o trabalhada no artigo original, para mostrar a possibilidade de replica\u00e7\u00e3o do fluxo de processamento implementado. A realiza\u00e7\u00e3o de ambos os exemplos \u00e9 feita seguindo os mesmos passos. Esses, conforme ilustrado na figura abaixo s\u00e3o divididos em tr\u00eas partes: Download dos dados ; Processamento (Jupyter) ; Processamento (Dagster) Fluxo de opera\u00e7\u00e3o dos exemplos No Download de dados , faz-se o download dos dados necess\u00e1rio para a execu\u00e7\u00e3o do exemplo. Esses dados s\u00e3o disponibilizados atrav\u00e9s do GitHub Release Assets . Em seguida, com os dados dispon\u00edveis, eles podem ser utilizados como entrada para o fluxo de processamento. Esse fluxo de processamento, est\u00e1 implementado em duas tecnologias diferentes , Jupyter Notebook e Dagster . Qualquer uma das implementa\u00e7\u00f5es leva aos mesmos resultados. A diferen\u00e7a est\u00e1 no tipo de ambiente disponibilizado pelas ferramentas. Com o Jupyter Notebook, tem-se uma experi\u00eancia interativa na execu\u00e7\u00e3o das etapas de processamento, enquanto que no Dagster a opera\u00e7\u00e3o ocorre em modo batch . A realiza\u00e7\u00e3o dessas etapas, conforme ser\u00e1 mostrado nas se\u00e7\u00f5es a seguir, \u00e9 feita atrav\u00e9s de Docker Composes. Cada parte apresentada possui um Docker Compose espec\u00edfico, para que, essas, sejam realizadas de forma independente e num ambiente isolado. Neste caso, apenas os dados s\u00e3o compartilhado entre as partes.","title":"Fluxo de opera\u00e7\u00e3o dos exemplos"},{"location":"reproducible-research/#automacao","text":"Conforme apresentado na Subse\u00e7\u00e3o anterior, para cada etapa realizada nos exemplos, tem-se um Docker Compose exclusivo. No entanto, ao utilizar essa estrat\u00e9gia, est\u00e1 sendo controlado apenas a recria\u00e7\u00e3o dos ambientes e as execu\u00e7\u00f5es. O uso desses Composes ainda \u00e9 algo que deve ser realizado pelos usu\u00e1rios. A realiza\u00e7\u00e3o dessas execu\u00e7\u00f5es pelo usu\u00e1rio n\u00e3o \u00e9 um problema. Pode ser feito sem grandes dificuldades. No entanto, quando testes est\u00e3o sendo feitos, tem-se opera\u00e7\u00f5es de configura\u00e7\u00e3o de cada etapa que podem ser muito repetitivas. Para evitar essas repeti\u00e7\u00f5es e os poss\u00edveis erros que essa a\u00e7\u00e3o pode causar, toda a l\u00f3gica necess\u00e1ria para a execu\u00e7\u00e3o de cada etapa, atrav\u00e9s do Docker Compose, foi colocada em um Makefile . Com esse Makefile , os conjuntos de comandos necess\u00e1rios para a execu\u00e7\u00e3o de cada etapa s\u00e3o abstra\u00eddos a simples execu\u00e7\u00f5es do GNU Make . Make e pesquisa reprodut\u00edvel A ideia da utiliza\u00e7\u00e3o do Make foi retirada do magn\u00edfico The Turing Way handbook to reproducible, ethical and collaborative data science . Para mais sobre o GNU Make e pesquisa reprodut\u00edvel, consulte o material Reproducibility with Make . Ao fazer o uso do GNU Make , conforme ilustrado na Figura abaixo, a intera\u00e7\u00e3o com o Docker Compose e poss\u00edveis configura\u00e7\u00f5es necess\u00e1rias nessas etapas, s\u00e3o realizadas atrav\u00e9s de c\u00f3digos prontos e testados. Com isso, evitamos que erros nos comandos impessam o uso de nosso material. Al\u00e9m disso, por se tratar de um documento de texto simples, para aqueles que desejam obter cada etapa realizada, basta abrir o arquivo e verificar o que \u00e9 realizado. Fluxo de opera\u00e7\u00e3o dos exemplos com Make Sendo assim, nos exemplos, ser\u00e1 feito o uso do GNU Make junto ao Makefile para que as etapas de configura\u00e7\u00e3o sejam automatizadas, tornando o uso dos materiais mais simples e diretos.","title":"Automa\u00e7\u00e3o"},{"location":"reproducible-research/#referencia-dos-comandos-disponiveis-no-makefile","text":"Para facilitar a utiliza\u00e7\u00e3o dos comandos dispon\u00edveis no Makefile , essa Subse\u00e7\u00e3o possui uma refer\u00eancia para cada um dos comandos dispon\u00edveis. Esses comandos, est\u00e3o divididos em dois grupos: Example e Replication . Os comandos Example , s\u00e3o aqueles que facilitam as opera\u00e7\u00f5es para o Exemplo m\u00ednimo . J\u00e1 os comandos Replication facilitam o exemplo de replica\u00e7\u00e3o. Desses grupos, deve-se notar que eles executam os mesmos comandos, os mesmos ambientes, trocando apenas os dados de entrada. Sendo assim, caso voc\u00ea deseje adaptar os c\u00f3digos para seus dados, esses comados podem ser utilizados. Alternativamente, os arquivos Docker Compose utilizados pelo Makefile tamb\u00e9m est\u00e3o dispon\u00edveis e podem ser modificados. Nos t\u00f3picos abaixo, ser\u00e3o apresentados os comandos que est\u00e3o dispon\u00edveis para cada um desses grupos: Example Comando Descri\u00e7\u00e3o example_cleanup_data Remove todos os dados (Entrada e sa\u00edda) utilizados no Exemplo m\u00ednimo example_download_data Realiza o download dos dados utilizados no Exemplo m\u00ednimo. example_pipeline Cria Container para a execu\u00e7\u00e3o do Exemplo m\u00ednimo com Dagster example_notebook Cria Container para execu\u00e7\u00e3o do Exemplo m\u00ednimo com Jupyter Notebook Replication Comando Descri\u00e7\u00e3o replication_cleanup_data Remove todos os dados (Entrada e sa\u00edda) utilizados no Exemplo de replica\u00e7\u00e3o replication_ download_data Realiza o download dos dados utilizados no Exemplo de replica\u00e7\u00e3o replication_pipeline Cria Container para a execu\u00e7\u00e3o do Exemplo de replica\u00e7\u00e3o com Dagster replication_notebook Cria Container para execu\u00e7\u00e3o do Exemplo de replica\u00e7\u00e3o com Jupyter Notebook Como pode-se notar, os comandos para ambos os exemplos s\u00e3o os mesmos. A mudan\u00e7a ocorre apenas na nomenclatura. Com rela\u00e7\u00e3o ao funcionamento, como mencionado anteriormente, as opera\u00e7\u00f5es s\u00e3o as mesmas, variando apenas nos dados de entrada que s\u00e3o utilizados.","title":"Refer\u00eancia dos comandos dispon\u00edveis no Makefile"},{"location":"reproducible-research/#pre-requisitos","text":"Antes de iniciar os exemplos, certique-se que de que possui todas as ferramentas necess\u00e1rias instaladas e configuradas em seu ambiente de trabalho. Abaixo, essas ferramentas s\u00e3o listadas e descritas: Git Sistema de controle de vers\u00f5es (Documenta\u00e7\u00e3o criada com git vers\u00e3o 2.25.1 . Vers\u00f5es posteriores devem suportar os comandos utilizados); Docker Software de virtualiza\u00e7\u00e3o baseado em Containers (Documenta\u00e7\u00e3o criada com Docker vers\u00e3o 0.10.12 . Vers\u00f5es posteriores devem suportar os comandos utilizados); Docker Compose Ferramenta para a orquestra\u00e7\u00e3o e gerenciamento de Docker Containers (Documenta\u00e7\u00e3o criada com Docker Compose vers\u00e3o 1.29.2 . Vers\u00f5es posteriores devem suportar os comandos utilizados). GNU Make Ferramenta de automatiza\u00e7\u00e3o e controle de fluxos de build e execu\u00e7\u00e3o (Documenta\u00e7\u00e3o criada com GNU Make vers\u00e3o 4.2.1 . Vers\u00f5es posteriores devem suportar os comandos utilizados). Caso voc\u00ea n\u00e3o tenha alguma dessas ferramentas, utilize os links acima para acessar as respectivas documenta\u00e7\u00f5es oficiais e realizar as instala\u00e7\u00f5es necess\u00e1rias. Sistema operacional O sistema operacional utilizado para a cria\u00e7\u00e3o de todos os materiais deste RC , foi o Ubuntu 20.04. Assim, \u00e9 esperado que os passos apresentados, possam ser realizados em outras distros ou sistemas equivalentes (e.g., MacOS , FreeBSD , openSUSE ). Para Windows, pode ser que adapta\u00e7\u00f5es sejam necess\u00e1rias. Caso voc\u00ea esteja utilizando o Windows e n\u00e3o deseja fazer modifica\u00e7\u00f5es para utilizar o conte\u00fado deste RC , n\u00f3s disponibilizados uma M\u00e1quina Virtual que pode ser utilizada. Para saber mais, por favor, consulte a Se\u00e7\u00e3o Ambientes computacionais - M\u00e1quina virtual com Vagrant . Ap\u00f3s a instala\u00e7\u00e3o e configura\u00e7\u00e3o de todas as ferramentas listadas acima, voc\u00ea est\u00e1 pronto para iniciar os exemplos.","title":"Pr\u00e9-requisitos"},{"location":"reproducible-research/minimal-example/","text":"Exemplo m\u00ednimo \u00b6 Pr\u00e9-requisitos Antes de iniciar esse exemplo, certifique-se de ter os pr\u00e9-requisitos instalados em seu ambiente de trabalho. Associado a este RC , est\u00e1 um artigo em que \u00e9 realizado experimentos de harmoniza\u00e7\u00e3o entre sat\u00e9lites-sensores Landsat-8/OLI e Sentinel-2/MSI. Conforme apresentado no Cap\u00edtulo de Refer\u00eancia , os materiais e ferramentas deste RC , representam todo o esfor\u00e7o de desenvolvimento empregado para a gera\u00e7\u00e3o dos resultados desse artigo. Nesta Se\u00e7\u00e3o, faz-se a apresenta\u00e7\u00e3o do uso desse material em um exemplo reprodut\u00edvel de aplica\u00e7\u00e3o do fluxo de processamento desenvolvido no artigo para a gera\u00e7\u00e3o de produtos de harmoniza\u00e7\u00e3o. Os dados utilizados s\u00e3o um pequeno subset de 4 cenas ( 2x Landsat-8/OLI e 2x Sentinel-2/MSI ) extra\u00eddo do conjunto de dados original. Espera-se que com esse exemplo, os pesquisadores e interessados sejam capazes de explorar os materiais produzidos e a forma de implementa\u00e7\u00e3o do fluxo de processamento do artigo. Refer\u00eancia Esta \u00e9 uma Se\u00e7\u00e3o pr\u00e1tica, onde faz-se o uso dos materiais disponibilizados no RC . Caso seja necess\u00e1rio obter mais informa\u00e7\u00f5es sobre as ferramentas utilizadas, por favor, consulte o Cap\u00edtulo de Refer\u00eancia . Download do Research Compendium \u00b6 O primeiro passo para a realiza\u00e7\u00e3o desse exemplo, \u00e9 o download deste RC e todos os seus materiais. Para isso, em um terminal, utilize a ferramenta git e fa\u00e7a o clone do reposit\u00f3rio onde o RC est\u00e1 armazenado: git clone https://github.com/brazil-data-cube/compendium-harmonization Ap\u00f3s o clone , um novo diret\u00f3rio ser\u00e1 criado no diret\u00f3rio em que voc\u00ea est\u00e1. O nome deste novo diret\u00f3rio \u00e9 compendium-harmonization : ls -ls . #> 4 drwxrwxr-x 3 ubuntu ubuntu 4096 May 2 00:44 compendium-harmonization Agora, acesse o diret\u00f3rio compendium-harmonization e liste os conte\u00fados: Mudando de diret\u00f3rio cd compendium-harmonization Listando o conte\u00fado do diret\u00f3rio ls -ls . #> total 76K #> drwxrwxr-x 9 ubuntu ubuntu 4.0K May 1 23:29 . #> drwxrwxr-x 4 ubuntu ubuntu 4.0K May 2 00:44 .. #> drwxrwxr-x 5 ubuntu ubuntu 4.0K Apr 14 17:00 analysis #> -rw-rw-r-- 1 ubuntu ubuntu 1.4K May 1 16:36 bootstrap.sh #> drwxrwxr-x 4 ubuntu ubuntu 4.0K Apr 14 17:00 composes #> drwxrwxr-x 4 ubuntu ubuntu 4.0K Apr 14 17:00 docker #> -rw-rw-r-- 1 ubuntu ubuntu 375 May 1 16:36 .dockerignore #> drwxrwxr-x 3 ubuntu ubuntu 4.0K May 1 16:44 docs #> drwxrwxr-x 7 ubuntu ubuntu 4.0K Apr 14 17:00 .git #> drwxrwxr-x 3 ubuntu ubuntu 4.0K Apr 15 22:53 .github #> -rw-rw-r-- 1 ubuntu ubuntu 4.6K May 1 16:36 .gitignore #> -rw-rw-r-- 1 ubuntu ubuntu 1.1K May 1 16:35 LICENSE #> -rw-rw-r-- 1 ubuntu ubuntu 2.7K May 1 16:36 Makefile #> -rw-rw-r-- 1 ubuntu ubuntu 4.5K Apr 9 20:01 README.md #> -rw-rw-r-- 1 ubuntu ubuntu 392 May 1 16:36 setenv.sh #> drwxrwxr-x 6 ubuntu ubuntu 4.0K Apr 14 17:00 tools #> -rw-rw-r-- 1 ubuntu ubuntu 3.4K May 1 16:36 Vagrantfile Como voc\u00ea pode observar, o conte\u00fado do diret\u00f3rio representa todos os materiais deste RC . Esse ser\u00e1 o conte\u00fado base utilizado para a realiza\u00e7\u00e3o desse tutorial. A descri\u00e7\u00e3o de cada arquivo diret\u00f3rio deste RC pode ser encontrada na Introdu\u00e7\u00e3o desta documenta\u00e7\u00e3o. Download dos dados \u00b6 Ap\u00f3s o download do RC , seguindo as etapas apresentadas na introdu\u00e7\u00e3o do Cap\u00edtulo ( Processamento de dados ), primeiro \u00e9 necess\u00e1rio que seja feito o download dos dados que ser\u00e3o utilizados no exemplo. Esses dados, conforme especificado na Se\u00e7\u00e3o Example Toolkit , s\u00e3o armazenados no GitHub Release Assets . Sendo assim, ser\u00e1 necess\u00e1rio utilizar o Example Toolkit . Essa ferramenta, faz o download dos dados e os organiza na estrutura requerida pelos scripts de processamento deste RC . Para fazer essa execu\u00e7\u00e3o, primeiro fa\u00e7a a cria\u00e7\u00e3o da rede Docker a qual os Containers criados neste tutorial ficar\u00e3o associados. Para isso, em seu terminal, utilize o seguinte comando: docker network create research_processing_network #> fdaa46b4fe70bd34b6cb0e59734376234d801599a1fb1cbe1d9fd66a8f5044b1 Agora, atrav\u00e9s do GNU Make , fa\u00e7a a execu\u00e7\u00e3o do comando example_download_data : make example_download_data Pro tip Na execu\u00e7\u00e3o do GNU Make , caso voc\u00ea tenha algum problema de permiss\u00e3o relacionado a execu\u00e7\u00e3o do arquivo setenv.sh , utilize o seguinte comando antes de executar o GNU Make novamente: chmod +x setenv.sh Esse comando, far\u00e1 a utiliza\u00e7\u00e3o do Docker Compose espec\u00edfico para o download de dados deste exemplo. Ao execut\u00e1-lo, o download dos dados ser\u00e1 iniciado e uma mensagem parecida com a apresentada abaixo dever\u00e1 ser exibida (Alguns campos foram omitidos do exemplo para torn\u00e1-lo mais agrad\u00e1vel de ser visualizado na documenta\u00e7\u00e3o): Creating example-minimal-download-data ... done Attaching to example-minimal-download-data ( omitted ) | 2022 -05-02 01 :16:20.078 | INFO | ( omitted ) - Downloading minimal-example_landsat8_data.zip ( omitted ) ( omitted ) | 2022 -05-02 01 :21:09.345 | INFO | ( omitted ) - Downloading minimal-example_lasrc_auxiliary_data.zip ( omitted ) ( omitted ) | 2022 -05-02 01 :22:35.845 | INFO | ( omitted ) - Downloading minimal-example_scene_id_list.zip ( omitted ) ( omitted ) | 2022 -05-02 01 :22:36.510 | INFO | ( omitted ) - Downloading minimal-example_sentinel2_data.zip ( omitted ) ( omitted ) | 2022 -05-02 01 :25:14.653 | INFO | ( omitted ) - All files are downloaded. example-minimal-download-data exited with code 0 Download dos dados Caso voc\u00ea deseje obter mais informa\u00e7\u00f5es sobre o processo de download dos dados, por favor, consulte a Se\u00e7\u00e3o de refer\u00eancia Scripts auxiliares . Seguindo a organiza\u00e7\u00e3o do RC , os dados baixados foram armazenados no diret\u00f3rio analysis/data/examples/minimal_example/raw_data/ : ls -ls analysis/data/examples/minimal_example/raw_data/ #> total 16 #> 4 drwxrwxrwx 4 root root 4096 May 2 01:25 landsat8_data #> 4 drwxrwxrwx 5 root root 4096 May 2 01:22 lasrc_auxiliary_data #> 4 drwxrwxrwx 2 root root 4096 May 2 01:22 scene_id_list #> 4 drwxrwxrwx 4 root root 4096 May 2 01:25 sentinel2_data Organiza\u00e7\u00e3o dos dados A organiza\u00e7\u00e3o dos dados no diret\u00f3rio analysis/data/examples/minimal_example/raw_data/ , seguem o padr\u00e3o requerido pelos scripts de processamento. Para mais informa\u00e7\u00f5es, por favor, consulte a Se\u00e7\u00e3o de refer\u00eancia Diret\u00f3rio de dados . Processando dados com Jupyter Notebook \u00b6 Dando continuidade ao fluxo apresentado na introdu\u00e7\u00e3o do Cap\u00edtulo , uma primeira forma de realizar a aplica\u00e7\u00e3o do processamento descrito no artigo associado a este RC , \u00e9 atrav\u00e9s de um Jupyter Notebook. Esse documento, tem a descri\u00e7\u00e3o detalhada de cada uma das etapas do fluxo de processamento. Neste exemplo, os notebooks ser\u00e3o a primeira abordagem apresentada. Jupyter Notebook em todo lugar Para saber mais sobre os Jupyter Notebooks neste RC , consulte a Se\u00e7\u00e3o Scripts de processamento - Jupyter Notebook . Para fazer o uso dos notebooks e processar os dados, voc\u00ea deve utilizar o comando example_notebook atrav\u00e9s do GNU Make . Esse comando, far\u00e1 a configura\u00e7\u00e3o do Container para que voc\u00ea possa executar o Jupyter Notebook atrav\u00e9s de uma interface Jupyter Lab : make example_notebook #> (omitted) | [C 2022-05-02 02:09:19.813 ServerApp] #> (omitted) | #> (omitted) | To access the server, open this file in a browser: #> (omitted) | file:///home/jovyan/.local/share/jupyter/runtime/jpserver-7-open.html #> (omitted) | Or copy and paste one of these URLs: #> (omitted) | http://7bed3d1c3851:8888/lab?token=e6ad88f2a1b6358e1de88ea5a99ba3fd0b872293d3c9e845 #> (omitted) | or http://127.0.0.1:8888/lab?token=e6ad88f2a1b6358e1de88ea5a99ba3fd0b872293d3c9e845 Ao executar este comando, ser\u00e1 exibido em seu terminal o endere\u00e7o para acesso ao Jupyter Lab atrav\u00e9s de uma interface web. Utilize seu navegador e acesse esse endere\u00e7o: firefox http://127.0.0.1:8888/lab?token = e6ad88f2a1b6358e1de88ea5a99ba3fd0b872293d3c9e845 Ap\u00f3s acessar, seguindo a organiza\u00e7\u00e3o deste RC , acesse o arquivo de processamento que est\u00e1 na seguinte estrutura de diret\u00f3rios: analysis > notebook > research-processing.ipynb . Para exemplificar todo esse processo, abaixo, tem-se um v\u00eddeo que apresenta cada um dos passos mencionados: Configura\u00e7\u00e3o do Jupyter Notebook para o processamento dos dados Ap\u00f3s a execu\u00e7\u00e3o, os produtos gerados estar\u00e3o dispon\u00edveis no diret\u00f3rio analysis/data/derived_data Removendo dados Ap\u00f3s os testes, caso voc\u00ea deseje remover os dados de entrada/sa\u00edda, de modo que eles n\u00e3o ocupem espa\u00e7o em seu disco, utilize atrav\u00e9s do GNU Make o comando example_cleanup_data : make example_cleanup_data Note que isso apagar\u00e1 todos os dados. Fazendo isso, para a pr\u00f3xima execu\u00e7\u00e3o, o download dos dados dever\u00e1 ser realizado novamente. Processando dados com Dagster \u00b6 A segunda abordagem para o processamento dos dados baixados \u00e9 feita com o uso do Dagster. Com essa ferramenta faz-se a execu\u00e7\u00e3o do fluxo de processamento dos dados em formato batch . Mais Dagster Para saber mais sobre os Dagster neste RC , consulte a Se\u00e7\u00e3o Scripts de processamento - Dagster . Para fazer o uso do Dagster e processar os dados, voc\u00ea deve utilizar o comando example_pipeline atrav\u00e9s do GNU Make . Esse comando far\u00e1 a configura\u00e7\u00e3o do Container para que voc\u00ea possa acessar o Dagster atrav\u00e9s da interface DagIt : make example_pipeline #> (omitted) #> (omitted) | Welcome to Dagster! #> (omitted) | #> (omitted) | If you have any questions or would like to engage with the Dagster team, please join us on Slack #> (omitted) | (https://bit.ly/39dvSsF). #> (omitted) | #> (omitted) | Serving on http://0.0.0.0:3000 in process 1 Ao executar este comando, ser\u00e1 exibido em seu terminal o endere\u00e7o para acessar o DagIt. Utilize seu navegador e acesse esse endere\u00e7o: firefox http://127.0.0.1:3000 Pro tip O endere\u00e7o apresentado no terminal \u00e9 0.0.0.0 e o acesso \u00e9 feito pelo endere\u00e7o 127.0.0.1 no exemplo acima. Isso \u00e9 poss\u00edvel j\u00e1 que, 0.0.0.0 significa que qualquer endere\u00e7o pode acessar o servi\u00e7o criado. Ao acessar o endere\u00e7o, voc\u00ea estar\u00e1 no DagIt e poder\u00e1 come\u00e7ar a processar os dados. A Figura abaixo apresenta um exemplo da interface DagIt que voc\u00ea ver\u00e1: Exemplo de interface DagIt. Para come\u00e7ar a processar os dados, na interface DagIt, selecione a op\u00e7\u00e3o Playground : Op\u00e7\u00e3o Playground na interface DagIt. Ao acessar a aba Playground , voc\u00ea ver\u00e1 um campo para a defini\u00e7\u00e3o das configura\u00e7\u00f5es que devem ser consideradas no processamento. Essa configura\u00e7\u00e3o \u00e9 utilizada para determina quais ser\u00e3o os dados de entrada, dados auxiliares e tamb\u00e9m o local onde os produtos gerados devem ser salvos. As op\u00e7\u00f5es dessa configura\u00e7\u00e3o dever\u00e3o ser definidas para que os dados baixados sejam considerados. A figura abaixo apresenta o campo onde a configura\u00e7\u00e3o deve ser determinada. Campo de configura\u00e7\u00e3o DagIt. Para te auxiliar nessa configura\u00e7\u00e3o, no momento em que os dados foram baixados (Subse\u00e7\u00e3o Download dos dados ), o script Example toolkit tamb\u00e9m fez a gera\u00e7\u00e3o da configura\u00e7\u00e3o Dagster necess\u00e1ria para utilizar os dados. Esse arquivo de configura\u00e7\u00e3o est\u00e1 dispon\u00edvel no diret\u00f3rio analysis/pipeline/ , com o nome config.yaml . Configura\u00e7\u00e3o Dagster e Example toolkit Para saber mais sobre o formato de configura\u00e7\u00e3o Dagster e como ele pode ser adaptado para seu contexto, por favor, consulte a Se\u00e7\u00e3o de refer\u00eancia Dagster - Configura\u00e7\u00f5es . Adicionalmente, caso voc\u00ea deseje entender o funcionamento do Example toolkit , consulte a Se\u00e7\u00e3o de refer\u00eancia Scripts auxiliares . Copie o conte\u00fado do arquivo config.yaml e cole no campo de configura\u00e7\u00e3o na interface DagIt: Campo de configura\u00e7\u00e3o DagIt (Preenchido). Feito isso, inicie o processamento clicando em Launch Execution . Para exemplificar cada um desses passos, abaixo, tem-se um v\u00eddeo com cada uma das etapas de configura\u00e7\u00e3o e uso Dagster mencionadas anteriormente: Configura\u00e7\u00e3o do Dagster para o processamento dos dados Ap\u00f3s a execu\u00e7\u00e3o, os produtos gerados estar\u00e3o dispon\u00edveis no diret\u00f3rio analysis/data/derived_data . Removendo dados Ap\u00f3s os testes, caso voc\u00ea deseje remover os dados de entrada/sa\u00edda, de modo que eles n\u00e3o ocupem espa\u00e7o em seu disco, utilize atrav\u00e9s do GNU Make o comando example_cleanup_data : make example_cleanup_data Note que isso apagar\u00e1 todos os dados. Fazendo isso, para a pr\u00f3xima execu\u00e7\u00e3o, o download dos dados dever\u00e1 ser realizado novamente.","title":"Exemplo m\u00ednimo"},{"location":"reproducible-research/minimal-example/#exemplo-minimo","text":"Pr\u00e9-requisitos Antes de iniciar esse exemplo, certifique-se de ter os pr\u00e9-requisitos instalados em seu ambiente de trabalho. Associado a este RC , est\u00e1 um artigo em que \u00e9 realizado experimentos de harmoniza\u00e7\u00e3o entre sat\u00e9lites-sensores Landsat-8/OLI e Sentinel-2/MSI. Conforme apresentado no Cap\u00edtulo de Refer\u00eancia , os materiais e ferramentas deste RC , representam todo o esfor\u00e7o de desenvolvimento empregado para a gera\u00e7\u00e3o dos resultados desse artigo. Nesta Se\u00e7\u00e3o, faz-se a apresenta\u00e7\u00e3o do uso desse material em um exemplo reprodut\u00edvel de aplica\u00e7\u00e3o do fluxo de processamento desenvolvido no artigo para a gera\u00e7\u00e3o de produtos de harmoniza\u00e7\u00e3o. Os dados utilizados s\u00e3o um pequeno subset de 4 cenas ( 2x Landsat-8/OLI e 2x Sentinel-2/MSI ) extra\u00eddo do conjunto de dados original. Espera-se que com esse exemplo, os pesquisadores e interessados sejam capazes de explorar os materiais produzidos e a forma de implementa\u00e7\u00e3o do fluxo de processamento do artigo. Refer\u00eancia Esta \u00e9 uma Se\u00e7\u00e3o pr\u00e1tica, onde faz-se o uso dos materiais disponibilizados no RC . Caso seja necess\u00e1rio obter mais informa\u00e7\u00f5es sobre as ferramentas utilizadas, por favor, consulte o Cap\u00edtulo de Refer\u00eancia .","title":"Exemplo m\u00ednimo"},{"location":"reproducible-research/minimal-example/#download-do-research-compendium","text":"O primeiro passo para a realiza\u00e7\u00e3o desse exemplo, \u00e9 o download deste RC e todos os seus materiais. Para isso, em um terminal, utilize a ferramenta git e fa\u00e7a o clone do reposit\u00f3rio onde o RC est\u00e1 armazenado: git clone https://github.com/brazil-data-cube/compendium-harmonization Ap\u00f3s o clone , um novo diret\u00f3rio ser\u00e1 criado no diret\u00f3rio em que voc\u00ea est\u00e1. O nome deste novo diret\u00f3rio \u00e9 compendium-harmonization : ls -ls . #> 4 drwxrwxr-x 3 ubuntu ubuntu 4096 May 2 00:44 compendium-harmonization Agora, acesse o diret\u00f3rio compendium-harmonization e liste os conte\u00fados: Mudando de diret\u00f3rio cd compendium-harmonization Listando o conte\u00fado do diret\u00f3rio ls -ls . #> total 76K #> drwxrwxr-x 9 ubuntu ubuntu 4.0K May 1 23:29 . #> drwxrwxr-x 4 ubuntu ubuntu 4.0K May 2 00:44 .. #> drwxrwxr-x 5 ubuntu ubuntu 4.0K Apr 14 17:00 analysis #> -rw-rw-r-- 1 ubuntu ubuntu 1.4K May 1 16:36 bootstrap.sh #> drwxrwxr-x 4 ubuntu ubuntu 4.0K Apr 14 17:00 composes #> drwxrwxr-x 4 ubuntu ubuntu 4.0K Apr 14 17:00 docker #> -rw-rw-r-- 1 ubuntu ubuntu 375 May 1 16:36 .dockerignore #> drwxrwxr-x 3 ubuntu ubuntu 4.0K May 1 16:44 docs #> drwxrwxr-x 7 ubuntu ubuntu 4.0K Apr 14 17:00 .git #> drwxrwxr-x 3 ubuntu ubuntu 4.0K Apr 15 22:53 .github #> -rw-rw-r-- 1 ubuntu ubuntu 4.6K May 1 16:36 .gitignore #> -rw-rw-r-- 1 ubuntu ubuntu 1.1K May 1 16:35 LICENSE #> -rw-rw-r-- 1 ubuntu ubuntu 2.7K May 1 16:36 Makefile #> -rw-rw-r-- 1 ubuntu ubuntu 4.5K Apr 9 20:01 README.md #> -rw-rw-r-- 1 ubuntu ubuntu 392 May 1 16:36 setenv.sh #> drwxrwxr-x 6 ubuntu ubuntu 4.0K Apr 14 17:00 tools #> -rw-rw-r-- 1 ubuntu ubuntu 3.4K May 1 16:36 Vagrantfile Como voc\u00ea pode observar, o conte\u00fado do diret\u00f3rio representa todos os materiais deste RC . Esse ser\u00e1 o conte\u00fado base utilizado para a realiza\u00e7\u00e3o desse tutorial. A descri\u00e7\u00e3o de cada arquivo diret\u00f3rio deste RC pode ser encontrada na Introdu\u00e7\u00e3o desta documenta\u00e7\u00e3o.","title":"Download do Research Compendium"},{"location":"reproducible-research/minimal-example/#download-dos-dados","text":"Ap\u00f3s o download do RC , seguindo as etapas apresentadas na introdu\u00e7\u00e3o do Cap\u00edtulo ( Processamento de dados ), primeiro \u00e9 necess\u00e1rio que seja feito o download dos dados que ser\u00e3o utilizados no exemplo. Esses dados, conforme especificado na Se\u00e7\u00e3o Example Toolkit , s\u00e3o armazenados no GitHub Release Assets . Sendo assim, ser\u00e1 necess\u00e1rio utilizar o Example Toolkit . Essa ferramenta, faz o download dos dados e os organiza na estrutura requerida pelos scripts de processamento deste RC . Para fazer essa execu\u00e7\u00e3o, primeiro fa\u00e7a a cria\u00e7\u00e3o da rede Docker a qual os Containers criados neste tutorial ficar\u00e3o associados. Para isso, em seu terminal, utilize o seguinte comando: docker network create research_processing_network #> fdaa46b4fe70bd34b6cb0e59734376234d801599a1fb1cbe1d9fd66a8f5044b1 Agora, atrav\u00e9s do GNU Make , fa\u00e7a a execu\u00e7\u00e3o do comando example_download_data : make example_download_data Pro tip Na execu\u00e7\u00e3o do GNU Make , caso voc\u00ea tenha algum problema de permiss\u00e3o relacionado a execu\u00e7\u00e3o do arquivo setenv.sh , utilize o seguinte comando antes de executar o GNU Make novamente: chmod +x setenv.sh Esse comando, far\u00e1 a utiliza\u00e7\u00e3o do Docker Compose espec\u00edfico para o download de dados deste exemplo. Ao execut\u00e1-lo, o download dos dados ser\u00e1 iniciado e uma mensagem parecida com a apresentada abaixo dever\u00e1 ser exibida (Alguns campos foram omitidos do exemplo para torn\u00e1-lo mais agrad\u00e1vel de ser visualizado na documenta\u00e7\u00e3o): Creating example-minimal-download-data ... done Attaching to example-minimal-download-data ( omitted ) | 2022 -05-02 01 :16:20.078 | INFO | ( omitted ) - Downloading minimal-example_landsat8_data.zip ( omitted ) ( omitted ) | 2022 -05-02 01 :21:09.345 | INFO | ( omitted ) - Downloading minimal-example_lasrc_auxiliary_data.zip ( omitted ) ( omitted ) | 2022 -05-02 01 :22:35.845 | INFO | ( omitted ) - Downloading minimal-example_scene_id_list.zip ( omitted ) ( omitted ) | 2022 -05-02 01 :22:36.510 | INFO | ( omitted ) - Downloading minimal-example_sentinel2_data.zip ( omitted ) ( omitted ) | 2022 -05-02 01 :25:14.653 | INFO | ( omitted ) - All files are downloaded. example-minimal-download-data exited with code 0 Download dos dados Caso voc\u00ea deseje obter mais informa\u00e7\u00f5es sobre o processo de download dos dados, por favor, consulte a Se\u00e7\u00e3o de refer\u00eancia Scripts auxiliares . Seguindo a organiza\u00e7\u00e3o do RC , os dados baixados foram armazenados no diret\u00f3rio analysis/data/examples/minimal_example/raw_data/ : ls -ls analysis/data/examples/minimal_example/raw_data/ #> total 16 #> 4 drwxrwxrwx 4 root root 4096 May 2 01:25 landsat8_data #> 4 drwxrwxrwx 5 root root 4096 May 2 01:22 lasrc_auxiliary_data #> 4 drwxrwxrwx 2 root root 4096 May 2 01:22 scene_id_list #> 4 drwxrwxrwx 4 root root 4096 May 2 01:25 sentinel2_data Organiza\u00e7\u00e3o dos dados A organiza\u00e7\u00e3o dos dados no diret\u00f3rio analysis/data/examples/minimal_example/raw_data/ , seguem o padr\u00e3o requerido pelos scripts de processamento. Para mais informa\u00e7\u00f5es, por favor, consulte a Se\u00e7\u00e3o de refer\u00eancia Diret\u00f3rio de dados .","title":"Download dos dados"},{"location":"reproducible-research/minimal-example/#processando-dados-com-jupyter-notebook","text":"Dando continuidade ao fluxo apresentado na introdu\u00e7\u00e3o do Cap\u00edtulo , uma primeira forma de realizar a aplica\u00e7\u00e3o do processamento descrito no artigo associado a este RC , \u00e9 atrav\u00e9s de um Jupyter Notebook. Esse documento, tem a descri\u00e7\u00e3o detalhada de cada uma das etapas do fluxo de processamento. Neste exemplo, os notebooks ser\u00e3o a primeira abordagem apresentada. Jupyter Notebook em todo lugar Para saber mais sobre os Jupyter Notebooks neste RC , consulte a Se\u00e7\u00e3o Scripts de processamento - Jupyter Notebook . Para fazer o uso dos notebooks e processar os dados, voc\u00ea deve utilizar o comando example_notebook atrav\u00e9s do GNU Make . Esse comando, far\u00e1 a configura\u00e7\u00e3o do Container para que voc\u00ea possa executar o Jupyter Notebook atrav\u00e9s de uma interface Jupyter Lab : make example_notebook #> (omitted) | [C 2022-05-02 02:09:19.813 ServerApp] #> (omitted) | #> (omitted) | To access the server, open this file in a browser: #> (omitted) | file:///home/jovyan/.local/share/jupyter/runtime/jpserver-7-open.html #> (omitted) | Or copy and paste one of these URLs: #> (omitted) | http://7bed3d1c3851:8888/lab?token=e6ad88f2a1b6358e1de88ea5a99ba3fd0b872293d3c9e845 #> (omitted) | or http://127.0.0.1:8888/lab?token=e6ad88f2a1b6358e1de88ea5a99ba3fd0b872293d3c9e845 Ao executar este comando, ser\u00e1 exibido em seu terminal o endere\u00e7o para acesso ao Jupyter Lab atrav\u00e9s de uma interface web. Utilize seu navegador e acesse esse endere\u00e7o: firefox http://127.0.0.1:8888/lab?token = e6ad88f2a1b6358e1de88ea5a99ba3fd0b872293d3c9e845 Ap\u00f3s acessar, seguindo a organiza\u00e7\u00e3o deste RC , acesse o arquivo de processamento que est\u00e1 na seguinte estrutura de diret\u00f3rios: analysis > notebook > research-processing.ipynb . Para exemplificar todo esse processo, abaixo, tem-se um v\u00eddeo que apresenta cada um dos passos mencionados: Configura\u00e7\u00e3o do Jupyter Notebook para o processamento dos dados Ap\u00f3s a execu\u00e7\u00e3o, os produtos gerados estar\u00e3o dispon\u00edveis no diret\u00f3rio analysis/data/derived_data Removendo dados Ap\u00f3s os testes, caso voc\u00ea deseje remover os dados de entrada/sa\u00edda, de modo que eles n\u00e3o ocupem espa\u00e7o em seu disco, utilize atrav\u00e9s do GNU Make o comando example_cleanup_data : make example_cleanup_data Note que isso apagar\u00e1 todos os dados. Fazendo isso, para a pr\u00f3xima execu\u00e7\u00e3o, o download dos dados dever\u00e1 ser realizado novamente.","title":"Processando dados com Jupyter Notebook"},{"location":"reproducible-research/minimal-example/#processando-dados-com-dagster","text":"A segunda abordagem para o processamento dos dados baixados \u00e9 feita com o uso do Dagster. Com essa ferramenta faz-se a execu\u00e7\u00e3o do fluxo de processamento dos dados em formato batch . Mais Dagster Para saber mais sobre os Dagster neste RC , consulte a Se\u00e7\u00e3o Scripts de processamento - Dagster . Para fazer o uso do Dagster e processar os dados, voc\u00ea deve utilizar o comando example_pipeline atrav\u00e9s do GNU Make . Esse comando far\u00e1 a configura\u00e7\u00e3o do Container para que voc\u00ea possa acessar o Dagster atrav\u00e9s da interface DagIt : make example_pipeline #> (omitted) #> (omitted) | Welcome to Dagster! #> (omitted) | #> (omitted) | If you have any questions or would like to engage with the Dagster team, please join us on Slack #> (omitted) | (https://bit.ly/39dvSsF). #> (omitted) | #> (omitted) | Serving on http://0.0.0.0:3000 in process 1 Ao executar este comando, ser\u00e1 exibido em seu terminal o endere\u00e7o para acessar o DagIt. Utilize seu navegador e acesse esse endere\u00e7o: firefox http://127.0.0.1:3000 Pro tip O endere\u00e7o apresentado no terminal \u00e9 0.0.0.0 e o acesso \u00e9 feito pelo endere\u00e7o 127.0.0.1 no exemplo acima. Isso \u00e9 poss\u00edvel j\u00e1 que, 0.0.0.0 significa que qualquer endere\u00e7o pode acessar o servi\u00e7o criado. Ao acessar o endere\u00e7o, voc\u00ea estar\u00e1 no DagIt e poder\u00e1 come\u00e7ar a processar os dados. A Figura abaixo apresenta um exemplo da interface DagIt que voc\u00ea ver\u00e1: Exemplo de interface DagIt. Para come\u00e7ar a processar os dados, na interface DagIt, selecione a op\u00e7\u00e3o Playground : Op\u00e7\u00e3o Playground na interface DagIt. Ao acessar a aba Playground , voc\u00ea ver\u00e1 um campo para a defini\u00e7\u00e3o das configura\u00e7\u00f5es que devem ser consideradas no processamento. Essa configura\u00e7\u00e3o \u00e9 utilizada para determina quais ser\u00e3o os dados de entrada, dados auxiliares e tamb\u00e9m o local onde os produtos gerados devem ser salvos. As op\u00e7\u00f5es dessa configura\u00e7\u00e3o dever\u00e3o ser definidas para que os dados baixados sejam considerados. A figura abaixo apresenta o campo onde a configura\u00e7\u00e3o deve ser determinada. Campo de configura\u00e7\u00e3o DagIt. Para te auxiliar nessa configura\u00e7\u00e3o, no momento em que os dados foram baixados (Subse\u00e7\u00e3o Download dos dados ), o script Example toolkit tamb\u00e9m fez a gera\u00e7\u00e3o da configura\u00e7\u00e3o Dagster necess\u00e1ria para utilizar os dados. Esse arquivo de configura\u00e7\u00e3o est\u00e1 dispon\u00edvel no diret\u00f3rio analysis/pipeline/ , com o nome config.yaml . Configura\u00e7\u00e3o Dagster e Example toolkit Para saber mais sobre o formato de configura\u00e7\u00e3o Dagster e como ele pode ser adaptado para seu contexto, por favor, consulte a Se\u00e7\u00e3o de refer\u00eancia Dagster - Configura\u00e7\u00f5es . Adicionalmente, caso voc\u00ea deseje entender o funcionamento do Example toolkit , consulte a Se\u00e7\u00e3o de refer\u00eancia Scripts auxiliares . Copie o conte\u00fado do arquivo config.yaml e cole no campo de configura\u00e7\u00e3o na interface DagIt: Campo de configura\u00e7\u00e3o DagIt (Preenchido). Feito isso, inicie o processamento clicando em Launch Execution . Para exemplificar cada um desses passos, abaixo, tem-se um v\u00eddeo com cada uma das etapas de configura\u00e7\u00e3o e uso Dagster mencionadas anteriormente: Configura\u00e7\u00e3o do Dagster para o processamento dos dados Ap\u00f3s a execu\u00e7\u00e3o, os produtos gerados estar\u00e3o dispon\u00edveis no diret\u00f3rio analysis/data/derived_data . Removendo dados Ap\u00f3s os testes, caso voc\u00ea deseje remover os dados de entrada/sa\u00edda, de modo que eles n\u00e3o ocupem espa\u00e7o em seu disco, utilize atrav\u00e9s do GNU Make o comando example_cleanup_data : make example_cleanup_data Note que isso apagar\u00e1 todos os dados. Fazendo isso, para a pr\u00f3xima execu\u00e7\u00e3o, o download dos dados dever\u00e1 ser realizado novamente.","title":"Processando dados com Dagster"},{"location":"reproducible-research/replication-example/","text":"Exemplo de replica\u00e7\u00e3o \u00b6 Pr\u00e9-requisitos Antes de iniciar esse exemplo, certifique-se de ter os pr\u00e9-requisitos instalados em seu ambiente de trabalho. Exemplo base Esse \u00e9 um exemplo que utiliza os materiais deste RC para o processamento de dados de uma regi\u00e3o que n\u00e3o foi considerada no artigo original. O objetivo \u00e9 mostrar que a ferramenta tem caracter\u00edsticas que as tornam reprodut\u00edveis e replic\u00e1veis. Sendo assim, caso voc\u00ea n\u00e3o tenha feito o primeiro exemplo ( Exemplo m\u00ednimo ), recomenda-se que antes de come\u00e7ar esse, voc\u00ea realize o anterior. Download do Research Compendium \u00b6 O primeiro passo para a realiza\u00e7\u00e3o desse exemplo, \u00e9 o download deste RC e todos os seus materiais. Para isso, em um terminal, utilize a ferramenta git e fa\u00e7a o clone do reposit\u00f3rio onde o RC est\u00e1 armazenado: git clone https://github.com/brazil-data-cube/compendium-harmonization Ap\u00f3s o clone , um novo diret\u00f3rio ser\u00e1 criado no diret\u00f3rio em que voc\u00ea est\u00e1. O nome deste novo diret\u00f3rio \u00e9 compendium-harmonization : ls -ls . #> 4 drwxrwxr-x 3 ubuntu ubuntu 4096 May 2 00:44 compendium-harmonization Agora, acesse o diret\u00f3rio compendium-harmonization e liste os conte\u00fados: Mudando de diret\u00f3rio cd compendium-harmonization Listando o conte\u00fado do diret\u00f3rio ls -ls . #> total 76K #> drwxrwxr-x 9 ubuntu ubuntu 4.0K May 1 23:29 . #> drwxrwxr-x 4 ubuntu ubuntu 4.0K May 2 00:44 .. #> drwxrwxr-x 5 ubuntu ubuntu 4.0K Apr 14 17:00 analysis #> -rw-rw-r-- 1 ubuntu ubuntu 1.4K May 1 16:36 bootstrap.sh #> drwxrwxr-x 4 ubuntu ubuntu 4.0K Apr 14 17:00 composes #> drwxrwxr-x 4 ubuntu ubuntu 4.0K Apr 14 17:00 docker #> -rw-rw-r-- 1 ubuntu ubuntu 375 May 1 16:36 .dockerignore #> drwxrwxr-x 3 ubuntu ubuntu 4.0K May 1 16:44 docs #> drwxrwxr-x 7 ubuntu ubuntu 4.0K Apr 14 17:00 .git #> drwxrwxr-x 3 ubuntu ubuntu 4.0K Apr 15 22:53 .github #> -rw-rw-r-- 1 ubuntu ubuntu 4.6K May 1 16:36 .gitignore #> -rw-rw-r-- 1 ubuntu ubuntu 1.1K May 1 16:35 LICENSE #> -rw-rw-r-- 1 ubuntu ubuntu 2.7K May 1 16:36 Makefile #> -rw-rw-r-- 1 ubuntu ubuntu 4.5K Apr 9 20:01 README.md #> -rw-rw-r-- 1 ubuntu ubuntu 392 May 1 16:36 setenv.sh #> drwxrwxr-x 6 ubuntu ubuntu 4.0K Apr 14 17:00 tools #> -rw-rw-r-- 1 ubuntu ubuntu 3.4K May 1 16:36 Vagrantfile Como voc\u00ea pode observar, o conte\u00fado do diret\u00f3rio representa todos os materiais deste RC . Esse ser\u00e1 o conte\u00fado base utilizado para a realiza\u00e7\u00e3o desse tutorial. Download dos dados \u00b6 Para realizar o download dos dados necess\u00e1rios para esse exemplo de replica\u00e7\u00e3o, pode-se utilizar o seguinte comando: make replication_ download_data Verifique se os dados foram baixados no diret\u00f3rio analysis/data/examples/replication_example/ . Caso tudo esteja correto, o conte\u00fado desse diret\u00f3rio dever\u00e1 ser parecido com o apresentado abaixo: . \u251c\u2500\u2500 landsat8_data \u251c\u2500\u2500 lasrc_auxiliary_data \u251c\u2500\u2500 scene_id_list \u2514\u2500\u2500 sentinel2_data Processamento de dados \u00b6 Para realizar o processamento dos dados, como j\u00e1 mencionado nas Se\u00e7\u00f5es anteriores, tem-se dispon\u00edvel duas op\u00e7\u00f5es: Jupyter Notebook ou Dagster . Para ambas abordagens adotadas, tem-se comandos de automa\u00e7\u00e3o no Makefile . Sendo assim, nos t\u00f3picos abaixo, de forma resumida, s\u00e3o apresentadas as formas com que cada um desses ambientes podem ser carregados com comandos simples: Jupyter Notebook make replication_notebook Modifica\u00e7\u00e3o necess\u00e1ria Como o exemplo de replica\u00e7\u00e3o utiliza dados diferentes dos apresentados no exemplo m\u00ednimo, faz-se necess\u00e1rio pequenas modifica\u00e7\u00f5es nos par\u00e2metros utilizados no Jupyter Notebook. Especificamente, o par\u00e2metro day_diff . Esse par\u00e2metro define a quantidade de dias de diferen\u00e7a entre imagens. Essa mudan\u00e7a deve ser feita apenas para testes onde faz-se o uso apenas do Sentinel-2. Com isso, voc\u00ea deve alterar as seguintes se\u00e7\u00f5es do Jupyter Notebook: 4.3.1: Searching for image pairs 4.4.1: Searching for image pairs 4.5.1: Searching for image pairs 4.6.1: Searching for image pairs Em todas elas, deve-se alterar o day_diff para 15 dias: validation_funcs . search_pairs_s2 ( sentinel2_sceneid_list , day_diff = 15 ) Dagster make replication_pipeline Modifica\u00e7\u00e3o necess\u00e1ria Como o exemplo de replica\u00e7\u00e3o utiliza dados diferentes dos apresentados no exemplo m\u00ednimo, faz-se necess\u00e1rio pequenas modifica\u00e7\u00f5es nos par\u00e2metros do pipeline. Especificamente, o par\u00e2metro day_diff deve ser alterado para 15 dias . Esse par\u00e2metro define a quantidade de dias de diferen\u00e7a entre imagens. Essa mudan\u00e7a deve ser feita apenas para testes onde faz-se o uso apenas do Sentinel-2. Com isso, voc\u00ea deve adicionar na se\u00e7\u00e3o solids de seu arquivo de configura\u00e7\u00e3o Dagster as seguintes mudan\u00e7as: solids : validation_sr_s2_sen2cor : config : day_difference : 15 validation_sr_s2_lasrc : config : day_difference : 15 validation_nbar_s2_sen2cor : config : day_difference : 15 validation_nbar_s2_lasrc : config : day_difference : 15 Com o ambiente escolhido, o processamento e an\u00e1lise dos dados podem ser realizados.","title":"Exemplo de replica\u00e7\u00e3o"},{"location":"reproducible-research/replication-example/#exemplo-de-replicacao","text":"Pr\u00e9-requisitos Antes de iniciar esse exemplo, certifique-se de ter os pr\u00e9-requisitos instalados em seu ambiente de trabalho. Exemplo base Esse \u00e9 um exemplo que utiliza os materiais deste RC para o processamento de dados de uma regi\u00e3o que n\u00e3o foi considerada no artigo original. O objetivo \u00e9 mostrar que a ferramenta tem caracter\u00edsticas que as tornam reprodut\u00edveis e replic\u00e1veis. Sendo assim, caso voc\u00ea n\u00e3o tenha feito o primeiro exemplo ( Exemplo m\u00ednimo ), recomenda-se que antes de come\u00e7ar esse, voc\u00ea realize o anterior.","title":"Exemplo de replica\u00e7\u00e3o"},{"location":"reproducible-research/replication-example/#download-do-research-compendium","text":"O primeiro passo para a realiza\u00e7\u00e3o desse exemplo, \u00e9 o download deste RC e todos os seus materiais. Para isso, em um terminal, utilize a ferramenta git e fa\u00e7a o clone do reposit\u00f3rio onde o RC est\u00e1 armazenado: git clone https://github.com/brazil-data-cube/compendium-harmonization Ap\u00f3s o clone , um novo diret\u00f3rio ser\u00e1 criado no diret\u00f3rio em que voc\u00ea est\u00e1. O nome deste novo diret\u00f3rio \u00e9 compendium-harmonization : ls -ls . #> 4 drwxrwxr-x 3 ubuntu ubuntu 4096 May 2 00:44 compendium-harmonization Agora, acesse o diret\u00f3rio compendium-harmonization e liste os conte\u00fados: Mudando de diret\u00f3rio cd compendium-harmonization Listando o conte\u00fado do diret\u00f3rio ls -ls . #> total 76K #> drwxrwxr-x 9 ubuntu ubuntu 4.0K May 1 23:29 . #> drwxrwxr-x 4 ubuntu ubuntu 4.0K May 2 00:44 .. #> drwxrwxr-x 5 ubuntu ubuntu 4.0K Apr 14 17:00 analysis #> -rw-rw-r-- 1 ubuntu ubuntu 1.4K May 1 16:36 bootstrap.sh #> drwxrwxr-x 4 ubuntu ubuntu 4.0K Apr 14 17:00 composes #> drwxrwxr-x 4 ubuntu ubuntu 4.0K Apr 14 17:00 docker #> -rw-rw-r-- 1 ubuntu ubuntu 375 May 1 16:36 .dockerignore #> drwxrwxr-x 3 ubuntu ubuntu 4.0K May 1 16:44 docs #> drwxrwxr-x 7 ubuntu ubuntu 4.0K Apr 14 17:00 .git #> drwxrwxr-x 3 ubuntu ubuntu 4.0K Apr 15 22:53 .github #> -rw-rw-r-- 1 ubuntu ubuntu 4.6K May 1 16:36 .gitignore #> -rw-rw-r-- 1 ubuntu ubuntu 1.1K May 1 16:35 LICENSE #> -rw-rw-r-- 1 ubuntu ubuntu 2.7K May 1 16:36 Makefile #> -rw-rw-r-- 1 ubuntu ubuntu 4.5K Apr 9 20:01 README.md #> -rw-rw-r-- 1 ubuntu ubuntu 392 May 1 16:36 setenv.sh #> drwxrwxr-x 6 ubuntu ubuntu 4.0K Apr 14 17:00 tools #> -rw-rw-r-- 1 ubuntu ubuntu 3.4K May 1 16:36 Vagrantfile Como voc\u00ea pode observar, o conte\u00fado do diret\u00f3rio representa todos os materiais deste RC . Esse ser\u00e1 o conte\u00fado base utilizado para a realiza\u00e7\u00e3o desse tutorial.","title":"Download do Research Compendium"},{"location":"reproducible-research/replication-example/#download-dos-dados","text":"Para realizar o download dos dados necess\u00e1rios para esse exemplo de replica\u00e7\u00e3o, pode-se utilizar o seguinte comando: make replication_ download_data Verifique se os dados foram baixados no diret\u00f3rio analysis/data/examples/replication_example/ . Caso tudo esteja correto, o conte\u00fado desse diret\u00f3rio dever\u00e1 ser parecido com o apresentado abaixo: . \u251c\u2500\u2500 landsat8_data \u251c\u2500\u2500 lasrc_auxiliary_data \u251c\u2500\u2500 scene_id_list \u2514\u2500\u2500 sentinel2_data","title":"Download dos dados"},{"location":"reproducible-research/replication-example/#processamento-de-dados","text":"Para realizar o processamento dos dados, como j\u00e1 mencionado nas Se\u00e7\u00f5es anteriores, tem-se dispon\u00edvel duas op\u00e7\u00f5es: Jupyter Notebook ou Dagster . Para ambas abordagens adotadas, tem-se comandos de automa\u00e7\u00e3o no Makefile . Sendo assim, nos t\u00f3picos abaixo, de forma resumida, s\u00e3o apresentadas as formas com que cada um desses ambientes podem ser carregados com comandos simples: Jupyter Notebook make replication_notebook Modifica\u00e7\u00e3o necess\u00e1ria Como o exemplo de replica\u00e7\u00e3o utiliza dados diferentes dos apresentados no exemplo m\u00ednimo, faz-se necess\u00e1rio pequenas modifica\u00e7\u00f5es nos par\u00e2metros utilizados no Jupyter Notebook. Especificamente, o par\u00e2metro day_diff . Esse par\u00e2metro define a quantidade de dias de diferen\u00e7a entre imagens. Essa mudan\u00e7a deve ser feita apenas para testes onde faz-se o uso apenas do Sentinel-2. Com isso, voc\u00ea deve alterar as seguintes se\u00e7\u00f5es do Jupyter Notebook: 4.3.1: Searching for image pairs 4.4.1: Searching for image pairs 4.5.1: Searching for image pairs 4.6.1: Searching for image pairs Em todas elas, deve-se alterar o day_diff para 15 dias: validation_funcs . search_pairs_s2 ( sentinel2_sceneid_list , day_diff = 15 ) Dagster make replication_pipeline Modifica\u00e7\u00e3o necess\u00e1ria Como o exemplo de replica\u00e7\u00e3o utiliza dados diferentes dos apresentados no exemplo m\u00ednimo, faz-se necess\u00e1rio pequenas modifica\u00e7\u00f5es nos par\u00e2metros do pipeline. Especificamente, o par\u00e2metro day_diff deve ser alterado para 15 dias . Esse par\u00e2metro define a quantidade de dias de diferen\u00e7a entre imagens. Essa mudan\u00e7a deve ser feita apenas para testes onde faz-se o uso apenas do Sentinel-2. Com isso, voc\u00ea deve adicionar na se\u00e7\u00e3o solids de seu arquivo de configura\u00e7\u00e3o Dagster as seguintes mudan\u00e7as: solids : validation_sr_s2_sen2cor : config : day_difference : 15 validation_sr_s2_lasrc : config : day_difference : 15 validation_nbar_s2_sen2cor : config : day_difference : 15 validation_nbar_s2_lasrc : config : day_difference : 15 Com o ambiente escolhido, o processamento e an\u00e1lise dos dados podem ser realizados.","title":"Processamento de dados"},{"location":"tools/","text":"Refer\u00eancia \u00b6 Esta \u00e9 o Cap\u00edtulo de refer\u00eancia deste RC . Aqui, s\u00e3o apresentados em detalhes as caracter\u00edsticas de cada um dos componentes dispon\u00edveis no RC e como eles podem ser utilizados individualmente, mesmo fora do contexto deste RC . Recomenda\u00e7\u00e3o \u00c9 recomendado que voc\u00ea fa\u00e7a o uso deste Cap\u00edtulo em conjunto com a execu\u00e7\u00e3o dos exemplos disponibilizados no Cap\u00edtulo Processamento de dados . Com isso, voc\u00ea pode ver a aplica\u00e7\u00e3o de cada um dos materiais e quando sentir a necessidade de mais esclarecimentos, voc\u00ea pode consultar o conte\u00fado dispon\u00edveis neste Cap\u00edtulo de ref\u00eancia.","title":"Introdu\u00e7\u00e3o"},{"location":"tools/#referencia","text":"Esta \u00e9 o Cap\u00edtulo de refer\u00eancia deste RC . Aqui, s\u00e3o apresentados em detalhes as caracter\u00edsticas de cada um dos componentes dispon\u00edveis no RC e como eles podem ser utilizados individualmente, mesmo fora do contexto deste RC . Recomenda\u00e7\u00e3o \u00c9 recomendado que voc\u00ea fa\u00e7a o uso deste Cap\u00edtulo em conjunto com a execu\u00e7\u00e3o dos exemplos disponibilizados no Cap\u00edtulo Processamento de dados . Com isso, voc\u00ea pode ver a aplica\u00e7\u00e3o de cada um dos materiais e quando sentir a necessidade de mais esclarecimentos, voc\u00ea pode consultar o conte\u00fado dispon\u00edveis neste Cap\u00edtulo de ref\u00eancia.","title":"Refer\u00eancia"},{"location":"tools/environment/","text":"Ambientes computacionais \u00b6 Por tr\u00e1s de cada etapa empregada nos scripts de processamento , conforme apresentado nas Se\u00e7\u00f5es anteriores, existem diversas ferramentas e bibliotecas de software sendo utilizadas. Algumas dessas ferramentas fazem o uso de tecnologias especiais para a execu\u00e7\u00e3o de suas opera\u00e7\u00f5es, como \u00e9 o caso da biblioteca research-processing , que utiliza Docker Containers para executar as fun\u00e7\u00f5es de processamento em ambientes isolados. Outras ferramentas, apenas fazem o uso do ambiente subjacente para sua execu\u00e7\u00e3o, como \u00e9 o caso do script auxiliar Example toolkit . Neste caso, \u00e9 exigido que o ambiente subjacente esteja configurado e pronto para executar o script . Em ambos os cen\u00e1rios apresentados, h\u00e1 desafios espec\u00edficos no que diz respeito ao gerenciamento dos ambientes computacionais utilizados. Por exemplo, pode ser necess\u00e1rio configura\u00e7\u00f5es espec\u00edficas no software para que ele opere junto a biblioteca research-processing , enquanto que configura\u00e7\u00f5es espec\u00edficas podem ser exigidas no uso do Example toolkit . Para resolver esses problemas e evitar que a configura\u00e7\u00e3o dos ambientes computacionais utilizados, suas depend\u00eancias e necessidades espec\u00edficas causem problemas para a reprodutibilidade e replicabilidade dos scripts de processamento criados neste RC , todos os ambientes necess\u00e1rios para a execu\u00e7\u00e3o das ferramentas foram organizadas em Docker Images. Essas, representam \"pacotes de ambientes\" prontos para uso, onde tem-se todas as depend\u00eancias e configura\u00e7\u00f5es necess\u00e1rias para a execu\u00e7\u00e3o de uma ferramenta espec\u00edfica. Nesta Se\u00e7\u00e3o, \u00e9 feita a apresenta\u00e7\u00e3o de cada uma dessas Docker Images, suas caracter\u00edsticas, configura\u00e7\u00f5es e formas de uso. Deve-se notar que, essas Docker Images n\u00e3o foram criadas para um sistema operacional espec\u00edfico, podendo ser utilizado em qualquer sistema que tenha suporte ao Docker. No entanto, deve-se notar que nesta documenta\u00e7\u00e3o, os comandos e formas de configura\u00e7\u00e3o apresentados, consideram como base o uso do Sistema Operacional Linux Ubuntu 20.04. Desta forma, mudan\u00e7as podem ser necess\u00e1rias nos comandos caso voc\u00ea esteja utilizando um sistema operacional diferente, como \u00e9 o caso do Windows. Mudan\u00e7as entre os sistemas operacionais Embora tenhamos esperan\u00e7a de que os comandos e dicas apresentados neste documento possam ser utilizados sem problemas em sistemas operacionais Linux (e.g., Ubuntu, Debian) e MacOS, n\u00e3o h\u00e1 uma garantia de que isso sempre se manter\u00e1 verdadeiro. Al\u00e9m disso, para aqueles que utilizam Windows, mudan\u00e7as nos comandos podem ser necess\u00e1rias. Com o objetivo de evitar que os materiais produzidos n\u00e3o possam ser utilizados por essa barreira tecnol\u00f3gica, n\u00f3s criamos uma M\u00e1quina Virtual Ubuntu 20.04, com todas as depend\u00eancias necess\u00e1rias (e.g., Docker , Docker Compose ) para que os comandos apresentados aqui, possam ser utilizados. Caso voc\u00ea precise utilizar essa M\u00e1quina Virtual, por favor, consulte a Se\u00e7\u00e3o M\u00e1quina Virtual com Vagrant . Docker Images \u00b6 Neste RC , existem diferentes tipos de ambientes que est\u00e3o sendo configurados dentro das Docker Images. No geral, essas Docker Images podem ser classificadas em dois tipos: Executable Ferramentas de linha de comando ( CLI , do ingl\u00eas Command-Line Interface ) s\u00e3o simples e diretas de utilizar e permitem que automa\u00e7\u00f5es sejam realizadas durante as etapas de processamento. Seguindo esse pensamento, as Docker Images Executable s\u00e3o aquelas criadas para armazenar um script que pode ser executado como uma CLI . Para isso, esse tipo de Docker Image tem as seguintes propriedades: Cada execu\u00e7\u00e3o da Docker Image representa uma execu\u00e7\u00e3o individual da ferramenta a qual est\u00e1 associada; Par\u00e2metros podem ser passados durante a execu\u00e7\u00e3o da Docker Image. Esses par\u00e2metros s\u00e3o utilizados para configurar a ferramenta que \u00e9 executada; Docker Volumes e vari\u00e1veis de ambientes, tamb\u00e9m podem ser utilizados para configurar a Docker Image, sendo utilizados, por exemplo, para determinar as entradas e sa\u00eddas e configura\u00e7\u00f5es da ferramenta executada. Environment Diferente das Docker Images Executable , essas Docker Images s\u00e3o criadas para servir como um ambiente completo que ser\u00e1 utilizado para a execu\u00e7\u00e3o de uma ferramenta, como um Jupyter Notebook ou Dagster Web Interface. A principal diferen\u00e7a entre esses dois tipos de Docker Images criados neste RC est\u00e1 em sua finalidade. Enquanto as Executables representam as ferramentas execut\u00e1veis, as Environment representam ambientes de fato, para uso e execu\u00e7\u00e3o de ferramentas espec\u00edficas. Nas Subse\u00e7\u00f5es a seguir, as Docker Images criadas neste RC ser\u00e3o apresentadas. Sen2Cor 2.9.0 \u00b6 Sen2cor \u00e9 um processador de corre\u00e7\u00e3o atmosf\u00e9rica para produtos Sentinel-2. Ele recebe como entrada produtos Sentinel-2 em n\u00edvel de radi\u00e2ncia de topo de atmosf\u00e9ra (ToA), tamb\u00e9m chamados Level-1C (L1C) e gera produtos \"Bottom of the atmosphere (BOA) reflectance\", al\u00e9m de fornecer tamb\u00e9m uma classifica\u00e7\u00e3o da cena. Com o uso dessa classifica\u00e7\u00e3o, \u00e9 poss\u00edvel, por exemplo, identificar pixels com influ\u00eancia de nuvem, sombra de nuvem ou neve. Mais sobre o Sen2Cor Para mais informa\u00e7\u00f5es sobre o Sen2Cor, consulte o Manual oficial do usu\u00e1rio . De modo a possibilitar que as execu\u00e7\u00f5es realizadas com Sen2Cor fossem reprodut\u00edveis e reutiliz\u00e1veis, preparou-se uma Docker Image espec\u00edfica para essa ferramenta. Essa Docker Image, nomeada de sen2cor , possui todas as depend\u00eancias e configura\u00e7\u00f5es necess\u00e1rias para a execu\u00e7\u00e3o do Sen2Cor. Vers\u00f5es do Sen2Cor O Sen2Cor , \u00e9 um software mantido e distribu\u00eddo pela E uropean S pace A gency (ESA) e continua lan\u00e7ando novas vers\u00f5es. Neste RC , foi considerado paras as atividades com o Sen2Cor, a vers\u00e3o 2.9.0 . Os t\u00f3picos a seguir, apresentam as principais caracter\u00edsticas desta Docker Image, como volumes, dados auxiliares necess\u00e1rios e forma de utiliza\u00e7\u00e3o. Dados auxiliares Para a execu\u00e7\u00e3o da sen2cor , \u00e9 necess\u00e1rio o uso de alguns dados auxiliares. Esses dados, dizem respeito ao ESACCI-LC for Sen2Cor data package , que \u00e9 utilizado pelos m\u00f3dulos de identifica\u00e7\u00e3o de nuvens e classifica\u00e7\u00e3o. A obten\u00e7\u00e3o dos dados pode ser feita seguindo os passos listados abaixo: Acesse o endere\u00e7o: http://maps.elie.ucl.ac.be/CCI/viewer/download.php ; Fa\u00e7a seu cadastro (caso n\u00e3o tenha) e login; Ap\u00f3s o login, procure pelo pacote ESACCI-LC for Sen2Cor data package ; Fa\u00e7a o download desse pacote (Arquivo zip ); Extraia o conte\u00fado em um diret\u00f3rio. Recomenda-se para esse diret\u00f3rio o nome CCI4SEN2COR . Ap\u00f3s a extra\u00e7\u00e3o dos arquivos, o diret\u00f3rio de destino dever\u00e1 conter os seguintes arquivos: ESACCI-LC-L4-WB-Map-150m-P13Y-2000-v4.0.tif (GeoTIFF); ESACCI-LC-L4-LCCS-Map-300m-P1Y-2015-v2.0.7.tif (GeoTIFF); ESACCI-LC-L4-Snow-Cond-500m-P13Y7D-2000-2012-v2.0 (Direct\u00f3rio). Volumes Para a utiliza\u00e7\u00e3o da sen2cor , \u00e9 necess\u00e1rio a defini\u00e7\u00e3o de alguns Docker Volumes. Esses volumes, especificam os dados de entrada, sa\u00edda, arquivos de configura\u00e7\u00e3o e dados auxiliares utilizados pela ferramenta. Abaixo, esses volumes s\u00e3o listados e descritos: Dados de entrada (Obrigat\u00f3rio) Diret\u00f3rio com os dados de entrada. Esse volume, deve mapear um diret\u00f3rio da m\u00e1quina local para o diret\u00f3rio /mnt/input_dir do Container. Recomenda-se que esse volume seja somente leitura, para garantir que nenhuma modifica\u00e7\u00e3o ser\u00e1 feita nos dados de entrada. Dados de sa\u00edda (Obrigat\u00f3rio) Diret\u00f3rio onde os produtos gerados ser\u00e3o armazenados. O volume criado, deve mapear um diret\u00f3rio da m\u00e1quina local para o diret\u00f3rio /mnt/output_dir do Container. Dados auxiliares (Obrigat\u00f3rio) Diret\u00f3rio com os dados auxiliares necess\u00e1rio para o funcionamento do Sen2Cor. O volume criado, deve mapear um diret\u00f3rio da m\u00e1quina local para o diret\u00f3rio /mnt/sen2cor-aux/CCI4SEN2COR do Container. Arquivo de configura\u00e7\u00e3o (Opcional) Volume para a defini\u00e7\u00e3o do arquivo de configura\u00e7\u00e3o L2A_GIPP.xml . O volume criado, deve mapear o arquivo L2A_GIPP.xml da m\u00e1quina local para o arquivo /opt/sen2cor/2.9.0/cfg/L2A_GIPP.xml no Container. Dados SRTM (Opcional) Volume para a especifica\u00e7\u00e3o do diret\u00f3rio com imagens SRTM que devem ser utilizados nas etapas do Sen2Cor. O volume criado, deve mapear um diret\u00f3rio da m\u00e1quina local para o diret\u00f3rio /mnt/sen2cor-aux/srtm do Container. Exemplo de utiliza\u00e7\u00e3o (Docker CLI ) O c\u00f3digo abaixo, apresenta um exemplo de utiliza\u00e7\u00e3o da Docker Image sen2cor atrav\u00e9s da Docker CLI . Nome da imagem No comando apresentado abaixo, a Docker Image sen2cor \u00e9 identificada como marujore/sen2cor:2.9.0 j\u00e1 que esta, est\u00e1 armazenada no perfil de usu\u00e1rio marujore no DockerHub e a vers\u00e3o escolhida \u00e9 a 2.9.0 . Formata\u00e7\u00e3o do comando O comando abaixo \u00e9 criado para ser dit\u00e1dico. Caso voc\u00ea queira utiliza-lo, n\u00e3o esque\u00e7a de substituir os valores e remover os espa\u00e7os entre cada linha. docker run --rm \\ # Volume: Dados de entrada --volume /path/to/input_dir:/mnt/input_dir:ro \\ # Volume: Dados de sa\u00edda --volume /path/to/output_dir:/mnt/output_dir:rw \\ # Dados auxiliares: Diret\u00f3rio CCI4SEN2COR --volume /path/to/CCI4SEN2COR:/mnt/aux_data \\ # Arquivo de configura\u00e7\u00e3o: L2A_GIPP.xml (Opcional) --volume /path/to/L2A_GIPP.xml:/opt/sen2cor/2.9.0/cfg/L2A_GIPP.xml \\ # Dados SRTM (Opcional) --volume /path/to/srtm:/root/sen2cor/2.9/dem/srtm \\ # Especifica\u00e7\u00e3o da Docker Image e cena a ser processada brazildatacube/sen2cor:2.9.0 S2A_MSIL1C_20210903T140021_N0301_R067_T21KVR_20210903T172609.SAFE A execu\u00e7\u00e3o do comando apresentado acima, far\u00e1 a cria\u00e7\u00e3o de um Docker Container sen2cor . Esse Docker Container far\u00e1 o processamento da cena S2A_MSIL1C_20210903T140021_N0301_R067_T21KVR_20210903T172609.SAFE . Neste comando, deve-se notar que, o diret\u00f3rio de entrada ( /path/to/input_dir ) especificado, deve conter um subdiret\u00f3rio com o mesmo nome da cena escolhida, neste caso S2A_MSIL1C_20210903T140021_N0301_R067_T21KVR_20210903T172609.SAFE . Al\u00e9m disso, \u00e9 esperado que nesse subdiret\u00f3rio, todos os dados da cena estejam dispon\u00edveis para o processamento. Para mais informa\u00e7\u00f5es, consulte o reposit\u00f3rio do GitHub , onde tem-se mantido o versionamento das mudan\u00e7as realizadas na Docker Image sen2cor . LaSRC 2.0.1 \u00b6 LaSRC \u00e9 um processador de corre\u00e7\u00e3o atmosf\u00e9rica originalmente proposto para produtos Landsat-8 Collection 1, sendo posteriormente adaptado para ser capaz tamb\u00e9m de corrigir produtos Sentinel-2. Ele recebe como entrada produtos Landsat em N\u00famero Digital (DN, do ingl\u00eas Digital Number ) ou produtos Sentinel-2 em n\u00edvel de radi\u00e2ncia de topo de atmosf\u00e9ra (ToA), tamb\u00e9m chamados Level-1C (L1C). O resultado desse processador consiste em produtos em n\u00edvel reflect\u00e2ncia de superf\u00edcie (SR, do ingl\u00eas Surface Reflectance ). Para facilitar a utiliza\u00e7\u00e3o do LaSRC neste RC , e garantir que a execu\u00e7\u00e3o seja reprodut\u00edvel e reutiliz\u00e1vel, fez-se a cria\u00e7\u00e3o de um Docker Image para o LaSRC, nomeada de lasrc . A lasrc , possui todas as depend\u00eancias e configura\u00e7\u00f5es necess\u00e1rias para a execu\u00e7\u00e3o do processador LaSRC. Os t\u00f3picos a seguir, apresentam as principais caracter\u00edsticas desta Docker Image, como volumes, dados auxiliares necess\u00e1rios e forma de utiliza\u00e7\u00e3o. Dados auxiliares Para a execu\u00e7\u00e3o da lasrc , \u00e9 necess\u00e1rio a defini\u00e7\u00e3o de alguns dados auxiliares. Para obter esses dados, voc\u00ea pode seguir os passos listados abaixo: Acesse: https://edclpdsftp.cr.usgs.gov/downloads/auxiliaries/lasrc_auxiliary/L8/ ; Fa\u00e7a o download de todos os conte\u00fados dispon\u00edveis listados, com exce\u00e7\u00e3o do diret\u00f3rio LADS . Os dados do diret\u00f3rio LADS tamb\u00e9m s\u00e3o requeridos para a utiliza\u00e7\u00e3o do LaSRC, no entanto, esse diret\u00f3rio cont\u00e9m dados di\u00e1rios de 2013 at\u00e9 os dias atuais, o que representa um grande volume de dados. Para tornar o processo mais r\u00e1pido, recomenda-se que seja feito o download apenas dos arquivos LADS das datas que ser\u00e3o processadas. Sele\u00e7\u00e3o de arquivos LADS Cada arquivo LADS refere-se a uma data do ano. Sendo assim, para processar uma imagem do dia 1\u00b0 de Janeiro de 2017, deve-se obter o arquivo LADS L8ANC2017001.hdf_fused em que 2017 representa o ano e o valor 001 a data 1\u00b0 de Janeiro em formato Juliano. Ao final da aquisi\u00e7\u00e3o dos dados auxiliares, o diret\u00f3rio onde os dados foram armazenados deve ter a seguinte estrutura: . \u251c\u2500\u2500 CMGDEM.hdf \u251c\u2500\u2500 LADS \u251c\u2500\u2500 LDCMLUT \u251c\u2500\u2500 MSILUT \u2514\u2500\u2500 ratiomapndwiexp.hdf Volumes Para a utiliza\u00e7\u00e3o da lasrc , \u00e9 necess\u00e1rio a defini\u00e7\u00e3o de alguns Docker Volumes. Esses volumes, especificam os dados de entrada, sa\u00edda e dados auxiliares utilizados pela ferramenta durante o processamento. Abaixo, tem-se um descritivo de todos volumes que devem ser criados durante a execu\u00e7\u00e3o da Docker Image LaSRC: Dados de entrada (Obrigat\u00f3rio) Diret\u00f3rio com os dados de entrada. Esse volume, deve mapear um diret\u00f3rio da m\u00e1quina local para o diret\u00f3rio /mnt/input_dir do Container. Recomenda-se que esse volume seja somente leitura, para garantir que nenhuma modifica\u00e7\u00e3o ser\u00e1 feita nos dados de entrada. Dados de sa\u00edda (Obrigat\u00f3rio) Diret\u00f3rio onde os produtos gerados ser\u00e3o armazenados. O volume criado, deve mapear um diret\u00f3rio da m\u00e1quina local para o diret\u00f3rio /mnt/output_dir do Container. Dados auxiliares (Obrigat\u00f3rio) Diret\u00f3rio com os dados auxiliares necess\u00e1rio para o funcionamento do LaSRC. O volume criado, deve mapear um diret\u00f3rio da m\u00e1quina local para o diret\u00f3rio /mnt/atmcor_aux/lasrc/L8 do Container. Exemplo de utiliza\u00e7\u00e3o (Docker CLI ) Os c\u00f3digos abaixo apresentam dois exemplos de utiliza\u00e7\u00e3o do lasrc , atrav\u00e9s da Docker CLI . No primeiro exemplo, faz-se o processamento de uma cena Landsat-8/OLI, enquanto no segundo \u00e9 processado uma cena Sentinel-2/MSI. Nome da imagem Nos comandos apresentados abaixo, a Docker Image lasrc \u00e9 identificada como marujore/lasrc:2.0.1 j\u00e1 que esta, est\u00e1 armazenada no perfil de usu\u00e1rio marujore no DockerHub e a vers\u00e3o escolhida \u00e9 a 2.0.1 . Formata\u00e7\u00e3o do comando O comando abaixo \u00e9 criado para ser dit\u00e1dico. Caso voc\u00ea queira utiliza-lo, n\u00e3o esque\u00e7a de substituir os valores e remover os espa\u00e7os entre cada linha. Exemplo para dados Landsat-8/OLI docker run --rm \\ # Volume: Dados de entrada --volume /path/to/input/:/mnt/input-dir:rw \\ # Volume: Dados de sa\u00edda --volume /path/to/output:/mnt/output-dir:rw \\ # Dados auxiliares (Dados L8/LADS) --volume /path/to/lasrc_auxiliaries/L8:/mnt/atmcor_aux/lasrc/L8:ro \\ # Especifica\u00e7\u00e3o da Docker Image e cena a ser processada --tty marujore/lasrc:2.0.1 LC08_L1TP_220069_20190112_20190131_01_T1 Exemplo para dados Sentinel-2/MSI docker run --rm \\ # Volume: Dados de entrada --volume /path/to/input/:/mnt/input-dir:rw \\ # Volume: Dados de sa\u00edda --volume /path/to/output:/mnt/output-dir:rw \\ # Dados auxiliares (Dados L8/LADS) --volume /path/to/lasrc_auxiliaries/L8:/mnt/atmcor_aux/lasrc/L8:ro \\ # Especifica\u00e7\u00e3o da Docker Image e cena a ser processada --tty marujore/lasrc:2.0.1 S2A_MSIL1C_20190105T132231_N0207_R038_T23LLF_20190105T145859.SAFE Como pode-se notar, a diferen\u00e7a para o uso da lasrc para os dados dos diferentes sat\u00e9lite-sensor, est\u00e1 apenas na especifica\u00e7\u00e3o do nome da cena. Deve-se notar tamb\u00e9m que, \u00e9 esperado, para ambos os casos que, no diret\u00f3rio de entrada ( /path/to/input/ ) tenha subdiret\u00f3rios com as cenas espec\u00edficas, neste caso LC08_L1TP_220069_20190112_20190131_01_T1 e S2A_MSIL1C_20190105T132231_N0207_R038_T23LLF_20190105T145859.SAFE . Al\u00e9m disso, \u00e9 esperado que nesses subdiret\u00f3rios, todos os dados das cenas estejam dispon\u00edveis para o processamento. Para mais informa\u00e7\u00f5es, consulte o reposit\u00f3rio do GitHub , onde tem-se mantido o versionamento das mudan\u00e7as realizadas na Docker Image lasrc . L8Angs \u00b6 Landsat Ang Tool \u00e9 uma ferramenta desenvolvida e mantida pela U nited S tates G eological S urvey . A ferramenta \u00e9 capaz de utilizar arquivos ANG.txt fornecidos junto \u00e0 produtos Landsat-8 para gerar bandas de angulos por pixel , no caso os \u00e2ngulos solar azimutal ( SAA ), solar zenital ( SZA ), sensor azimutal ( VAA ) e sensor zenital ( VZA ). As bandas s\u00e3o geradas com a mesma resolu\u00e7\u00e3o espacial das bandas espectrais do sensor OLI acoplados ao sat\u00e9lite Landsat-8. Mais sobre o Landsat Ang Tool Para informa\u00e7\u00f5es detalhadas sobre o Landsat Ang Tool, consulte o site oficial da USGS sobre a ferramenta . Neste RC , as imagens Landsat-8/OLI (Collection-2) s\u00e3o obtidas j\u00e1 processadas em n\u00edvel de reflect\u00e2ncia de superf\u00edcie (L2). Entretanto, para processamentos posteriores, \u00e9 necess\u00e1rio a gera\u00e7\u00e3o das bandas de \u00e2ngulos. Neste caso, faz-se o uso do Landsat Ang Tool . A instala\u00e7\u00e3o e configura\u00e7\u00e3o do Landsat Ang Tool pode tornar dif\u00edcil a reprodu\u00e7\u00e3o e replica\u00e7\u00e3o no futuro. Sendo assim, para facilitar as opera\u00e7\u00f5es deste RC que utilizam essa ferramenta, fez-se a cria\u00e7\u00e3o de uma Docker Image espec\u00edfica para ela, nomeada de l8angs . Os t\u00f3picos a seguir, apresentam as principais caracter\u00edsticas desta Docker Image, como volumes e dados auxiliares necess\u00e1rios para a execu\u00e7\u00e3o. Tamb\u00e9m s\u00e3o apresentados exemplos de utiliza\u00e7\u00e3o atrav\u00e9s da Docker CLI . Volumes Para a utiliza\u00e7\u00e3o da l8angs , \u00e9 necess\u00e1rio a defini\u00e7\u00e3o do seguinte volume, durante a execu\u00e7\u00e3o: Dados de entrada (Obrigat\u00f3rio) Diret\u00f3rio com os dados de entrada. O volume criado, deve mapear um diret\u00f3rio da m\u00e1quina local para o diret\u00f3rio /mnt/input-dir do Container. Os dados gerados s\u00e3o criados no mesmo diret\u00f3rio de entrada, sendo este o comportamento padr\u00e3o da ferramenta. Exemplo de utiliza\u00e7\u00e3o (Docker CLI ) O c\u00f3digo abaixo, apresenta um exemplo de utiliza\u00e7\u00e3o da Docker Image l8angs atrav\u00e9s da Docker CLI . Nome da imagem Nos comandos apresentados abaixo, a Docker Image l8angs \u00e9 identificada como marujore/landsat-angles:2.0.1 j\u00e1 que esta, est\u00e1 armazenada no perfil de usu\u00e1rio marujore no DockerHub e a vers\u00e3o escolhida \u00e9 a latest . Formata\u00e7\u00e3o do comando O comando abaixo \u00e9 criado para ser dit\u00e1dico. Caso voc\u00ea queira utiliza-lo, n\u00e3o esque\u00e7a de substituir os valores e remover os espa\u00e7os entre cada linha. docker run --rm \\ # Volume: Dados de entrada -v /path/to/input/:/mnt/input-dir:rw \\ # Especifica\u00e7\u00e3o da Docker Image e cena a ser processada marujore/landsat-angles:latest LC08_L2SP_222081_20190502_20200829_02_T1 A execu\u00e7\u00e3o do comando apresentado acima, far\u00e1 a cria\u00e7\u00e3o de um Docker Container l8angs . Esse Docker Container far\u00e1 o processamento da cena LC08_L2SP_222081_20190502_20200829_02_T1 . Neste comando, deve-se notar que, o diret\u00f3rio de entrada ( /path/to/input/ ) especificado, deve conter um subdiret\u00f3rio com o mesmo nome da cena escolhida, neste caso LC08_L2SP_222081_20190502_20200829_02_T1 . Al\u00e9m disso, \u00e9 esperado que nesse subdiret\u00f3rio, todos os dados da cena estejam dispon\u00edveis para o processamento. Para mais informa\u00e7\u00f5es, consulte o reposit\u00f3rio do GitHub , onde tem-se mantido o versionamento das mudan\u00e7as realizadas na Docker Image l8angs . Sensor Harm \u00b6 Neste RC , as imagens Landsat-8 Collection-2 j\u00e1 obtidas em n\u00edvel de reflect\u00e2ncia de superf\u00edcie (L2) e as imagens Sentinel-2 (processadas para reflect\u00e2ncia de superf\u00edcie tanto utilizando Sen2cor quanto LaSRC) s\u00e3o harmonizadas utilizando a biblioteca sensor-harm . Para potencializar a reprodu\u00e7\u00e3o e replica\u00e7\u00e3o no uso dessa ferramenta, fez-se a cria\u00e7\u00e3o da Docker Image sensor-harm . Nesta Image, est\u00e3o dispon\u00edveis todas as depend\u00eancias e configura\u00e7\u00f5es necess\u00e1rias para a execu\u00e7\u00e3o do sensor-harm . Os t\u00f3picos a seguir, apresentam as principais caracter\u00edsticas desta Docker Image, volumes requeridos e exemplos de utiliza\u00e7\u00e3o. Volumes Para a utiliza\u00e7\u00e3o da sensor-harm , \u00e9 necess\u00e1rio a defini\u00e7\u00e3o de alguns Docker Volumes. Esses volumes, especificam os dados de entrada e dados auxiliares utilizados pelo sensor-harm. Abaixo, esses volumes s\u00e3o listados e descritos: Dados de entrada (Obrigat\u00f3rio) Diret\u00f3rio com os dados de entrada. Esse volume, deve mapear um diret\u00f3rio da m\u00e1quina local para o diret\u00f3rio /mnt/input-dir do Container. Recomenda-se que esse volume seja somente leitura, para garantir que nenhuma modifica\u00e7\u00e3o ser\u00e1 feita nos dados de entrada. Dados de sa\u00edda (Obrigat\u00f3rio) Diret\u00f3rio onde os produtos gerados ser\u00e3o armazenados. O volume criado, deve mapear um diret\u00f3rio da m\u00e1quina local para o diret\u00f3rio /mnt/output-dir do Container. Diret\u00f3rio de \u00e2ngulos (Obrigat\u00f3rio apenas para dados Landsat-8/OLI) Diret\u00f3rio com os dados de \u00e2ngulos da cena que ser\u00e1 processada. O volume criado, deve mapear um diret\u00f3rio da m\u00e1quina local para o diret\u00f3rio /mnt/angles-dir do Container. Recomenda-se que esse volume seja somente leitura, para garantir que nenhuma modifica\u00e7\u00e3o ser\u00e1 feita nos dados durante o processamento. Exemplo de utiliza\u00e7\u00e3o (Docker CLI ) Os c\u00f3digos abaixo apresentam dois exemplos de utiliza\u00e7\u00e3o do sensor-harm , atrav\u00e9s da Docker CLI . No primeiro exemplo, faz-se o processamento de uma cena Landsat-8/OLI, enquanto no segundo \u00e9 processado uma cena Sentinel-2/MSI. Nome da imagem Nos comandos apresentados abaixo, a Docker Image sensor-harm \u00e9 identificada como marujore/sensor-harm:latest j\u00e1 que esta, est\u00e1 armazenada no perfil de usu\u00e1rio marujore no DockerHub e a vers\u00e3o escolhida \u00e9 a latest . Formata\u00e7\u00e3o do comando O comando abaixo \u00e9 criado para ser dit\u00e1dico. Caso voc\u00ea queira utiliza-lo, n\u00e3o esque\u00e7a de substituir os valores e remover os espa\u00e7os entre cada linha. Exemplo para dados Landsat-8/OLI docker run --rm \\ # Volume: Dados de entrada --volume /path/to/input/:/mnt/input-dir:ro \\ # Volume: Dados de sa\u00edda --volume /path/to/output:/mnt/output-dir:rw \\ # Diret\u00f3rio de \u00e2ngulos (Apenas Landsat-8/OLI) --volume /path/to/angles:/mnt/angles-dir:ro \\ # Especifica\u00e7\u00e3o da Docker Image e cena a ser processada --tty marujore/sensor-harm:latest LC08_L1TP_220069_20190112_20190131_01_T1 Exemplo para dados Sentinel-2/MSI docker run --rm \\ # Volume: Dados de entrada --volume /path/to/input/:/mnt/input-dir:ro \\ # Volume: Dados de sa\u00edda --volume /path/to/output:/mnt/output-dir:rw \\ # Diret\u00f3rio de \u00e2ngulos (Apenas Landsat-8/OLI) --volume /path/to/angles:/mnt/angles-dir:ro \\ # Especifica\u00e7\u00e3o da Docker Image e cena a ser processada --tty marujore/sensor-harm:latest S2A_MSIL1C_20190105T132231_N0207_R038_T23LLF_20190105T145859.SAFE Como pode-se notar, a diferen\u00e7a para o uso da sensor-harm para os dados dos diferentes sat\u00e9lite-sensor, est\u00e1 apenas na especifica\u00e7\u00e3o do nome da cena. Deve-se notar tamb\u00e9m que, \u00e9 esperado, para ambos os casos que, no diret\u00f3rio de entrada ( /path/to/input/ ) tenha subdiret\u00f3rios com as cenas espec\u00edficas, neste caso LC08_L1TP_220069_20190112_20190131_01_T1 e S2A_MSIL1C_20190105T132231_N0207_R038_T23LLF_20190105T145859.SAFE . Al\u00e9m disso, \u00e9 esperado que nesses subdiret\u00f3rios, todos os dados das cenas estejam dispon\u00edveis para o processamento. Para mais informa\u00e7\u00f5es, consulte o reposit\u00f3rio do GitHub , onde tem-se mantido o versionamento das mudan\u00e7as realizadas na Docker Image sensor-harm . Docker Images para Scripts de processamento \u00b6 Conforme apresentado na Se\u00e7\u00e3o Scripts de processamento , foram criados duas formas de realizar a execu\u00e7\u00e3o da metodologia dos experimentos deste RC . Uma utiliza o Jupyter Notebook , sendo voltada para o processamento interativo dos c\u00f3digos. A segunda op\u00e7\u00e3o, por outro lado, utiliza o Dagster e facilita execu\u00e7\u00f5es em lote e controle de erro. Para facilitar a utiliza\u00e7\u00e3o de ambas as abordagens, fez-se a cria\u00e7\u00e3o de Docker Images com os ambientes necess\u00e1rios \u00e0 execu\u00e7\u00e3o de cada um deles. Com isso, evita-se que depend\u00eancias tenham de ser instaladas ou configuradas para a execu\u00e7\u00e3o dos scripts de processamento . Os t\u00f3picos a seguir, apresentam as principais caracter\u00edsticas dessas Docker Images, volumes requeridos, configura\u00e7\u00e3o e exemplos de utiliza\u00e7\u00e3o. Funcionamento dos scripts Caso voc\u00ea esteja interessado em reutilizar essas Docker Images, recomenda-se antes a leitura do modo de funcionamento dos scripts de processamento , bem como das bibliotecas de software utilizadas por esses scripts . Jupyter Notebook \u00b6 Para a execu\u00e7\u00e3o da vers\u00e3o Jupyter Notebook, fez-se a cria\u00e7\u00e3o da Docker Image research-processing-jupyter . Essa Docker Image disponibiliza uma interface JupyterLab , com as depend\u00eancias Python necess\u00e1rias para a execu\u00e7\u00e3o dos scripts . Al\u00e9m disso, nessa Docker Image, tem-se tamb\u00e9m o Docker instalado, para que os scripts sejam capazes de operar e criar outros Docker Containers de processamento. Base do ambiente A cria\u00e7\u00e3o da research-processing-jupyter , foi realizada utilizando como base a Docker Image jupyter/minimal-notebook , disponibilizada pelo time de desenvolvimento do projeto Jupyter . Com isso, todas as configura\u00e7\u00f5es e vari\u00e1veis de ambientes dispon\u00edveis na Docker Image jupyter/minimal-notebook tamb\u00e9m s\u00e3o aplic\u00e1veis a research-processing-jupyter . Nos t\u00f3picos abaixo, \u00e9 feita a apresenta\u00e7\u00e3o das configura\u00e7\u00f5es necess\u00e1rias para utilizar essa Docker Image. Exemplos de utiliza\u00e7\u00e3o com a CLI do Docker e Docker Compose , tamb\u00e9m ser\u00e3o apresentados. Vari\u00e1veis de ambiente Para a utiliza\u00e7\u00e3o da research-processing-jupyter , \u00e9 necess\u00e1rio definir a seguinte vari\u00e1vel de ambiente: DATA_DIRECTORY (Obrigat\u00f3rio) Vari\u00e1vel de ambiente para determinar o diret\u00f3rio, na m\u00e1quina local, onde os dados baixados devem ser salvos. Volumes A execu\u00e7\u00e3o do research-processing-jupyter deve ser realizada com a defini\u00e7\u00e3o de dois volumes: Volume de dados (Obrigat\u00f3rio) Volume onde os dados ser\u00e3o armazenados. Seguindo o modelo de execu\u00e7\u00e3o das fun\u00e7\u00f5es de processamento utilizadas nos scripts , esse volume ser\u00e1 utilizado por fun\u00e7\u00f5es dentro do mesmo container ( Local ) ou em outros containers ( Containerized ). Desta forma, a defini\u00e7\u00e3o desse volume deve ser feita de modo a atender duas exig\u00eancias: O volume deve ser do tipo Bind Mount ; O mapeamento do volume ( Bind Mount ) deve utilizar, na m\u00e1quina local e no Container, o mesmo caminho definido na vari\u00e1vel DATA_DIRECTORY . Com essas defini\u00e7\u00f5es, o volume ser\u00e1 vis\u00edvel dentro do Container da research-processing-jupyter e tamb\u00e9m pelos Containers auxiliares de processamentos que forem gerados durante a execu\u00e7\u00e3o dos scripts de processamento . Volume Daemon Socket (Obrigat\u00f3rio) Para que os scripts sejam capazes de gerar Docker Containers de processamento, \u00e9 necess\u00e1rio a defini\u00e7\u00e3o do Daemon Socket como um volume. Ao fazer isso, o Docker dentro do container criado com a imagem research-processing-jupyter \u00e9 capaz de interagir com o Docker da m\u00e1quina local, permitindo que Containers de processamento sejam criados. Defini\u00e7\u00e3o de usu\u00e1rio De forma complementar a defini\u00e7\u00e3o do Daemon Socket volume , para a execu\u00e7\u00e3o da research-processing-jupyter , \u00e9 necess\u00e1rio especificar um usu\u00e1rio ( UID ) e grupo ( GID ) da m\u00e1quina local que tenham permiss\u00e3o para interagir com o Docker Daemon. Esses valores, ser\u00e3o aplicados ao usu\u00e1rio padr\u00e3o do Container, de modo que o Docker permita que ele tamb\u00e9m interaga com o Docker Daemon da m\u00e1quina local. Permiss\u00f5es no Docker Caso voc\u00ea esteja interessado em entender os detalhes do motivo por tr\u00e1s da defini\u00e7\u00e3o dos usu\u00e1rios, recomendamos que voc\u00ea consulte a documenta\u00e7\u00e3o oficial do Docker . Para que voc\u00ea fa\u00e7a a defini\u00e7\u00e3o do usu\u00e1rio, no momento da execu\u00e7\u00e3o da research-processing-jupyter , voc\u00ea pode utilizar o par\u00e2metro --user . Caso voc\u00ea deseje utilizar o Docker Compose, o campo user pode ser utilizado para essa defini\u00e7\u00e3o. Exemplo de utiliza\u00e7\u00e3o (Docker CLI ) Abaixo, faz-se a apresenta\u00e7\u00e3o de um exemplo de uso da Docker Image research-processing-jupyter atrav\u00e9s da Docker CLI : Formata\u00e7\u00e3o do comando O comando abaixo \u00e9 criado para ser dit\u00e1dico. Caso voc\u00ea queira utiliza-lo, n\u00e3o esque\u00e7a de substituir os valores e remover os espa\u00e7os entre cada linha. docker run \\ --name research-processing-jupyter \\ # Defini\u00e7\u00e3o do usu\u00e1rio --user ${ UID } : ${ GID } \\ # Vari\u00e1veis de ambiente --env JUPYTER_ENABLE_LAB = yes \\ # Ativando JupyterLab --env DATA_DIRECTORY = /my-data-dir \\ # Volume: Volume de dados --volume /my-data-dir:/my-data-dir \\ # Volume: Volume Daemon Socket --volume /var/run/docker.sock:/var/run/docker.sock:ro \\ # Porta de rede por onde o servi\u00e7o poder\u00e1 ser acessado --publish 8888 :8888 \\ # Especifica\u00e7\u00e3o da Docker Image brazildatacube/research-processing-jupyter:latest Defini\u00e7\u00e3o do usu\u00e1rio Para a defini\u00e7\u00e3o do usu\u00e1rio, utilizando vari\u00e1veis de ambiente ( ${UID} e ${GID} ), como realizado no comando acima, antes de executar o comando Docker, utilize os seguintes comandos: export UID = ` id -u $USER ` export GID = ` cut -d: -f3 < < ( getent group docker ) ` Ap\u00f3s a execu\u00e7\u00e3o do comando acima, dever\u00e1 ser produzir um resultado parecido com o apresentado abaixo: # (Omitted) [ I 2022 -04-30 19 :22:50.684 ServerApp ] Use Control-C to stop this server and shut down all kernels ( twice to skip confirmation ) . [ C 2022 -04-30 19 :22:50.694 ServerApp ] To access the server, open this file in a browser: file:///home/jovyan/.local/share/jupyter/runtime/jpserver-7-open.html Or copy and paste one of these URLs: http://ae88466ccb18:8888/lab?token = 0497e15e042d52cfb498a2edf3d2c6e5874e79b4808ca860 or http://127.0.0.1:8888/lab?token = 0497e15e042d52cfb498a2edf3d2c6e5874e79b4808ca860 Ap\u00f3s a execu\u00e7\u00e3o deste comando, utilizando um navegador e acesse o endere\u00e7o do JupyterLab apresentado (Substitua o endere\u00e7o abaixo pelo que foi exibido em seu terminal): firefox http://127.0.0.1:8888/lab?token = 0497e15e042d52cfb498a2edf3d2c6e5874e79b4808ca860 Exemplo de utiliza\u00e7\u00e3o (Docker Compose) Abaixo, o mesmo exemplo de execu\u00e7\u00e3o feito com a Docker CLI \u00e9 realizado com Docker Compose . Para isso, primeiro fez-se a cria\u00e7\u00e3o do arquivo docker-compose.yml com o seguinte conte\u00fado: docker-compose.yml version: '3.2' services: my-notebook: # Defini\u00e7\u00e3o do usu\u00e1rio user: ${UID}:${GID} image: brazildatacube/research-processing-jupyter:latest environment: # Vari\u00e1veis de ambiente - JUPYTER_ENABLE_LAB=yes - DATA_DIRECTORY=/my-data-dir volumes: # Volume: Volume de dados - type: bind source: /my-data-dir target: /my-data-dir # Volume: Volume Daemon Socket - type: bind source: /var/run/docker.sock target: /var/run/docker.sock ports: # Porta de rede por onde o servi\u00e7o poder\u00e1 ser acessado - \"8888:8888\" Defini\u00e7\u00e3o do usu\u00e1rio Para a defini\u00e7\u00e3o do usu\u00e1rio, utilizando vari\u00e1veis de ambiente ( ${UID} e ${GID} ), como apresentado no docker-compose.yml , antes de executar o comando docker-compose , utilize os seguintes comandos: export UID = ` id -u $USER ` export GID = ` cut -d: -f3 < < ( getent group docker ) ` Com o arquivo criado, fez-se a execu\u00e7\u00e3o do compose: docker-compose -f docker-compose.yml up A sa\u00edda do comando acima deve ser parecida com: # (Omitted) [ I 2022 -04-30 19 :23:57.260 ServerApp ] http://afd0fe2755a7:8888/lab?token = 6d37701a6787bd58a7f92f29f33709970df11a742bf3b79b [ I 2022 -04-30 19 :23:57.260 ServerApp ] or http://127.0.0.1:8888/lab?token = 6d37701a6787bd58a7f92f29f33709970df11a742bf3b79b [ I 2022 -04-30 19 :23:57.260 ServerApp ] Use Control-C to stop this server and shut down all kernels ( twice to skip confirmation ) . [ C 2022 -04-30 19 :23:57.264 ServerApp ] To access the server, open this file in a browser: file:///home/jovyan/.local/share/jupyter/runtime/jpserver-8-open.html Or copy and paste one of these URLs: http://afd0fe2755a7:8888/lab?token = 6d37701a6787bd58a7f92f29f33709970df11a742bf3b79b or http://127.0.0.1:8888/lab?token = 6d37701a6787bd58a7f92f29f33709970df11a742bf3b79b Com esta informa\u00e7\u00e3o, pode-se realizar o acesso ao JupyterLab no navegador. Para isso, abra o link exibido em seu terminal em um navegador: firefox http://127.0.0.1:8888/lab?token = 6d37701a6787bd58a7f92f29f33709970df11a742bf3b79b Dagster \u00b6 Para a execu\u00e7\u00e3o da vers\u00e3o Dagster, fez-se a cria\u00e7\u00e3o da Docker Image research-processing-dagster . Essa Docker Image possui o Dagster (vers\u00e3o 0.12.15 ), junto ao DagIt , uma interface web para configurar e interagir com o Dagster. Tem-se tamb\u00e9m uma instala\u00e7\u00e3o do Docker, para que os scripts sejam capazes de operar e criar outros Docker Containers de processamento. Vari\u00e1veis de ambiente Para a utiliza\u00e7\u00e3o da research-processing-dagster , \u00e9 necess\u00e1rio definir a seguinte vari\u00e1vel de ambiente: DATA_DIRECTORY (Obrigat\u00f3rio) Vari\u00e1vel de ambiente para determinar o diret\u00f3rio, na m\u00e1quina local, onde os dados baixados devem ser salvos. Volumes A execu\u00e7\u00e3o do research-processing-dagster deve ser realizada com a defini\u00e7\u00e3o dos seguintes volumes Docker volumes : Volume de dados (Obrigat\u00f3rio) Volume onde os dados ser\u00e3o armazenados. Seguindo o modelo de execu\u00e7\u00e3o das fun\u00e7\u00f5es de processamento utilizadas nos scripts , esse volume ser\u00e1 utilizado por fun\u00e7\u00f5es dentro do mesmo container ( Local ) ou em outros containers ( Containerized ). Desta forma, a defini\u00e7\u00e3o desse volume deve ser feita de modo a atender duas exig\u00eancias: O volume deve ser do tipo Bind Mount ; O mapeamento do volume ( Bind Mount ) deve utilizar, na m\u00e1quina local e no Container, o mesmo caminho definido na vari\u00e1vel DATA_DIRECTORY . Com essas defini\u00e7\u00f5es, o volume ser\u00e1 vis\u00edvel dentro do Container da research-processing-dagster e tamb\u00e9m pelos Containers auxiliares de processamentos. Volume Daemon Socket Para que os scripts sejam capazes de gerar Docker Containers de processamento, \u00e9 necess\u00e1rio a defini\u00e7\u00e3o do Daemon Socket como um volume. Ao fazer isso, o Docker dentro do container criado com a imagem research-processing-dagster \u00e9 capaz de interagir com o Docker da m\u00e1quina externa, permitindo que containers de processamento sejam criados. Exemplo de utiliza\u00e7\u00e3o (Docker CLI ) Abaixo, faz-se a apresenta\u00e7\u00e3o de um exemplo de uso da Docker Image research-processing-dagster atrav\u00e9s da Docker CLI : Formata\u00e7\u00e3o do comando O comando abaixo \u00e9 criado para ser dit\u00e1dico. Caso voc\u00ea queira utiliza-lo, n\u00e3o esque\u00e7a de substituir os valores e remover os espa\u00e7os entre cada linha. docker run \\ --name research-processing-dagster \\ # Vari\u00e1veis de ambiente --env DATA_DIRECTORY = /my-data-dir \\ # Volume: Volume de dados --volume /my-data-dir:/my-data-dir \\ # Volume: Volume Daemon Socket --volume /var/run/docker.sock:/var/run/docker.sock:ro \\ # Porta de rede por onde o servi\u00e7o poder\u00e1 ser acessado --publish 3000 :3000 \\ # Especifica\u00e7\u00e3o da Docker Image brazildatacube/research-processing-dagster:latest Ap\u00f3s a execu\u00e7\u00e3o do comando acima, um resultado parecido com o apresentado abaixo deve aparecer: # (Omitted) Welcome to Dagster! If you have any questions or would like to engage with the Dagster team, please join us on Slack ( https://bit.ly/39dvSsF ) . Serving on http://0.0.0.0:3000 in process 1 Ap\u00f3s a execu\u00e7\u00e3o deste comando, utilizando um navegador e acesse o endere\u00e7o do DagIt apresentado: firefox http://127.0.0.1:3000 Exemplo de utiliza\u00e7\u00e3o (Docker Compose) Abaixo, o mesmo exemplo de execu\u00e7\u00e3o feito com a Docker CLI \u00e9 realizado com Docker Compose . Para isso, primeiro fez-se a cria\u00e7\u00e3o do arquivo docker-compose.yml com o seguinte conte\u00fado: docker-compose.yml version: '3.2' services: my-dagster: image: brazildatacube/research-processing-dagster:latest environment: # Vari\u00e1veis de ambiente - DATA_DIRECTORY=/my-data-dir volumes: # Volume: Volume de dados - type: bind source: /my-data-dir target: /my-data-dir # Volume: Volume Daemon Socket - type: bind source: /var/run/docker.sock target: /var/run/docker.sock ports: # Porta de rede por onde o servi\u00e7o poder\u00e1 ser acessado - \"3000:3000\" Com o arquivo criado, fez-se a execu\u00e7\u00e3o do compose: docker-compose -f docker-compose.yml up A sa\u00edda do comando acima \u00e9 parecida com: # (Omitted) Welcome to Dagster! If you have any questions or would like to engage with the Dagster team, please join us on Slack ( https://bit.ly/39dvSsF ) . Serving on http://0.0.0.0:3000 in process 1 Agora, utilizando como base o endere\u00e7o http://0.0.0.0:3000 exibido na tela, acesse o Dagster em seu navegador: firefox http://127.0.0.1:3000 Example toolkit environment \u00b6 Para facilitar a utiliza\u00e7\u00e3o do Example toolkit , fez-se a cria\u00e7\u00e3o da Docker Image example-toolkit-docker . Esta Docker Image, possui todas as depend\u00eancias necess\u00e1rias para a execu\u00e7\u00e3o do Example toolkit . Nos t\u00f3picos abaixo, \u00e9 feita a apresenta\u00e7\u00e3o das configura\u00e7\u00f5es necess\u00e1rias para utilizar essa Docker Image. Exemplos de utiliza\u00e7\u00e3o com a CLI do Docker e Docker Compose , tamb\u00e9m ser\u00e3o apresentados. Vari\u00e1veis de ambiente Na utiliza\u00e7\u00e3o do Example toolkit , toda a configura\u00e7\u00e3o \u00e9 feita atrav\u00e9s de vari\u00e1veis de ambiente. Na example-toolkit-docker , manteve-se o mesmo padr\u00e3o. Sendo assim, antes de realizar a execu\u00e7\u00e3o dessa Image, deve-se definir as vari\u00e1veis de ambiente obrigat\u00f3rias. As mesmas vari\u00e1veis de ambiente v\u00e1lidas para o Example toolkit s\u00e3o v\u00e1lidas para a example-toolkit-docker . Para verificar a lista completa das vari\u00e1veis de ambiente do Example toolkit , juntamente com a explica\u00e7\u00e3o de cada uma delas, consulte a Se\u00e7\u00e3o Example toolkit - Utiliza\u00e7\u00e3o . Volumes Para a utiliza\u00e7\u00e3o da example-toolkit-docker , \u00e9 necess\u00e1rio a defini\u00e7\u00e3o de alguns Docker Volumes. Esses volumes, especificam os dados de entrada, sa\u00edda, configura\u00e7\u00f5es e dados auxiliares. Abaixo, tem-se a apresenta\u00e7\u00e3o de cada um desses volumes: Volume de dados (Obrigat\u00f3rio) Diret\u00f3rio onde os dados baixados ser\u00e3o armazenados. Esse volume precisa ser criado no mesmo diret\u00f3rio definido na vari\u00e1vel de ambiente DOWNLOAD_OUTPUT_DIRECTORY (Configura\u00e7\u00e3o do Example toolkit ). Volume de configura\u00e7\u00e3o Dagster (Obrigat\u00f3rio) Diret\u00f3rio onde o arquivo de configura\u00e7\u00e3o Dagster gerado ser\u00e1 salvo. Esse volume precisa ser criado no mesmo diret\u00f3rio definido na vari\u00e1vel de ambiente PIPELINE_DIR (Configura\u00e7\u00e3o do Example toolkit ). Volume de configura\u00e7\u00e3o do download (Obrigat\u00f3rio) Arquivo de configura\u00e7\u00e3o com a refer\u00eancia aos dados que precisam ser baixado. O arquivo definido nesse volume deve ser o mesmo especificado na vari\u00e1vel DOWNLOAD_REFERENCE_FILE (Configura\u00e7\u00e3o do Example toolkit ). Exemplo de utiliza\u00e7\u00e3o (Docker CLI ) Abaixo, faz-se a execu\u00e7\u00e3o da Docker Image, identificada com a tag example-toolkit-docker : Formata\u00e7\u00e3o do comando O comando abaixo \u00e9 criado para ser dit\u00e1dico. Caso voc\u00ea queira utiliza-lo, n\u00e3o esque\u00e7a de substituir os valores e remover os espa\u00e7os entre cada linha. docker run \\ --name example-toolkit-docker \\ # Vari\u00e1veis de ambiente --env RAW_DATA_DIR = /compendium/data/raw_data \\ --env DERIVED_DATA_DIR = /compendium/data/derived_data \\ --env PIPELINE_DIR = /compendium/config \\ --env DOWNLOAD_OUTPUT_DIRECTORY = /compendium/data \\ --env DOWNLOAD_REFERENCE_FILE = /compendium/config/example-toolkit.json \\ # Volume: Volume de dados --volume /my-data/dir:/compendium/data \\ # Volume: Volume de configura\u00e7\u00e3o Dagster --volume /my-dagster/dir:/compendium/config \\ # Volume: Volume de configura\u00e7\u00e3o do download --volume /my-toolkit/config.json:/compendium/config/example-toolkit.json \\ # Especifica\u00e7\u00e3o da Docker Image brazildatacube/example-toolkit-docker:latest Ap\u00f3s a execu\u00e7\u00e3o do comando acima, dever\u00e1 ser produzir um resultado parecido com o apresentado abaixo: # (Omitted) 2022 -04-30 14 :59:16.525 | INFO | pipeline_steps:download_data_files_from_github:59 - Downloading minimal-example_landsat8_data.zip ( Sometimes the progress bar may not appear. Please, be patient. ) 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 2 .05G/2.05G [ 03 :43< 00 :00, 9 .17MB/s ] 2022 -04-30 15 :03:32.059 | INFO | pipeline_steps:download_data_files_from_github:59 - Downloading minimal-example_lasrc_auxiliary_data.zip ( Sometimes the progress bar may not appear. Please, be patient. ) 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 341M/341M [ 00 :35< 00 :00, 9 .57MB/s ] 2022 -04-30 15 :04:44.977 | INFO | pipeline_steps:download_data_files_from_github:59 - Downloading minimal-example_scene_id_list.zip ( Sometimes the progress bar may not appear. Please, be patient. ) 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 2 .17k/2.17k [ 00 :00< 00 :00, 1 .16MB/s ] 2022 -04-30 15 :04:45.690 | INFO | pipeline_steps:download_data_files_from_github:59 - Downloading minimal-example_sentinel2_data.zip ( Sometimes the progress bar may not appear. Please, be patient. ) 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 1 .14G/1.14G [ 02 :12< 00 :00, 8 .59MB/s ] 2022 -04-30 15 :07:15.765 | INFO | pipeline_steps:download_data_files_from_github:92 - All files are downloaded. Exemplo de utiliza\u00e7\u00e3o (Docker Compose) Abaixo, o mesmo exemplo de execu\u00e7\u00e3o feito com a Docker CLI \u00e9 realizado com Docker Compose . Para isso, primeiro fez-se a cria\u00e7\u00e3o do arquivo docker-compose.yml com o seguinte conte\u00fado: docker-compose.yml version: '3.2' services: my-dagster: # Especifica\u00e7\u00e3o da Docker Image image: brazildatacube/example-toolkit-docker:latest environment: # Vari\u00e1veis de ambiente - RAW_DATA_DIR=/compendium/data/raw_data - DERIVED_DATA_DIR=/compendium/data/derived_data - PIPELINE_DIR=/compendium/config - DOWNLOAD_OUTPUT_DIRECTORY=/compendium/data - DOWNLOAD_REFERENCE_FILE=/compendium/config/example-toolkit.json volumes: # Volume: Volume de dados - type: bind source: /my-data/dir target: /compendium/data # Volume: Volume de configura\u00e7\u00e3o Dagster - type: bind source: /my-dagster/dir target: /compendium/config # Volume: Volume de configura\u00e7\u00e3o do download - type: bind source: /my-toolkit/config.json target: /compendium/config/example-toolkit.json ports: # Porta de rede por onde o servi\u00e7o poder\u00e1 ser acessado - \"3000:3000\" Com o arquivo criado, fez-se a execu\u00e7\u00e3o do compose: docker-compose -f docker-compose.yml up A sa\u00edda do comando acima \u00e9 parecida com: # (Omitted) Welcome to Dagster! If you have any questions or would like to engage with the Dagster team, please join us on Slack ( https://bit.ly/39dvSsF ) . Serving on http://0.0.0.0:3000 in process 1 M\u00e1quina virtual com Vagrant \u00b6 Todos os recursos disponibilizados neste RC , foram desenvolvidos, testados e utilizados em ambiente Linux . Especificamente, fez-se o uso do Ubuntu 20.04 . Testes com o Ubuntu 20.10 tamb\u00e9m foram realizados. Em teoria, os c\u00f3digos executados, podem ser adaptados e utilizados em outros sistemas operacionais, como Windows e MacOS . No entanto, \u00e9 importante notar que, n\u00e3o existe uma garantia de que todos os comandos, configura\u00e7\u00f5es e depend\u00eancias utilizadas estar\u00e3o dispon\u00edveis para outros ambientes. Mesmo com o uso do Docker, pode ser que caracter\u00edsticas espec\u00edficas utilizadas, como os Daemon Socket , n\u00e3o estejam dispon\u00edveis. Para resolver este problema e evitar que o sistema operacional seja uma barreira para a reprodu\u00e7\u00e3o e replica\u00e7\u00e3o do material deste RC , fez-se a cria\u00e7\u00e3o de uma M\u00e1quina Virtual (VM, do ingl\u00eas Virtual Machine ). Com uma VM, diferente do Docker, tem-se a virtualiza\u00e7\u00e3o de um sistema completo, o que remove qualquer depend\u00eancia com o sistema adjacente. A cria\u00e7\u00e3o dessa VM, foi realizada com o aux\u00edlio do Vagrant , uma ferramenta que facilita o gerenciamento e provisionamento de m\u00e1quinas virtuais, desenvolvida pela Hashicorp . Vagrant est\u00e1 dispon\u00edvel para Windows, Linux, MacOS e outros sistemas operacionais. Com essa ferramenta, atrav\u00e9s de um arquivo de descri\u00e7\u00e3o ( Vagrantfile ), \u00e9 poss\u00edvel especificar uma VM completa, considerando elementos como: Quantidade de mem\u00f3ria RAM; Quantidade de CPU; Sistema operacional; Rede; Pacotes instalados. Al\u00e9m dessas, muitas outras configura\u00e7\u00f5es est\u00e3o dispon\u00edveis. Utilizando dessas caracter\u00edsticas, neste RC , fez-se a cria\u00e7\u00e3o de um Vagrantfile que especifica uma VM Ubuntu 20.04 , j\u00e1 preparada com as principais depend\u00eancias necess\u00e1rias para uso dos materiais deste RC (e.g., Docker, Docker Compose). Por padr\u00e3o, a m\u00e1quina \u00e9 criada com 12 GB de RAM e 8 CPUs . Recursos da VM A quantidade de recursos definida para a VM foi feita considerando como base uma m\u00e1quina de 24 GB de RAM e 12 CPUs. Caso seja necess\u00e1rio, atrav\u00e9s do Vagrantfile , esses valores podem ser alterados. Para isso, altere as seguintes propriedades dispon\u00edveis no arquivo: vb.memory = \"12288\" # 12 GB vb.cpus = \"8\" O Vagrant suporta v\u00e1rios Providers , que s\u00e3o as ferramentas utilizadas para criar as VMs. Neste RC , fez-se o uso do Provider Open Source VirtualBox . Instala\u00e7\u00e3o do Vagrant \u00b6 Para come\u00e7ar a utilizar a VM atrav\u00e9s do Vagrant, o primeiro passo \u00e9 realizar a instala\u00e7\u00e3o do Vagrant propriamente dito. Para isso, recomenda-se a utiliza\u00e7\u00e3o da documenta\u00e7\u00e3o oficial . Utiliza\u00e7\u00e3o da VM via Vagrant \u00b6 Uma vez que o Vagrant est\u00e1 instalado em seu sistema, para a cria\u00e7\u00e3o da VM, o primeiro passo \u00e9 realizar o clone do reposit\u00f3rio onde est\u00e3o todos os materiais deste RC : git clone https://github.com/brazil-data-cube/compendium-harmonization Ap\u00f3s o clone, acesse o diret\u00f3rio compendium-harmonization : cd compendium-harmonization Dentro do reposit\u00f3rio, voc\u00ea ser\u00e1 capaz de ver todos os materiais deste RC : ls -lha #> -rwxrwxrwx 1 felipe felipe 368 Apr 9 20:01 .dockerignore #> drwxrwxrwx 1 felipe felipe 512 Apr 14 17:00 .git #> drwxrwxrwx 1 felipe felipe 512 Apr 15 22:53 .github #> -rwxrwxrwx 1 felipe felipe 4.3K Apr 10 08:42 .gitignore #> -rwxrwxrwx 1 felipe felipe 1.1K Apr 9 20:01 LICENSE #> -rwxrwxrwx 1 felipe felipe 2.7K Apr 30 18:38 Makefile #> -rwxrwxrwx 1 felipe felipe 4.5K Apr 9 20:01 README.md #> -rwxrwxrwx 1 felipe felipe 3.4K Apr 15 22:53 Vagrantfile #> drwxrwxrwx 1 felipe felipe 512 Apr 14 17:00 analysis #> -rwxrwxrwx 1 felipe felipe 1.4K Apr 10 08:19 bootstrap.sh #> drwxrwxrwx 1 felipe felipe 512 Apr 14 17:00 composes #> drwxrwxrwx 1 felipe felipe 512 Apr 14 17:00 docker #> -rwxrwxrwx 1 felipe felipe 383 Apr 10 07:39 setenv.sh #> drwxrwxrwx 1 felipe felipe 512 Apr 14 17:00 tools Dentre esses arquivos, note que h\u00e1 dispon\u00edvel o arquivo Vagrantfile . Esse arquivo, como citado anteriormente, tem toda a especifica\u00e7\u00e3o da VM que deve ser criada. Para criar a VM com esse arquivo, utilize o seguinte comand: vagrant up #> Bringing machine 'default' up with 'virtualbox' provider... #> ==> default: Checking if box 'alvistack/ubuntu-20.04' version '20220415.1.1' is up to date... #> ==> default: A newer version of the box 'alvistack/ubuntu-20.04' for provider 'virtualbox' is #> ==> default: available! You currently have version '20220415.1.1'. The latest is version #> ==> default: '20220430.1.2'. Run `vagrant box update` to update. #> ==> default: Resuming suspended VM... #> ==> default: Booting VM... #> ==> default: Waiting for machine to boot. This may take a few minutes... #> default: SSH address: 127.0.0.1:2222 #> default: SSH username: vagrant #> default: SSH auth method: private key Ap\u00f3s a execu\u00e7\u00e3o desse arquivo, a VM j\u00e1 estar\u00e1 criada e pronta para ser utilizada. Neste caso, para acessar a VM, utilize o comando: vagrant ssh #> Welcome to Ubuntu 20.04.4 LTS (GNU/Linux 5.13.0-39-generic x86_64) #> * Documentation: https://help.ubuntu.com #> * Management: https://landscape.canonical.com #> * Support: https://ubuntu.com/advantage # (Omitted) #> vagrant@ubuntu:~$ Ao acessar o ambiente, voc\u00ea est\u00e1 pronto para utilizar os materiais disponibilizados neste RC . Por exemplo, caso voc\u00ea queira acessar os materiais do RC que voc\u00ea fez o clone para criar a VM, voc\u00ea pode acessar o diret\u00f3rio /compendium : Mudando de diret\u00f3rio cd /compendium Listando os arquivos ls -lha #> drwxrwxrwx 1 vagrant vagrant 0 Apr 14 20:00 analysis #> -rwxrwxrwx 1 vagrant vagrant 1.4K Apr 10 11:19 bootstrap.sh #> drwxrwxrwx 1 vagrant vagrant 0 Apr 14 20:00 composes #> drwxrwxrwx 1 vagrant vagrant 0 Apr 14 20:00 docker #> -rwxrwxrwx 1 vagrant vagrant 368 Apr 9 23:01 .dockerignore #> -rwxrwxrwx 1 vagrant vagrant 17 Apr 10 11:25 .env #> drwxrwxrwx 1 vagrant vagrant 0 Apr 16 01:53 .github #> -rwxrwxrwx 1 vagrant vagrant 4.3K Apr 10 11:42 .gitignore #> -rwxrwxrwx 1 vagrant vagrant 1.1K Apr 9 23:01 LICENSE #> -rwxrwxrwx 1 vagrant vagrant 2.7K Apr 30 21:38 Makefile #> -rwxrwxrwx 1 vagrant vagrant 4.5K Apr 9 23:01 README.md #> -rwxrwxrwx 1 vagrant vagrant 383 Apr 10 10:39 setenv.sh #> drwxrwxrwx 1 vagrant vagrant 4.0K Apr 14 20:00 tools #> drwxrwxrwx 1 vagrant vagrant 0 May 1 19:16 .vagrant #> -rwxrwxrwx 1 vagrant vagrant 3.4K Apr 16 01:53 Vagrantfile","title":"Ambientes computacionais"},{"location":"tools/environment/#ambientes-computacionais","text":"Por tr\u00e1s de cada etapa empregada nos scripts de processamento , conforme apresentado nas Se\u00e7\u00f5es anteriores, existem diversas ferramentas e bibliotecas de software sendo utilizadas. Algumas dessas ferramentas fazem o uso de tecnologias especiais para a execu\u00e7\u00e3o de suas opera\u00e7\u00f5es, como \u00e9 o caso da biblioteca research-processing , que utiliza Docker Containers para executar as fun\u00e7\u00f5es de processamento em ambientes isolados. Outras ferramentas, apenas fazem o uso do ambiente subjacente para sua execu\u00e7\u00e3o, como \u00e9 o caso do script auxiliar Example toolkit . Neste caso, \u00e9 exigido que o ambiente subjacente esteja configurado e pronto para executar o script . Em ambos os cen\u00e1rios apresentados, h\u00e1 desafios espec\u00edficos no que diz respeito ao gerenciamento dos ambientes computacionais utilizados. Por exemplo, pode ser necess\u00e1rio configura\u00e7\u00f5es espec\u00edficas no software para que ele opere junto a biblioteca research-processing , enquanto que configura\u00e7\u00f5es espec\u00edficas podem ser exigidas no uso do Example toolkit . Para resolver esses problemas e evitar que a configura\u00e7\u00e3o dos ambientes computacionais utilizados, suas depend\u00eancias e necessidades espec\u00edficas causem problemas para a reprodutibilidade e replicabilidade dos scripts de processamento criados neste RC , todos os ambientes necess\u00e1rios para a execu\u00e7\u00e3o das ferramentas foram organizadas em Docker Images. Essas, representam \"pacotes de ambientes\" prontos para uso, onde tem-se todas as depend\u00eancias e configura\u00e7\u00f5es necess\u00e1rias para a execu\u00e7\u00e3o de uma ferramenta espec\u00edfica. Nesta Se\u00e7\u00e3o, \u00e9 feita a apresenta\u00e7\u00e3o de cada uma dessas Docker Images, suas caracter\u00edsticas, configura\u00e7\u00f5es e formas de uso. Deve-se notar que, essas Docker Images n\u00e3o foram criadas para um sistema operacional espec\u00edfico, podendo ser utilizado em qualquer sistema que tenha suporte ao Docker. No entanto, deve-se notar que nesta documenta\u00e7\u00e3o, os comandos e formas de configura\u00e7\u00e3o apresentados, consideram como base o uso do Sistema Operacional Linux Ubuntu 20.04. Desta forma, mudan\u00e7as podem ser necess\u00e1rias nos comandos caso voc\u00ea esteja utilizando um sistema operacional diferente, como \u00e9 o caso do Windows. Mudan\u00e7as entre os sistemas operacionais Embora tenhamos esperan\u00e7a de que os comandos e dicas apresentados neste documento possam ser utilizados sem problemas em sistemas operacionais Linux (e.g., Ubuntu, Debian) e MacOS, n\u00e3o h\u00e1 uma garantia de que isso sempre se manter\u00e1 verdadeiro. Al\u00e9m disso, para aqueles que utilizam Windows, mudan\u00e7as nos comandos podem ser necess\u00e1rias. Com o objetivo de evitar que os materiais produzidos n\u00e3o possam ser utilizados por essa barreira tecnol\u00f3gica, n\u00f3s criamos uma M\u00e1quina Virtual Ubuntu 20.04, com todas as depend\u00eancias necess\u00e1rias (e.g., Docker , Docker Compose ) para que os comandos apresentados aqui, possam ser utilizados. Caso voc\u00ea precise utilizar essa M\u00e1quina Virtual, por favor, consulte a Se\u00e7\u00e3o M\u00e1quina Virtual com Vagrant .","title":"Ambientes computacionais"},{"location":"tools/environment/#docker-images","text":"Neste RC , existem diferentes tipos de ambientes que est\u00e3o sendo configurados dentro das Docker Images. No geral, essas Docker Images podem ser classificadas em dois tipos: Executable Ferramentas de linha de comando ( CLI , do ingl\u00eas Command-Line Interface ) s\u00e3o simples e diretas de utilizar e permitem que automa\u00e7\u00f5es sejam realizadas durante as etapas de processamento. Seguindo esse pensamento, as Docker Images Executable s\u00e3o aquelas criadas para armazenar um script que pode ser executado como uma CLI . Para isso, esse tipo de Docker Image tem as seguintes propriedades: Cada execu\u00e7\u00e3o da Docker Image representa uma execu\u00e7\u00e3o individual da ferramenta a qual est\u00e1 associada; Par\u00e2metros podem ser passados durante a execu\u00e7\u00e3o da Docker Image. Esses par\u00e2metros s\u00e3o utilizados para configurar a ferramenta que \u00e9 executada; Docker Volumes e vari\u00e1veis de ambientes, tamb\u00e9m podem ser utilizados para configurar a Docker Image, sendo utilizados, por exemplo, para determinar as entradas e sa\u00eddas e configura\u00e7\u00f5es da ferramenta executada. Environment Diferente das Docker Images Executable , essas Docker Images s\u00e3o criadas para servir como um ambiente completo que ser\u00e1 utilizado para a execu\u00e7\u00e3o de uma ferramenta, como um Jupyter Notebook ou Dagster Web Interface. A principal diferen\u00e7a entre esses dois tipos de Docker Images criados neste RC est\u00e1 em sua finalidade. Enquanto as Executables representam as ferramentas execut\u00e1veis, as Environment representam ambientes de fato, para uso e execu\u00e7\u00e3o de ferramentas espec\u00edficas. Nas Subse\u00e7\u00f5es a seguir, as Docker Images criadas neste RC ser\u00e3o apresentadas.","title":"Docker Images "},{"location":"tools/environment/#sen2cor-290","text":"Sen2cor \u00e9 um processador de corre\u00e7\u00e3o atmosf\u00e9rica para produtos Sentinel-2. Ele recebe como entrada produtos Sentinel-2 em n\u00edvel de radi\u00e2ncia de topo de atmosf\u00e9ra (ToA), tamb\u00e9m chamados Level-1C (L1C) e gera produtos \"Bottom of the atmosphere (BOA) reflectance\", al\u00e9m de fornecer tamb\u00e9m uma classifica\u00e7\u00e3o da cena. Com o uso dessa classifica\u00e7\u00e3o, \u00e9 poss\u00edvel, por exemplo, identificar pixels com influ\u00eancia de nuvem, sombra de nuvem ou neve. Mais sobre o Sen2Cor Para mais informa\u00e7\u00f5es sobre o Sen2Cor, consulte o Manual oficial do usu\u00e1rio . De modo a possibilitar que as execu\u00e7\u00f5es realizadas com Sen2Cor fossem reprodut\u00edveis e reutiliz\u00e1veis, preparou-se uma Docker Image espec\u00edfica para essa ferramenta. Essa Docker Image, nomeada de sen2cor , possui todas as depend\u00eancias e configura\u00e7\u00f5es necess\u00e1rias para a execu\u00e7\u00e3o do Sen2Cor. Vers\u00f5es do Sen2Cor O Sen2Cor , \u00e9 um software mantido e distribu\u00eddo pela E uropean S pace A gency (ESA) e continua lan\u00e7ando novas vers\u00f5es. Neste RC , foi considerado paras as atividades com o Sen2Cor, a vers\u00e3o 2.9.0 . Os t\u00f3picos a seguir, apresentam as principais caracter\u00edsticas desta Docker Image, como volumes, dados auxiliares necess\u00e1rios e forma de utiliza\u00e7\u00e3o. Dados auxiliares Para a execu\u00e7\u00e3o da sen2cor , \u00e9 necess\u00e1rio o uso de alguns dados auxiliares. Esses dados, dizem respeito ao ESACCI-LC for Sen2Cor data package , que \u00e9 utilizado pelos m\u00f3dulos de identifica\u00e7\u00e3o de nuvens e classifica\u00e7\u00e3o. A obten\u00e7\u00e3o dos dados pode ser feita seguindo os passos listados abaixo: Acesse o endere\u00e7o: http://maps.elie.ucl.ac.be/CCI/viewer/download.php ; Fa\u00e7a seu cadastro (caso n\u00e3o tenha) e login; Ap\u00f3s o login, procure pelo pacote ESACCI-LC for Sen2Cor data package ; Fa\u00e7a o download desse pacote (Arquivo zip ); Extraia o conte\u00fado em um diret\u00f3rio. Recomenda-se para esse diret\u00f3rio o nome CCI4SEN2COR . Ap\u00f3s a extra\u00e7\u00e3o dos arquivos, o diret\u00f3rio de destino dever\u00e1 conter os seguintes arquivos: ESACCI-LC-L4-WB-Map-150m-P13Y-2000-v4.0.tif (GeoTIFF); ESACCI-LC-L4-LCCS-Map-300m-P1Y-2015-v2.0.7.tif (GeoTIFF); ESACCI-LC-L4-Snow-Cond-500m-P13Y7D-2000-2012-v2.0 (Direct\u00f3rio). Volumes Para a utiliza\u00e7\u00e3o da sen2cor , \u00e9 necess\u00e1rio a defini\u00e7\u00e3o de alguns Docker Volumes. Esses volumes, especificam os dados de entrada, sa\u00edda, arquivos de configura\u00e7\u00e3o e dados auxiliares utilizados pela ferramenta. Abaixo, esses volumes s\u00e3o listados e descritos: Dados de entrada (Obrigat\u00f3rio) Diret\u00f3rio com os dados de entrada. Esse volume, deve mapear um diret\u00f3rio da m\u00e1quina local para o diret\u00f3rio /mnt/input_dir do Container. Recomenda-se que esse volume seja somente leitura, para garantir que nenhuma modifica\u00e7\u00e3o ser\u00e1 feita nos dados de entrada. Dados de sa\u00edda (Obrigat\u00f3rio) Diret\u00f3rio onde os produtos gerados ser\u00e3o armazenados. O volume criado, deve mapear um diret\u00f3rio da m\u00e1quina local para o diret\u00f3rio /mnt/output_dir do Container. Dados auxiliares (Obrigat\u00f3rio) Diret\u00f3rio com os dados auxiliares necess\u00e1rio para o funcionamento do Sen2Cor. O volume criado, deve mapear um diret\u00f3rio da m\u00e1quina local para o diret\u00f3rio /mnt/sen2cor-aux/CCI4SEN2COR do Container. Arquivo de configura\u00e7\u00e3o (Opcional) Volume para a defini\u00e7\u00e3o do arquivo de configura\u00e7\u00e3o L2A_GIPP.xml . O volume criado, deve mapear o arquivo L2A_GIPP.xml da m\u00e1quina local para o arquivo /opt/sen2cor/2.9.0/cfg/L2A_GIPP.xml no Container. Dados SRTM (Opcional) Volume para a especifica\u00e7\u00e3o do diret\u00f3rio com imagens SRTM que devem ser utilizados nas etapas do Sen2Cor. O volume criado, deve mapear um diret\u00f3rio da m\u00e1quina local para o diret\u00f3rio /mnt/sen2cor-aux/srtm do Container. Exemplo de utiliza\u00e7\u00e3o (Docker CLI ) O c\u00f3digo abaixo, apresenta um exemplo de utiliza\u00e7\u00e3o da Docker Image sen2cor atrav\u00e9s da Docker CLI . Nome da imagem No comando apresentado abaixo, a Docker Image sen2cor \u00e9 identificada como marujore/sen2cor:2.9.0 j\u00e1 que esta, est\u00e1 armazenada no perfil de usu\u00e1rio marujore no DockerHub e a vers\u00e3o escolhida \u00e9 a 2.9.0 . Formata\u00e7\u00e3o do comando O comando abaixo \u00e9 criado para ser dit\u00e1dico. Caso voc\u00ea queira utiliza-lo, n\u00e3o esque\u00e7a de substituir os valores e remover os espa\u00e7os entre cada linha. docker run --rm \\ # Volume: Dados de entrada --volume /path/to/input_dir:/mnt/input_dir:ro \\ # Volume: Dados de sa\u00edda --volume /path/to/output_dir:/mnt/output_dir:rw \\ # Dados auxiliares: Diret\u00f3rio CCI4SEN2COR --volume /path/to/CCI4SEN2COR:/mnt/aux_data \\ # Arquivo de configura\u00e7\u00e3o: L2A_GIPP.xml (Opcional) --volume /path/to/L2A_GIPP.xml:/opt/sen2cor/2.9.0/cfg/L2A_GIPP.xml \\ # Dados SRTM (Opcional) --volume /path/to/srtm:/root/sen2cor/2.9/dem/srtm \\ # Especifica\u00e7\u00e3o da Docker Image e cena a ser processada brazildatacube/sen2cor:2.9.0 S2A_MSIL1C_20210903T140021_N0301_R067_T21KVR_20210903T172609.SAFE A execu\u00e7\u00e3o do comando apresentado acima, far\u00e1 a cria\u00e7\u00e3o de um Docker Container sen2cor . Esse Docker Container far\u00e1 o processamento da cena S2A_MSIL1C_20210903T140021_N0301_R067_T21KVR_20210903T172609.SAFE . Neste comando, deve-se notar que, o diret\u00f3rio de entrada ( /path/to/input_dir ) especificado, deve conter um subdiret\u00f3rio com o mesmo nome da cena escolhida, neste caso S2A_MSIL1C_20210903T140021_N0301_R067_T21KVR_20210903T172609.SAFE . Al\u00e9m disso, \u00e9 esperado que nesse subdiret\u00f3rio, todos os dados da cena estejam dispon\u00edveis para o processamento. Para mais informa\u00e7\u00f5es, consulte o reposit\u00f3rio do GitHub , onde tem-se mantido o versionamento das mudan\u00e7as realizadas na Docker Image sen2cor .","title":"Sen2Cor 2.9.0"},{"location":"tools/environment/#lasrc-201","text":"LaSRC \u00e9 um processador de corre\u00e7\u00e3o atmosf\u00e9rica originalmente proposto para produtos Landsat-8 Collection 1, sendo posteriormente adaptado para ser capaz tamb\u00e9m de corrigir produtos Sentinel-2. Ele recebe como entrada produtos Landsat em N\u00famero Digital (DN, do ingl\u00eas Digital Number ) ou produtos Sentinel-2 em n\u00edvel de radi\u00e2ncia de topo de atmosf\u00e9ra (ToA), tamb\u00e9m chamados Level-1C (L1C). O resultado desse processador consiste em produtos em n\u00edvel reflect\u00e2ncia de superf\u00edcie (SR, do ingl\u00eas Surface Reflectance ). Para facilitar a utiliza\u00e7\u00e3o do LaSRC neste RC , e garantir que a execu\u00e7\u00e3o seja reprodut\u00edvel e reutiliz\u00e1vel, fez-se a cria\u00e7\u00e3o de um Docker Image para o LaSRC, nomeada de lasrc . A lasrc , possui todas as depend\u00eancias e configura\u00e7\u00f5es necess\u00e1rias para a execu\u00e7\u00e3o do processador LaSRC. Os t\u00f3picos a seguir, apresentam as principais caracter\u00edsticas desta Docker Image, como volumes, dados auxiliares necess\u00e1rios e forma de utiliza\u00e7\u00e3o. Dados auxiliares Para a execu\u00e7\u00e3o da lasrc , \u00e9 necess\u00e1rio a defini\u00e7\u00e3o de alguns dados auxiliares. Para obter esses dados, voc\u00ea pode seguir os passos listados abaixo: Acesse: https://edclpdsftp.cr.usgs.gov/downloads/auxiliaries/lasrc_auxiliary/L8/ ; Fa\u00e7a o download de todos os conte\u00fados dispon\u00edveis listados, com exce\u00e7\u00e3o do diret\u00f3rio LADS . Os dados do diret\u00f3rio LADS tamb\u00e9m s\u00e3o requeridos para a utiliza\u00e7\u00e3o do LaSRC, no entanto, esse diret\u00f3rio cont\u00e9m dados di\u00e1rios de 2013 at\u00e9 os dias atuais, o que representa um grande volume de dados. Para tornar o processo mais r\u00e1pido, recomenda-se que seja feito o download apenas dos arquivos LADS das datas que ser\u00e3o processadas. Sele\u00e7\u00e3o de arquivos LADS Cada arquivo LADS refere-se a uma data do ano. Sendo assim, para processar uma imagem do dia 1\u00b0 de Janeiro de 2017, deve-se obter o arquivo LADS L8ANC2017001.hdf_fused em que 2017 representa o ano e o valor 001 a data 1\u00b0 de Janeiro em formato Juliano. Ao final da aquisi\u00e7\u00e3o dos dados auxiliares, o diret\u00f3rio onde os dados foram armazenados deve ter a seguinte estrutura: . \u251c\u2500\u2500 CMGDEM.hdf \u251c\u2500\u2500 LADS \u251c\u2500\u2500 LDCMLUT \u251c\u2500\u2500 MSILUT \u2514\u2500\u2500 ratiomapndwiexp.hdf Volumes Para a utiliza\u00e7\u00e3o da lasrc , \u00e9 necess\u00e1rio a defini\u00e7\u00e3o de alguns Docker Volumes. Esses volumes, especificam os dados de entrada, sa\u00edda e dados auxiliares utilizados pela ferramenta durante o processamento. Abaixo, tem-se um descritivo de todos volumes que devem ser criados durante a execu\u00e7\u00e3o da Docker Image LaSRC: Dados de entrada (Obrigat\u00f3rio) Diret\u00f3rio com os dados de entrada. Esse volume, deve mapear um diret\u00f3rio da m\u00e1quina local para o diret\u00f3rio /mnt/input_dir do Container. Recomenda-se que esse volume seja somente leitura, para garantir que nenhuma modifica\u00e7\u00e3o ser\u00e1 feita nos dados de entrada. Dados de sa\u00edda (Obrigat\u00f3rio) Diret\u00f3rio onde os produtos gerados ser\u00e3o armazenados. O volume criado, deve mapear um diret\u00f3rio da m\u00e1quina local para o diret\u00f3rio /mnt/output_dir do Container. Dados auxiliares (Obrigat\u00f3rio) Diret\u00f3rio com os dados auxiliares necess\u00e1rio para o funcionamento do LaSRC. O volume criado, deve mapear um diret\u00f3rio da m\u00e1quina local para o diret\u00f3rio /mnt/atmcor_aux/lasrc/L8 do Container. Exemplo de utiliza\u00e7\u00e3o (Docker CLI ) Os c\u00f3digos abaixo apresentam dois exemplos de utiliza\u00e7\u00e3o do lasrc , atrav\u00e9s da Docker CLI . No primeiro exemplo, faz-se o processamento de uma cena Landsat-8/OLI, enquanto no segundo \u00e9 processado uma cena Sentinel-2/MSI. Nome da imagem Nos comandos apresentados abaixo, a Docker Image lasrc \u00e9 identificada como marujore/lasrc:2.0.1 j\u00e1 que esta, est\u00e1 armazenada no perfil de usu\u00e1rio marujore no DockerHub e a vers\u00e3o escolhida \u00e9 a 2.0.1 . Formata\u00e7\u00e3o do comando O comando abaixo \u00e9 criado para ser dit\u00e1dico. Caso voc\u00ea queira utiliza-lo, n\u00e3o esque\u00e7a de substituir os valores e remover os espa\u00e7os entre cada linha. Exemplo para dados Landsat-8/OLI docker run --rm \\ # Volume: Dados de entrada --volume /path/to/input/:/mnt/input-dir:rw \\ # Volume: Dados de sa\u00edda --volume /path/to/output:/mnt/output-dir:rw \\ # Dados auxiliares (Dados L8/LADS) --volume /path/to/lasrc_auxiliaries/L8:/mnt/atmcor_aux/lasrc/L8:ro \\ # Especifica\u00e7\u00e3o da Docker Image e cena a ser processada --tty marujore/lasrc:2.0.1 LC08_L1TP_220069_20190112_20190131_01_T1 Exemplo para dados Sentinel-2/MSI docker run --rm \\ # Volume: Dados de entrada --volume /path/to/input/:/mnt/input-dir:rw \\ # Volume: Dados de sa\u00edda --volume /path/to/output:/mnt/output-dir:rw \\ # Dados auxiliares (Dados L8/LADS) --volume /path/to/lasrc_auxiliaries/L8:/mnt/atmcor_aux/lasrc/L8:ro \\ # Especifica\u00e7\u00e3o da Docker Image e cena a ser processada --tty marujore/lasrc:2.0.1 S2A_MSIL1C_20190105T132231_N0207_R038_T23LLF_20190105T145859.SAFE Como pode-se notar, a diferen\u00e7a para o uso da lasrc para os dados dos diferentes sat\u00e9lite-sensor, est\u00e1 apenas na especifica\u00e7\u00e3o do nome da cena. Deve-se notar tamb\u00e9m que, \u00e9 esperado, para ambos os casos que, no diret\u00f3rio de entrada ( /path/to/input/ ) tenha subdiret\u00f3rios com as cenas espec\u00edficas, neste caso LC08_L1TP_220069_20190112_20190131_01_T1 e S2A_MSIL1C_20190105T132231_N0207_R038_T23LLF_20190105T145859.SAFE . Al\u00e9m disso, \u00e9 esperado que nesses subdiret\u00f3rios, todos os dados das cenas estejam dispon\u00edveis para o processamento. Para mais informa\u00e7\u00f5es, consulte o reposit\u00f3rio do GitHub , onde tem-se mantido o versionamento das mudan\u00e7as realizadas na Docker Image lasrc .","title":"LaSRC 2.0.1"},{"location":"tools/environment/#l8angs","text":"Landsat Ang Tool \u00e9 uma ferramenta desenvolvida e mantida pela U nited S tates G eological S urvey . A ferramenta \u00e9 capaz de utilizar arquivos ANG.txt fornecidos junto \u00e0 produtos Landsat-8 para gerar bandas de angulos por pixel , no caso os \u00e2ngulos solar azimutal ( SAA ), solar zenital ( SZA ), sensor azimutal ( VAA ) e sensor zenital ( VZA ). As bandas s\u00e3o geradas com a mesma resolu\u00e7\u00e3o espacial das bandas espectrais do sensor OLI acoplados ao sat\u00e9lite Landsat-8. Mais sobre o Landsat Ang Tool Para informa\u00e7\u00f5es detalhadas sobre o Landsat Ang Tool, consulte o site oficial da USGS sobre a ferramenta . Neste RC , as imagens Landsat-8/OLI (Collection-2) s\u00e3o obtidas j\u00e1 processadas em n\u00edvel de reflect\u00e2ncia de superf\u00edcie (L2). Entretanto, para processamentos posteriores, \u00e9 necess\u00e1rio a gera\u00e7\u00e3o das bandas de \u00e2ngulos. Neste caso, faz-se o uso do Landsat Ang Tool . A instala\u00e7\u00e3o e configura\u00e7\u00e3o do Landsat Ang Tool pode tornar dif\u00edcil a reprodu\u00e7\u00e3o e replica\u00e7\u00e3o no futuro. Sendo assim, para facilitar as opera\u00e7\u00f5es deste RC que utilizam essa ferramenta, fez-se a cria\u00e7\u00e3o de uma Docker Image espec\u00edfica para ela, nomeada de l8angs . Os t\u00f3picos a seguir, apresentam as principais caracter\u00edsticas desta Docker Image, como volumes e dados auxiliares necess\u00e1rios para a execu\u00e7\u00e3o. Tamb\u00e9m s\u00e3o apresentados exemplos de utiliza\u00e7\u00e3o atrav\u00e9s da Docker CLI . Volumes Para a utiliza\u00e7\u00e3o da l8angs , \u00e9 necess\u00e1rio a defini\u00e7\u00e3o do seguinte volume, durante a execu\u00e7\u00e3o: Dados de entrada (Obrigat\u00f3rio) Diret\u00f3rio com os dados de entrada. O volume criado, deve mapear um diret\u00f3rio da m\u00e1quina local para o diret\u00f3rio /mnt/input-dir do Container. Os dados gerados s\u00e3o criados no mesmo diret\u00f3rio de entrada, sendo este o comportamento padr\u00e3o da ferramenta. Exemplo de utiliza\u00e7\u00e3o (Docker CLI ) O c\u00f3digo abaixo, apresenta um exemplo de utiliza\u00e7\u00e3o da Docker Image l8angs atrav\u00e9s da Docker CLI . Nome da imagem Nos comandos apresentados abaixo, a Docker Image l8angs \u00e9 identificada como marujore/landsat-angles:2.0.1 j\u00e1 que esta, est\u00e1 armazenada no perfil de usu\u00e1rio marujore no DockerHub e a vers\u00e3o escolhida \u00e9 a latest . Formata\u00e7\u00e3o do comando O comando abaixo \u00e9 criado para ser dit\u00e1dico. Caso voc\u00ea queira utiliza-lo, n\u00e3o esque\u00e7a de substituir os valores e remover os espa\u00e7os entre cada linha. docker run --rm \\ # Volume: Dados de entrada -v /path/to/input/:/mnt/input-dir:rw \\ # Especifica\u00e7\u00e3o da Docker Image e cena a ser processada marujore/landsat-angles:latest LC08_L2SP_222081_20190502_20200829_02_T1 A execu\u00e7\u00e3o do comando apresentado acima, far\u00e1 a cria\u00e7\u00e3o de um Docker Container l8angs . Esse Docker Container far\u00e1 o processamento da cena LC08_L2SP_222081_20190502_20200829_02_T1 . Neste comando, deve-se notar que, o diret\u00f3rio de entrada ( /path/to/input/ ) especificado, deve conter um subdiret\u00f3rio com o mesmo nome da cena escolhida, neste caso LC08_L2SP_222081_20190502_20200829_02_T1 . Al\u00e9m disso, \u00e9 esperado que nesse subdiret\u00f3rio, todos os dados da cena estejam dispon\u00edveis para o processamento. Para mais informa\u00e7\u00f5es, consulte o reposit\u00f3rio do GitHub , onde tem-se mantido o versionamento das mudan\u00e7as realizadas na Docker Image l8angs .","title":"L8Angs"},{"location":"tools/environment/#sensor-harm","text":"Neste RC , as imagens Landsat-8 Collection-2 j\u00e1 obtidas em n\u00edvel de reflect\u00e2ncia de superf\u00edcie (L2) e as imagens Sentinel-2 (processadas para reflect\u00e2ncia de superf\u00edcie tanto utilizando Sen2cor quanto LaSRC) s\u00e3o harmonizadas utilizando a biblioteca sensor-harm . Para potencializar a reprodu\u00e7\u00e3o e replica\u00e7\u00e3o no uso dessa ferramenta, fez-se a cria\u00e7\u00e3o da Docker Image sensor-harm . Nesta Image, est\u00e3o dispon\u00edveis todas as depend\u00eancias e configura\u00e7\u00f5es necess\u00e1rias para a execu\u00e7\u00e3o do sensor-harm . Os t\u00f3picos a seguir, apresentam as principais caracter\u00edsticas desta Docker Image, volumes requeridos e exemplos de utiliza\u00e7\u00e3o. Volumes Para a utiliza\u00e7\u00e3o da sensor-harm , \u00e9 necess\u00e1rio a defini\u00e7\u00e3o de alguns Docker Volumes. Esses volumes, especificam os dados de entrada e dados auxiliares utilizados pelo sensor-harm. Abaixo, esses volumes s\u00e3o listados e descritos: Dados de entrada (Obrigat\u00f3rio) Diret\u00f3rio com os dados de entrada. Esse volume, deve mapear um diret\u00f3rio da m\u00e1quina local para o diret\u00f3rio /mnt/input-dir do Container. Recomenda-se que esse volume seja somente leitura, para garantir que nenhuma modifica\u00e7\u00e3o ser\u00e1 feita nos dados de entrada. Dados de sa\u00edda (Obrigat\u00f3rio) Diret\u00f3rio onde os produtos gerados ser\u00e3o armazenados. O volume criado, deve mapear um diret\u00f3rio da m\u00e1quina local para o diret\u00f3rio /mnt/output-dir do Container. Diret\u00f3rio de \u00e2ngulos (Obrigat\u00f3rio apenas para dados Landsat-8/OLI) Diret\u00f3rio com os dados de \u00e2ngulos da cena que ser\u00e1 processada. O volume criado, deve mapear um diret\u00f3rio da m\u00e1quina local para o diret\u00f3rio /mnt/angles-dir do Container. Recomenda-se que esse volume seja somente leitura, para garantir que nenhuma modifica\u00e7\u00e3o ser\u00e1 feita nos dados durante o processamento. Exemplo de utiliza\u00e7\u00e3o (Docker CLI ) Os c\u00f3digos abaixo apresentam dois exemplos de utiliza\u00e7\u00e3o do sensor-harm , atrav\u00e9s da Docker CLI . No primeiro exemplo, faz-se o processamento de uma cena Landsat-8/OLI, enquanto no segundo \u00e9 processado uma cena Sentinel-2/MSI. Nome da imagem Nos comandos apresentados abaixo, a Docker Image sensor-harm \u00e9 identificada como marujore/sensor-harm:latest j\u00e1 que esta, est\u00e1 armazenada no perfil de usu\u00e1rio marujore no DockerHub e a vers\u00e3o escolhida \u00e9 a latest . Formata\u00e7\u00e3o do comando O comando abaixo \u00e9 criado para ser dit\u00e1dico. Caso voc\u00ea queira utiliza-lo, n\u00e3o esque\u00e7a de substituir os valores e remover os espa\u00e7os entre cada linha. Exemplo para dados Landsat-8/OLI docker run --rm \\ # Volume: Dados de entrada --volume /path/to/input/:/mnt/input-dir:ro \\ # Volume: Dados de sa\u00edda --volume /path/to/output:/mnt/output-dir:rw \\ # Diret\u00f3rio de \u00e2ngulos (Apenas Landsat-8/OLI) --volume /path/to/angles:/mnt/angles-dir:ro \\ # Especifica\u00e7\u00e3o da Docker Image e cena a ser processada --tty marujore/sensor-harm:latest LC08_L1TP_220069_20190112_20190131_01_T1 Exemplo para dados Sentinel-2/MSI docker run --rm \\ # Volume: Dados de entrada --volume /path/to/input/:/mnt/input-dir:ro \\ # Volume: Dados de sa\u00edda --volume /path/to/output:/mnt/output-dir:rw \\ # Diret\u00f3rio de \u00e2ngulos (Apenas Landsat-8/OLI) --volume /path/to/angles:/mnt/angles-dir:ro \\ # Especifica\u00e7\u00e3o da Docker Image e cena a ser processada --tty marujore/sensor-harm:latest S2A_MSIL1C_20190105T132231_N0207_R038_T23LLF_20190105T145859.SAFE Como pode-se notar, a diferen\u00e7a para o uso da sensor-harm para os dados dos diferentes sat\u00e9lite-sensor, est\u00e1 apenas na especifica\u00e7\u00e3o do nome da cena. Deve-se notar tamb\u00e9m que, \u00e9 esperado, para ambos os casos que, no diret\u00f3rio de entrada ( /path/to/input/ ) tenha subdiret\u00f3rios com as cenas espec\u00edficas, neste caso LC08_L1TP_220069_20190112_20190131_01_T1 e S2A_MSIL1C_20190105T132231_N0207_R038_T23LLF_20190105T145859.SAFE . Al\u00e9m disso, \u00e9 esperado que nesses subdiret\u00f3rios, todos os dados das cenas estejam dispon\u00edveis para o processamento. Para mais informa\u00e7\u00f5es, consulte o reposit\u00f3rio do GitHub , onde tem-se mantido o versionamento das mudan\u00e7as realizadas na Docker Image sensor-harm .","title":"Sensor Harm"},{"location":"tools/environment/#docker-images-para-scripts-de-processamento","text":"Conforme apresentado na Se\u00e7\u00e3o Scripts de processamento , foram criados duas formas de realizar a execu\u00e7\u00e3o da metodologia dos experimentos deste RC . Uma utiliza o Jupyter Notebook , sendo voltada para o processamento interativo dos c\u00f3digos. A segunda op\u00e7\u00e3o, por outro lado, utiliza o Dagster e facilita execu\u00e7\u00f5es em lote e controle de erro. Para facilitar a utiliza\u00e7\u00e3o de ambas as abordagens, fez-se a cria\u00e7\u00e3o de Docker Images com os ambientes necess\u00e1rios \u00e0 execu\u00e7\u00e3o de cada um deles. Com isso, evita-se que depend\u00eancias tenham de ser instaladas ou configuradas para a execu\u00e7\u00e3o dos scripts de processamento . Os t\u00f3picos a seguir, apresentam as principais caracter\u00edsticas dessas Docker Images, volumes requeridos, configura\u00e7\u00e3o e exemplos de utiliza\u00e7\u00e3o. Funcionamento dos scripts Caso voc\u00ea esteja interessado em reutilizar essas Docker Images, recomenda-se antes a leitura do modo de funcionamento dos scripts de processamento , bem como das bibliotecas de software utilizadas por esses scripts .","title":"Docker Images para Scripts de processamento"},{"location":"tools/environment/#jupyter-notebook","text":"Para a execu\u00e7\u00e3o da vers\u00e3o Jupyter Notebook, fez-se a cria\u00e7\u00e3o da Docker Image research-processing-jupyter . Essa Docker Image disponibiliza uma interface JupyterLab , com as depend\u00eancias Python necess\u00e1rias para a execu\u00e7\u00e3o dos scripts . Al\u00e9m disso, nessa Docker Image, tem-se tamb\u00e9m o Docker instalado, para que os scripts sejam capazes de operar e criar outros Docker Containers de processamento. Base do ambiente A cria\u00e7\u00e3o da research-processing-jupyter , foi realizada utilizando como base a Docker Image jupyter/minimal-notebook , disponibilizada pelo time de desenvolvimento do projeto Jupyter . Com isso, todas as configura\u00e7\u00f5es e vari\u00e1veis de ambientes dispon\u00edveis na Docker Image jupyter/minimal-notebook tamb\u00e9m s\u00e3o aplic\u00e1veis a research-processing-jupyter . Nos t\u00f3picos abaixo, \u00e9 feita a apresenta\u00e7\u00e3o das configura\u00e7\u00f5es necess\u00e1rias para utilizar essa Docker Image. Exemplos de utiliza\u00e7\u00e3o com a CLI do Docker e Docker Compose , tamb\u00e9m ser\u00e3o apresentados. Vari\u00e1veis de ambiente Para a utiliza\u00e7\u00e3o da research-processing-jupyter , \u00e9 necess\u00e1rio definir a seguinte vari\u00e1vel de ambiente: DATA_DIRECTORY (Obrigat\u00f3rio) Vari\u00e1vel de ambiente para determinar o diret\u00f3rio, na m\u00e1quina local, onde os dados baixados devem ser salvos. Volumes A execu\u00e7\u00e3o do research-processing-jupyter deve ser realizada com a defini\u00e7\u00e3o de dois volumes: Volume de dados (Obrigat\u00f3rio) Volume onde os dados ser\u00e3o armazenados. Seguindo o modelo de execu\u00e7\u00e3o das fun\u00e7\u00f5es de processamento utilizadas nos scripts , esse volume ser\u00e1 utilizado por fun\u00e7\u00f5es dentro do mesmo container ( Local ) ou em outros containers ( Containerized ). Desta forma, a defini\u00e7\u00e3o desse volume deve ser feita de modo a atender duas exig\u00eancias: O volume deve ser do tipo Bind Mount ; O mapeamento do volume ( Bind Mount ) deve utilizar, na m\u00e1quina local e no Container, o mesmo caminho definido na vari\u00e1vel DATA_DIRECTORY . Com essas defini\u00e7\u00f5es, o volume ser\u00e1 vis\u00edvel dentro do Container da research-processing-jupyter e tamb\u00e9m pelos Containers auxiliares de processamentos que forem gerados durante a execu\u00e7\u00e3o dos scripts de processamento . Volume Daemon Socket (Obrigat\u00f3rio) Para que os scripts sejam capazes de gerar Docker Containers de processamento, \u00e9 necess\u00e1rio a defini\u00e7\u00e3o do Daemon Socket como um volume. Ao fazer isso, o Docker dentro do container criado com a imagem research-processing-jupyter \u00e9 capaz de interagir com o Docker da m\u00e1quina local, permitindo que Containers de processamento sejam criados. Defini\u00e7\u00e3o de usu\u00e1rio De forma complementar a defini\u00e7\u00e3o do Daemon Socket volume , para a execu\u00e7\u00e3o da research-processing-jupyter , \u00e9 necess\u00e1rio especificar um usu\u00e1rio ( UID ) e grupo ( GID ) da m\u00e1quina local que tenham permiss\u00e3o para interagir com o Docker Daemon. Esses valores, ser\u00e3o aplicados ao usu\u00e1rio padr\u00e3o do Container, de modo que o Docker permita que ele tamb\u00e9m interaga com o Docker Daemon da m\u00e1quina local. Permiss\u00f5es no Docker Caso voc\u00ea esteja interessado em entender os detalhes do motivo por tr\u00e1s da defini\u00e7\u00e3o dos usu\u00e1rios, recomendamos que voc\u00ea consulte a documenta\u00e7\u00e3o oficial do Docker . Para que voc\u00ea fa\u00e7a a defini\u00e7\u00e3o do usu\u00e1rio, no momento da execu\u00e7\u00e3o da research-processing-jupyter , voc\u00ea pode utilizar o par\u00e2metro --user . Caso voc\u00ea deseje utilizar o Docker Compose, o campo user pode ser utilizado para essa defini\u00e7\u00e3o. Exemplo de utiliza\u00e7\u00e3o (Docker CLI ) Abaixo, faz-se a apresenta\u00e7\u00e3o de um exemplo de uso da Docker Image research-processing-jupyter atrav\u00e9s da Docker CLI : Formata\u00e7\u00e3o do comando O comando abaixo \u00e9 criado para ser dit\u00e1dico. Caso voc\u00ea queira utiliza-lo, n\u00e3o esque\u00e7a de substituir os valores e remover os espa\u00e7os entre cada linha. docker run \\ --name research-processing-jupyter \\ # Defini\u00e7\u00e3o do usu\u00e1rio --user ${ UID } : ${ GID } \\ # Vari\u00e1veis de ambiente --env JUPYTER_ENABLE_LAB = yes \\ # Ativando JupyterLab --env DATA_DIRECTORY = /my-data-dir \\ # Volume: Volume de dados --volume /my-data-dir:/my-data-dir \\ # Volume: Volume Daemon Socket --volume /var/run/docker.sock:/var/run/docker.sock:ro \\ # Porta de rede por onde o servi\u00e7o poder\u00e1 ser acessado --publish 8888 :8888 \\ # Especifica\u00e7\u00e3o da Docker Image brazildatacube/research-processing-jupyter:latest Defini\u00e7\u00e3o do usu\u00e1rio Para a defini\u00e7\u00e3o do usu\u00e1rio, utilizando vari\u00e1veis de ambiente ( ${UID} e ${GID} ), como realizado no comando acima, antes de executar o comando Docker, utilize os seguintes comandos: export UID = ` id -u $USER ` export GID = ` cut -d: -f3 < < ( getent group docker ) ` Ap\u00f3s a execu\u00e7\u00e3o do comando acima, dever\u00e1 ser produzir um resultado parecido com o apresentado abaixo: # (Omitted) [ I 2022 -04-30 19 :22:50.684 ServerApp ] Use Control-C to stop this server and shut down all kernels ( twice to skip confirmation ) . [ C 2022 -04-30 19 :22:50.694 ServerApp ] To access the server, open this file in a browser: file:///home/jovyan/.local/share/jupyter/runtime/jpserver-7-open.html Or copy and paste one of these URLs: http://ae88466ccb18:8888/lab?token = 0497e15e042d52cfb498a2edf3d2c6e5874e79b4808ca860 or http://127.0.0.1:8888/lab?token = 0497e15e042d52cfb498a2edf3d2c6e5874e79b4808ca860 Ap\u00f3s a execu\u00e7\u00e3o deste comando, utilizando um navegador e acesse o endere\u00e7o do JupyterLab apresentado (Substitua o endere\u00e7o abaixo pelo que foi exibido em seu terminal): firefox http://127.0.0.1:8888/lab?token = 0497e15e042d52cfb498a2edf3d2c6e5874e79b4808ca860 Exemplo de utiliza\u00e7\u00e3o (Docker Compose) Abaixo, o mesmo exemplo de execu\u00e7\u00e3o feito com a Docker CLI \u00e9 realizado com Docker Compose . Para isso, primeiro fez-se a cria\u00e7\u00e3o do arquivo docker-compose.yml com o seguinte conte\u00fado: docker-compose.yml version: '3.2' services: my-notebook: # Defini\u00e7\u00e3o do usu\u00e1rio user: ${UID}:${GID} image: brazildatacube/research-processing-jupyter:latest environment: # Vari\u00e1veis de ambiente - JUPYTER_ENABLE_LAB=yes - DATA_DIRECTORY=/my-data-dir volumes: # Volume: Volume de dados - type: bind source: /my-data-dir target: /my-data-dir # Volume: Volume Daemon Socket - type: bind source: /var/run/docker.sock target: /var/run/docker.sock ports: # Porta de rede por onde o servi\u00e7o poder\u00e1 ser acessado - \"8888:8888\" Defini\u00e7\u00e3o do usu\u00e1rio Para a defini\u00e7\u00e3o do usu\u00e1rio, utilizando vari\u00e1veis de ambiente ( ${UID} e ${GID} ), como apresentado no docker-compose.yml , antes de executar o comando docker-compose , utilize os seguintes comandos: export UID = ` id -u $USER ` export GID = ` cut -d: -f3 < < ( getent group docker ) ` Com o arquivo criado, fez-se a execu\u00e7\u00e3o do compose: docker-compose -f docker-compose.yml up A sa\u00edda do comando acima deve ser parecida com: # (Omitted) [ I 2022 -04-30 19 :23:57.260 ServerApp ] http://afd0fe2755a7:8888/lab?token = 6d37701a6787bd58a7f92f29f33709970df11a742bf3b79b [ I 2022 -04-30 19 :23:57.260 ServerApp ] or http://127.0.0.1:8888/lab?token = 6d37701a6787bd58a7f92f29f33709970df11a742bf3b79b [ I 2022 -04-30 19 :23:57.260 ServerApp ] Use Control-C to stop this server and shut down all kernels ( twice to skip confirmation ) . [ C 2022 -04-30 19 :23:57.264 ServerApp ] To access the server, open this file in a browser: file:///home/jovyan/.local/share/jupyter/runtime/jpserver-8-open.html Or copy and paste one of these URLs: http://afd0fe2755a7:8888/lab?token = 6d37701a6787bd58a7f92f29f33709970df11a742bf3b79b or http://127.0.0.1:8888/lab?token = 6d37701a6787bd58a7f92f29f33709970df11a742bf3b79b Com esta informa\u00e7\u00e3o, pode-se realizar o acesso ao JupyterLab no navegador. Para isso, abra o link exibido em seu terminal em um navegador: firefox http://127.0.0.1:8888/lab?token = 6d37701a6787bd58a7f92f29f33709970df11a742bf3b79b","title":"Jupyter Notebook"},{"location":"tools/environment/#dagster","text":"Para a execu\u00e7\u00e3o da vers\u00e3o Dagster, fez-se a cria\u00e7\u00e3o da Docker Image research-processing-dagster . Essa Docker Image possui o Dagster (vers\u00e3o 0.12.15 ), junto ao DagIt , uma interface web para configurar e interagir com o Dagster. Tem-se tamb\u00e9m uma instala\u00e7\u00e3o do Docker, para que os scripts sejam capazes de operar e criar outros Docker Containers de processamento. Vari\u00e1veis de ambiente Para a utiliza\u00e7\u00e3o da research-processing-dagster , \u00e9 necess\u00e1rio definir a seguinte vari\u00e1vel de ambiente: DATA_DIRECTORY (Obrigat\u00f3rio) Vari\u00e1vel de ambiente para determinar o diret\u00f3rio, na m\u00e1quina local, onde os dados baixados devem ser salvos. Volumes A execu\u00e7\u00e3o do research-processing-dagster deve ser realizada com a defini\u00e7\u00e3o dos seguintes volumes Docker volumes : Volume de dados (Obrigat\u00f3rio) Volume onde os dados ser\u00e3o armazenados. Seguindo o modelo de execu\u00e7\u00e3o das fun\u00e7\u00f5es de processamento utilizadas nos scripts , esse volume ser\u00e1 utilizado por fun\u00e7\u00f5es dentro do mesmo container ( Local ) ou em outros containers ( Containerized ). Desta forma, a defini\u00e7\u00e3o desse volume deve ser feita de modo a atender duas exig\u00eancias: O volume deve ser do tipo Bind Mount ; O mapeamento do volume ( Bind Mount ) deve utilizar, na m\u00e1quina local e no Container, o mesmo caminho definido na vari\u00e1vel DATA_DIRECTORY . Com essas defini\u00e7\u00f5es, o volume ser\u00e1 vis\u00edvel dentro do Container da research-processing-dagster e tamb\u00e9m pelos Containers auxiliares de processamentos. Volume Daemon Socket Para que os scripts sejam capazes de gerar Docker Containers de processamento, \u00e9 necess\u00e1rio a defini\u00e7\u00e3o do Daemon Socket como um volume. Ao fazer isso, o Docker dentro do container criado com a imagem research-processing-dagster \u00e9 capaz de interagir com o Docker da m\u00e1quina externa, permitindo que containers de processamento sejam criados. Exemplo de utiliza\u00e7\u00e3o (Docker CLI ) Abaixo, faz-se a apresenta\u00e7\u00e3o de um exemplo de uso da Docker Image research-processing-dagster atrav\u00e9s da Docker CLI : Formata\u00e7\u00e3o do comando O comando abaixo \u00e9 criado para ser dit\u00e1dico. Caso voc\u00ea queira utiliza-lo, n\u00e3o esque\u00e7a de substituir os valores e remover os espa\u00e7os entre cada linha. docker run \\ --name research-processing-dagster \\ # Vari\u00e1veis de ambiente --env DATA_DIRECTORY = /my-data-dir \\ # Volume: Volume de dados --volume /my-data-dir:/my-data-dir \\ # Volume: Volume Daemon Socket --volume /var/run/docker.sock:/var/run/docker.sock:ro \\ # Porta de rede por onde o servi\u00e7o poder\u00e1 ser acessado --publish 3000 :3000 \\ # Especifica\u00e7\u00e3o da Docker Image brazildatacube/research-processing-dagster:latest Ap\u00f3s a execu\u00e7\u00e3o do comando acima, um resultado parecido com o apresentado abaixo deve aparecer: # (Omitted) Welcome to Dagster! If you have any questions or would like to engage with the Dagster team, please join us on Slack ( https://bit.ly/39dvSsF ) . Serving on http://0.0.0.0:3000 in process 1 Ap\u00f3s a execu\u00e7\u00e3o deste comando, utilizando um navegador e acesse o endere\u00e7o do DagIt apresentado: firefox http://127.0.0.1:3000 Exemplo de utiliza\u00e7\u00e3o (Docker Compose) Abaixo, o mesmo exemplo de execu\u00e7\u00e3o feito com a Docker CLI \u00e9 realizado com Docker Compose . Para isso, primeiro fez-se a cria\u00e7\u00e3o do arquivo docker-compose.yml com o seguinte conte\u00fado: docker-compose.yml version: '3.2' services: my-dagster: image: brazildatacube/research-processing-dagster:latest environment: # Vari\u00e1veis de ambiente - DATA_DIRECTORY=/my-data-dir volumes: # Volume: Volume de dados - type: bind source: /my-data-dir target: /my-data-dir # Volume: Volume Daemon Socket - type: bind source: /var/run/docker.sock target: /var/run/docker.sock ports: # Porta de rede por onde o servi\u00e7o poder\u00e1 ser acessado - \"3000:3000\" Com o arquivo criado, fez-se a execu\u00e7\u00e3o do compose: docker-compose -f docker-compose.yml up A sa\u00edda do comando acima \u00e9 parecida com: # (Omitted) Welcome to Dagster! If you have any questions or would like to engage with the Dagster team, please join us on Slack ( https://bit.ly/39dvSsF ) . Serving on http://0.0.0.0:3000 in process 1 Agora, utilizando como base o endere\u00e7o http://0.0.0.0:3000 exibido na tela, acesse o Dagster em seu navegador: firefox http://127.0.0.1:3000","title":"Dagster"},{"location":"tools/environment/#example-toolkit-environment","text":"Para facilitar a utiliza\u00e7\u00e3o do Example toolkit , fez-se a cria\u00e7\u00e3o da Docker Image example-toolkit-docker . Esta Docker Image, possui todas as depend\u00eancias necess\u00e1rias para a execu\u00e7\u00e3o do Example toolkit . Nos t\u00f3picos abaixo, \u00e9 feita a apresenta\u00e7\u00e3o das configura\u00e7\u00f5es necess\u00e1rias para utilizar essa Docker Image. Exemplos de utiliza\u00e7\u00e3o com a CLI do Docker e Docker Compose , tamb\u00e9m ser\u00e3o apresentados. Vari\u00e1veis de ambiente Na utiliza\u00e7\u00e3o do Example toolkit , toda a configura\u00e7\u00e3o \u00e9 feita atrav\u00e9s de vari\u00e1veis de ambiente. Na example-toolkit-docker , manteve-se o mesmo padr\u00e3o. Sendo assim, antes de realizar a execu\u00e7\u00e3o dessa Image, deve-se definir as vari\u00e1veis de ambiente obrigat\u00f3rias. As mesmas vari\u00e1veis de ambiente v\u00e1lidas para o Example toolkit s\u00e3o v\u00e1lidas para a example-toolkit-docker . Para verificar a lista completa das vari\u00e1veis de ambiente do Example toolkit , juntamente com a explica\u00e7\u00e3o de cada uma delas, consulte a Se\u00e7\u00e3o Example toolkit - Utiliza\u00e7\u00e3o . Volumes Para a utiliza\u00e7\u00e3o da example-toolkit-docker , \u00e9 necess\u00e1rio a defini\u00e7\u00e3o de alguns Docker Volumes. Esses volumes, especificam os dados de entrada, sa\u00edda, configura\u00e7\u00f5es e dados auxiliares. Abaixo, tem-se a apresenta\u00e7\u00e3o de cada um desses volumes: Volume de dados (Obrigat\u00f3rio) Diret\u00f3rio onde os dados baixados ser\u00e3o armazenados. Esse volume precisa ser criado no mesmo diret\u00f3rio definido na vari\u00e1vel de ambiente DOWNLOAD_OUTPUT_DIRECTORY (Configura\u00e7\u00e3o do Example toolkit ). Volume de configura\u00e7\u00e3o Dagster (Obrigat\u00f3rio) Diret\u00f3rio onde o arquivo de configura\u00e7\u00e3o Dagster gerado ser\u00e1 salvo. Esse volume precisa ser criado no mesmo diret\u00f3rio definido na vari\u00e1vel de ambiente PIPELINE_DIR (Configura\u00e7\u00e3o do Example toolkit ). Volume de configura\u00e7\u00e3o do download (Obrigat\u00f3rio) Arquivo de configura\u00e7\u00e3o com a refer\u00eancia aos dados que precisam ser baixado. O arquivo definido nesse volume deve ser o mesmo especificado na vari\u00e1vel DOWNLOAD_REFERENCE_FILE (Configura\u00e7\u00e3o do Example toolkit ). Exemplo de utiliza\u00e7\u00e3o (Docker CLI ) Abaixo, faz-se a execu\u00e7\u00e3o da Docker Image, identificada com a tag example-toolkit-docker : Formata\u00e7\u00e3o do comando O comando abaixo \u00e9 criado para ser dit\u00e1dico. Caso voc\u00ea queira utiliza-lo, n\u00e3o esque\u00e7a de substituir os valores e remover os espa\u00e7os entre cada linha. docker run \\ --name example-toolkit-docker \\ # Vari\u00e1veis de ambiente --env RAW_DATA_DIR = /compendium/data/raw_data \\ --env DERIVED_DATA_DIR = /compendium/data/derived_data \\ --env PIPELINE_DIR = /compendium/config \\ --env DOWNLOAD_OUTPUT_DIRECTORY = /compendium/data \\ --env DOWNLOAD_REFERENCE_FILE = /compendium/config/example-toolkit.json \\ # Volume: Volume de dados --volume /my-data/dir:/compendium/data \\ # Volume: Volume de configura\u00e7\u00e3o Dagster --volume /my-dagster/dir:/compendium/config \\ # Volume: Volume de configura\u00e7\u00e3o do download --volume /my-toolkit/config.json:/compendium/config/example-toolkit.json \\ # Especifica\u00e7\u00e3o da Docker Image brazildatacube/example-toolkit-docker:latest Ap\u00f3s a execu\u00e7\u00e3o do comando acima, dever\u00e1 ser produzir um resultado parecido com o apresentado abaixo: # (Omitted) 2022 -04-30 14 :59:16.525 | INFO | pipeline_steps:download_data_files_from_github:59 - Downloading minimal-example_landsat8_data.zip ( Sometimes the progress bar may not appear. Please, be patient. ) 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 2 .05G/2.05G [ 03 :43< 00 :00, 9 .17MB/s ] 2022 -04-30 15 :03:32.059 | INFO | pipeline_steps:download_data_files_from_github:59 - Downloading minimal-example_lasrc_auxiliary_data.zip ( Sometimes the progress bar may not appear. Please, be patient. ) 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 341M/341M [ 00 :35< 00 :00, 9 .57MB/s ] 2022 -04-30 15 :04:44.977 | INFO | pipeline_steps:download_data_files_from_github:59 - Downloading minimal-example_scene_id_list.zip ( Sometimes the progress bar may not appear. Please, be patient. ) 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 2 .17k/2.17k [ 00 :00< 00 :00, 1 .16MB/s ] 2022 -04-30 15 :04:45.690 | INFO | pipeline_steps:download_data_files_from_github:59 - Downloading minimal-example_sentinel2_data.zip ( Sometimes the progress bar may not appear. Please, be patient. ) 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 1 .14G/1.14G [ 02 :12< 00 :00, 8 .59MB/s ] 2022 -04-30 15 :07:15.765 | INFO | pipeline_steps:download_data_files_from_github:92 - All files are downloaded. Exemplo de utiliza\u00e7\u00e3o (Docker Compose) Abaixo, o mesmo exemplo de execu\u00e7\u00e3o feito com a Docker CLI \u00e9 realizado com Docker Compose . Para isso, primeiro fez-se a cria\u00e7\u00e3o do arquivo docker-compose.yml com o seguinte conte\u00fado: docker-compose.yml version: '3.2' services: my-dagster: # Especifica\u00e7\u00e3o da Docker Image image: brazildatacube/example-toolkit-docker:latest environment: # Vari\u00e1veis de ambiente - RAW_DATA_DIR=/compendium/data/raw_data - DERIVED_DATA_DIR=/compendium/data/derived_data - PIPELINE_DIR=/compendium/config - DOWNLOAD_OUTPUT_DIRECTORY=/compendium/data - DOWNLOAD_REFERENCE_FILE=/compendium/config/example-toolkit.json volumes: # Volume: Volume de dados - type: bind source: /my-data/dir target: /compendium/data # Volume: Volume de configura\u00e7\u00e3o Dagster - type: bind source: /my-dagster/dir target: /compendium/config # Volume: Volume de configura\u00e7\u00e3o do download - type: bind source: /my-toolkit/config.json target: /compendium/config/example-toolkit.json ports: # Porta de rede por onde o servi\u00e7o poder\u00e1 ser acessado - \"3000:3000\" Com o arquivo criado, fez-se a execu\u00e7\u00e3o do compose: docker-compose -f docker-compose.yml up A sa\u00edda do comando acima \u00e9 parecida com: # (Omitted) Welcome to Dagster! If you have any questions or would like to engage with the Dagster team, please join us on Slack ( https://bit.ly/39dvSsF ) . Serving on http://0.0.0.0:3000 in process 1","title":"Example toolkit environment"},{"location":"tools/environment/#maquina-virtual-com-vagrant","text":"Todos os recursos disponibilizados neste RC , foram desenvolvidos, testados e utilizados em ambiente Linux . Especificamente, fez-se o uso do Ubuntu 20.04 . Testes com o Ubuntu 20.10 tamb\u00e9m foram realizados. Em teoria, os c\u00f3digos executados, podem ser adaptados e utilizados em outros sistemas operacionais, como Windows e MacOS . No entanto, \u00e9 importante notar que, n\u00e3o existe uma garantia de que todos os comandos, configura\u00e7\u00f5es e depend\u00eancias utilizadas estar\u00e3o dispon\u00edveis para outros ambientes. Mesmo com o uso do Docker, pode ser que caracter\u00edsticas espec\u00edficas utilizadas, como os Daemon Socket , n\u00e3o estejam dispon\u00edveis. Para resolver este problema e evitar que o sistema operacional seja uma barreira para a reprodu\u00e7\u00e3o e replica\u00e7\u00e3o do material deste RC , fez-se a cria\u00e7\u00e3o de uma M\u00e1quina Virtual (VM, do ingl\u00eas Virtual Machine ). Com uma VM, diferente do Docker, tem-se a virtualiza\u00e7\u00e3o de um sistema completo, o que remove qualquer depend\u00eancia com o sistema adjacente. A cria\u00e7\u00e3o dessa VM, foi realizada com o aux\u00edlio do Vagrant , uma ferramenta que facilita o gerenciamento e provisionamento de m\u00e1quinas virtuais, desenvolvida pela Hashicorp . Vagrant est\u00e1 dispon\u00edvel para Windows, Linux, MacOS e outros sistemas operacionais. Com essa ferramenta, atrav\u00e9s de um arquivo de descri\u00e7\u00e3o ( Vagrantfile ), \u00e9 poss\u00edvel especificar uma VM completa, considerando elementos como: Quantidade de mem\u00f3ria RAM; Quantidade de CPU; Sistema operacional; Rede; Pacotes instalados. Al\u00e9m dessas, muitas outras configura\u00e7\u00f5es est\u00e3o dispon\u00edveis. Utilizando dessas caracter\u00edsticas, neste RC , fez-se a cria\u00e7\u00e3o de um Vagrantfile que especifica uma VM Ubuntu 20.04 , j\u00e1 preparada com as principais depend\u00eancias necess\u00e1rias para uso dos materiais deste RC (e.g., Docker, Docker Compose). Por padr\u00e3o, a m\u00e1quina \u00e9 criada com 12 GB de RAM e 8 CPUs . Recursos da VM A quantidade de recursos definida para a VM foi feita considerando como base uma m\u00e1quina de 24 GB de RAM e 12 CPUs. Caso seja necess\u00e1rio, atrav\u00e9s do Vagrantfile , esses valores podem ser alterados. Para isso, altere as seguintes propriedades dispon\u00edveis no arquivo: vb.memory = \"12288\" # 12 GB vb.cpus = \"8\" O Vagrant suporta v\u00e1rios Providers , que s\u00e3o as ferramentas utilizadas para criar as VMs. Neste RC , fez-se o uso do Provider Open Source VirtualBox .","title":"M\u00e1quina virtual com Vagrant  "},{"location":"tools/environment/#instalacao-do-vagrant","text":"Para come\u00e7ar a utilizar a VM atrav\u00e9s do Vagrant, o primeiro passo \u00e9 realizar a instala\u00e7\u00e3o do Vagrant propriamente dito. Para isso, recomenda-se a utiliza\u00e7\u00e3o da documenta\u00e7\u00e3o oficial .","title":"Instala\u00e7\u00e3o do Vagrant"},{"location":"tools/environment/#utilizacao-da-vm-via-vagrant","text":"Uma vez que o Vagrant est\u00e1 instalado em seu sistema, para a cria\u00e7\u00e3o da VM, o primeiro passo \u00e9 realizar o clone do reposit\u00f3rio onde est\u00e3o todos os materiais deste RC : git clone https://github.com/brazil-data-cube/compendium-harmonization Ap\u00f3s o clone, acesse o diret\u00f3rio compendium-harmonization : cd compendium-harmonization Dentro do reposit\u00f3rio, voc\u00ea ser\u00e1 capaz de ver todos os materiais deste RC : ls -lha #> -rwxrwxrwx 1 felipe felipe 368 Apr 9 20:01 .dockerignore #> drwxrwxrwx 1 felipe felipe 512 Apr 14 17:00 .git #> drwxrwxrwx 1 felipe felipe 512 Apr 15 22:53 .github #> -rwxrwxrwx 1 felipe felipe 4.3K Apr 10 08:42 .gitignore #> -rwxrwxrwx 1 felipe felipe 1.1K Apr 9 20:01 LICENSE #> -rwxrwxrwx 1 felipe felipe 2.7K Apr 30 18:38 Makefile #> -rwxrwxrwx 1 felipe felipe 4.5K Apr 9 20:01 README.md #> -rwxrwxrwx 1 felipe felipe 3.4K Apr 15 22:53 Vagrantfile #> drwxrwxrwx 1 felipe felipe 512 Apr 14 17:00 analysis #> -rwxrwxrwx 1 felipe felipe 1.4K Apr 10 08:19 bootstrap.sh #> drwxrwxrwx 1 felipe felipe 512 Apr 14 17:00 composes #> drwxrwxrwx 1 felipe felipe 512 Apr 14 17:00 docker #> -rwxrwxrwx 1 felipe felipe 383 Apr 10 07:39 setenv.sh #> drwxrwxrwx 1 felipe felipe 512 Apr 14 17:00 tools Dentre esses arquivos, note que h\u00e1 dispon\u00edvel o arquivo Vagrantfile . Esse arquivo, como citado anteriormente, tem toda a especifica\u00e7\u00e3o da VM que deve ser criada. Para criar a VM com esse arquivo, utilize o seguinte comand: vagrant up #> Bringing machine 'default' up with 'virtualbox' provider... #> ==> default: Checking if box 'alvistack/ubuntu-20.04' version '20220415.1.1' is up to date... #> ==> default: A newer version of the box 'alvistack/ubuntu-20.04' for provider 'virtualbox' is #> ==> default: available! You currently have version '20220415.1.1'. The latest is version #> ==> default: '20220430.1.2'. Run `vagrant box update` to update. #> ==> default: Resuming suspended VM... #> ==> default: Booting VM... #> ==> default: Waiting for machine to boot. This may take a few minutes... #> default: SSH address: 127.0.0.1:2222 #> default: SSH username: vagrant #> default: SSH auth method: private key Ap\u00f3s a execu\u00e7\u00e3o desse arquivo, a VM j\u00e1 estar\u00e1 criada e pronta para ser utilizada. Neste caso, para acessar a VM, utilize o comando: vagrant ssh #> Welcome to Ubuntu 20.04.4 LTS (GNU/Linux 5.13.0-39-generic x86_64) #> * Documentation: https://help.ubuntu.com #> * Management: https://landscape.canonical.com #> * Support: https://ubuntu.com/advantage # (Omitted) #> vagrant@ubuntu:~$ Ao acessar o ambiente, voc\u00ea est\u00e1 pronto para utilizar os materiais disponibilizados neste RC . Por exemplo, caso voc\u00ea queira acessar os materiais do RC que voc\u00ea fez o clone para criar a VM, voc\u00ea pode acessar o diret\u00f3rio /compendium : Mudando de diret\u00f3rio cd /compendium Listando os arquivos ls -lha #> drwxrwxrwx 1 vagrant vagrant 0 Apr 14 20:00 analysis #> -rwxrwxrwx 1 vagrant vagrant 1.4K Apr 10 11:19 bootstrap.sh #> drwxrwxrwx 1 vagrant vagrant 0 Apr 14 20:00 composes #> drwxrwxrwx 1 vagrant vagrant 0 Apr 14 20:00 docker #> -rwxrwxrwx 1 vagrant vagrant 368 Apr 9 23:01 .dockerignore #> -rwxrwxrwx 1 vagrant vagrant 17 Apr 10 11:25 .env #> drwxrwxrwx 1 vagrant vagrant 0 Apr 16 01:53 .github #> -rwxrwxrwx 1 vagrant vagrant 4.3K Apr 10 11:42 .gitignore #> -rwxrwxrwx 1 vagrant vagrant 1.1K Apr 9 23:01 LICENSE #> -rwxrwxrwx 1 vagrant vagrant 2.7K Apr 30 21:38 Makefile #> -rwxrwxrwx 1 vagrant vagrant 4.5K Apr 9 23:01 README.md #> -rwxrwxrwx 1 vagrant vagrant 383 Apr 10 10:39 setenv.sh #> drwxrwxrwx 1 vagrant vagrant 4.0K Apr 14 20:00 tools #> drwxrwxrwx 1 vagrant vagrant 0 May 1 19:16 .vagrant #> -rwxrwxrwx 1 vagrant vagrant 3.4K Apr 16 01:53 Vagrantfile","title":"Utiliza\u00e7\u00e3o da VM via Vagrant"},{"location":"tools/libraries/","text":"Bibliotecas de software \u00b6 A reprodutibilidade nos permite refazer o caminho que foi trilhado originalmente para a obten\u00e7\u00e3o dos resultados de uma pesquisa cient\u00edfica. Com isso, outros pesquisadores e mesmo nossos pr\u00f3prios \"eus\" do futuro podem se beneficiar e entender o que fizemos. Isso da margem para que poss\u00edveis erros sejam facilmente detectados e corrigidos. Dentre as v\u00e1rias a\u00e7\u00f5es que podem ser realizadas para atingir a reprodutibilidade, est\u00e1 a capacidade de automatizar os passos j\u00e1 realizado. Isso tr\u00e1s uma s\u00e9rie de benef\u00edcios, dentre eles: Evita poss\u00edveis \"erros\" por leitura de resultados incorretos que n\u00e3o s\u00e3o mais v\u00e1lidos para a execu\u00e7\u00e3o corrente; Permite a verifica\u00e7\u00e3o de todo o fluxo de processamento (O que tamb\u00e9m ajuda na escrita do relat\u00f3rio / artigo); Diminui\u00e7\u00e3o do overhead de execu\u00e7\u00e3o do trabalho. H\u00e1 alguns anos atr\u00e1s, essa caracter\u00edstica, era limitada nas pesquisas, por conta do uso de software com interfaces gr\u00e1ficas que acabam n\u00e3o permitindo a automa\u00e7\u00e3o do trabalho realizado atrav\u00e9s dos \"bot\u00f5es e clicks\" na tela. Hoje em dia, com o aumento e dissemina\u00e7\u00e3o de linguagens de alto n\u00edvel para o desenvolvimento de pesquisas, como R e Python , isso mudou por completo. Podemos automatizar as tarefas de forma mais f\u00e1cil. Al\u00e9m disso, a partir do momento que um script de processamento \u00e9 criado, toda a l\u00f3gica realmente aplicada nos dados \u00e9 descrita de forma clara e direta. Neste RC , para que todos as etapas pudessem ser modeladas atrav\u00e9s de scripts , fez-se a cria\u00e7\u00e3o de diversas bibliotecas de c\u00f3digo Python. Cada biblioteca possui uma responsabilidade \u00fanica, o que nos ajudou a manter o trabalho organizado durante seu desenvolvimento. Ao mesmo tempo, essa abordagem facilita a reutiliza\u00e7\u00e3o dos c\u00f3digos criados. Aqueles que desejam reutilizar o trabalho desenvolvido, podem faz\u00ea-lo com a importa\u00e7\u00e3o dessas bibliotecas em seu projeto Python. Bibliotecas de c\u00f3digo dispon\u00edveis \u00b6 Para a ado\u00e7\u00e3o dessa abordagem de desenvolvimento, baseada em bibliotecas de responsabilidade \u00fanica, que pudessem ser reutilizadas, fez-se necess\u00e1rio a ado\u00e7\u00e3o de alguns crit\u00e9rios que nos ajudasse a manter tudo organizado e reutiliz\u00e1vel. Al\u00e9m disso, nossa vis\u00e3o com a cria\u00e7\u00e3o de v\u00e1rias bibliotecas para a composi\u00e7\u00e3o dos scripts de processamento do trabalho \u00e9 que essas, devem ser modeladas de modo a possibilitar seu uso conjunto. Como blocos que podem ser juntados para construir um muro. Considerando essas caracter\u00edsticas, inicialmente foi definido que neste RC , dois tipos de bibliotecas seriam desenvolvidas: Base Bibliotecas que fornecem os recursos e funcionalidades base para a realiza\u00e7\u00e3o de uma a\u00e7\u00e3o (e.g., Gera\u00e7\u00e3o de bandas de \u00e2ngulos) Aplica\u00e7\u00e3o Bibliotecas que atrav\u00e9s da uni\u00e3o de bibliotecas Base , disponibiliza funcionalidades que permitem a aplica\u00e7\u00e3o de diferentes metodologias e fluxos de processamento (e.g., Gera\u00e7\u00e3o de produtos harmonizados). Partindo dessas defini\u00e7\u00f5es, para a produ\u00e7\u00e3o deste RC e a gera\u00e7\u00e3o de seus resultados, fez-se o desenvolvimento de duas bibliotecas Base : s2-angs Disponibiliza funcionalidades para a gera\u00e7\u00e3o de bandas \u00e2ngulos de imagens Sentinel-2; sensor-harm Permite a gera\u00e7\u00e3o de produtos harmonizados. Essas bibliotecas est\u00e3o dispon\u00edveis e podem ser instaladas como qualquer outra biblioteca da linguagem Python e utilizada em diferentes projetos. Assim, caso seja de seu interesse, \u00e9 poss\u00edvel, por exemplo, instalar a biblioteca sensor-harm em seu projeto Python e utilizar as funcionalidades disponibilizadas para a gera\u00e7\u00e3o de produtos harmonizados. Para ambas as bibliotecas a \u00fanica restri\u00e7\u00e3o est\u00e1 no respeito aos formatos de entradas esperados pelas fun\u00e7\u00f5es dessas bibliotecas. Ao seguir corretamente, voc\u00ea n\u00e3o dever\u00e1 ter problemas para fazer sua utiliza\u00e7\u00e3o. Com base nessas duas bibliotecas, fez-se a cria\u00e7\u00e3o de uma biblioteca Aplica\u00e7\u00e3o : research-processing Fornece funcionalidades que permitem a aplica\u00e7\u00e3o da metodologia de processamento de dados utilizadas na gera\u00e7\u00e3o dos resultados deste RC . Parte de suas funcionalidades \u00e9 criada com base nas bibliotecas s2-angs e sensor-harm . Para resumir, a rela\u00e7\u00e3o entre essas bibliotecas \u00e9 sumarizada na Figura abaixo. Libraries organization Detalhes do funcionamento e funcionalidades de cada uma dessas bibliotecas s\u00e3o apresentados nas se\u00e7\u00f5es a seguir. Bibliotecas na metodologia dos experimentos \u00b6 Para que se tenha uma ideia onde cada uma das bibliotecas apresentadas anteriormente s\u00e3o utilizadas na metodologia de processamento realizada neste RC , tem-se na Figura abaixo a rela\u00e7\u00e3o de cada uma das etapas e a biblioteca utilizada. Processing workflow with libraries Especifica\u00e7\u00e3o das bibliotecas \u00b6 Nesta se\u00e7\u00e3o, de forma complementar a vis\u00e3o geral apresentada at\u00e9 aqui, \u00e9 feita a especifica\u00e7\u00e3o das funcionalidades e forma de uso de cada uma das bibliotecas mencionadas anteriormente. Sentinel-2 Angle Generator Python Library (s2-angs) \u00b6 A biblioteca s2-angs , como mencionado anteriormente, \u00e9 respons\u00e1vel pela gera\u00e7\u00e3o de bandas de \u00e2ngulos para imagens Sentinel-2. Essas bandas cont\u00e9m informa\u00e7\u00f5es, por pixel, de \u00e2ngulos solar azimutal ( SAA ), solar zenital ( SZA ), sensor azimutal ( VAA ) e sensor zenital ( VZA ). Essas informa\u00e7\u00f5es s\u00e3o extra\u00eddas dos metadados de imagens Sentinel-2. Inicialmente esses dados s\u00e3o fornecidos em forma de matrizes de 23x23 (linhas X colunas), ou seja, em uma resolu\u00e7\u00e3o espacial de aproximadamente 5000 metros. Entretanto, essa informa\u00e7\u00e3o precisa estar em resolu\u00e7\u00e3o espacial equivalente \u00e0 das bandas espectrais do sensor ( 10 , 20 ou 60 metros) para que possam ser aproveitadas em corre\u00e7\u00f5es por pixel. Assim, a biblioteca s2-angs \u00e9 capaz de estimar os \u00e2ngulos e salv\u00e1-slos arquivos .tif , tanto em sua resolu\u00e7\u00e3o espacial original, quanto reamostrados para a resolu\u00e7\u00e3o espacial das bandas do sensor. Com isso, pode-se listar como funcionalidades principais dessa biblioteca: Gera\u00e7\u00e3o de bandas de \u00e2ngulos ( SAA , SZA , VAA e VZA ); Reamostragem das bandas de \u00e2ngulos para resolu\u00e7\u00e3o de bandas do pr\u00f3prio sensor. Principais opera\u00e7\u00f5es dispon\u00edveis \u00b6 A tabela abaixo apresenta um resumo das principais opera\u00e7\u00f5es que est\u00e3o dispon\u00edveis na biblioteca s2-angs . Function Description s2_angs.gen_s2_ang Function to generate the Sentinel-2 Angle bands Exemplo de utiliza\u00e7\u00e3o \u00b6 Para realizar a gera\u00e7\u00e3o de banda de \u00e2ngulo utilizando a biblioteca s2_angs , basta fazer o uso da fun\u00e7\u00e3o gen_s2_ang . Essa fun\u00e7\u00e3o aceita como entrada .zip , .SAFE directory ou diret\u00f3rio com imagens Sentinel-2 . No bloco de c\u00f3digo abaixo, tem-se um exemplo em que um arquivo .zip \u00e9 utilizado como entrada na fun\u00e7\u00e3o: s2-angs example code 1 2 3 4 5 import s2_angs s2_angs . gen_s2_ang ( \"S2B_MSIL1C_20191223T131239_N0208_R138_T23KMR_20191223T135458.zip\" ) O c\u00f3digo acima far\u00e1 a gera\u00e7\u00e3o das bandas de \u00e2ngulos da imagem definida na entrada. Exemplos de resultados podem ser visualizados nas figuras abaixo: Intermediary results (matrix 23x23) Solar Azimuth Solar Zenith View Azimuth View Zenith Solar Azimuth intermediary result Solar Zenith intermediary result View Azimuth intermediary result View Zenith intermediary result Resampled results Solar Azimuth Solar Zenith View Azimuth View Zenith Solar Azimuth resampled result Solar Zenith resampled result View Azimuth resampled result View Zenith resampled result Para mais informa\u00e7\u00f5es, por favor, consulte o reposit\u00f3rio oficial da biblioteca s2-angs . Sensor Harmonization Python Library (sensor-harm) \u00b6 Neste RC , parte dos resultados consiste em produtos harmonizados, ou seja, produtos de n\u00edvel de reflect\u00e2ncia de superf\u00edcie com corre\u00e7\u00e3o para os efeitos de Fun\u00e7\u00e3o de distribui\u00e7\u00e3o de reflect\u00e2ncia bidirecional (BRDF) e ajustes espectrais. Para isso, fez-se a cria\u00e7\u00e3o da biblioteca sensor-harm . Nessa biblioteca, a corre\u00e7\u00e3o BRDF \u00e9 feita utilizando o m\u00e9todo do c-factor para gerar produtos Nadir BRDF-Adjusted Reflectance (NBAR), enquanto que o ajuste espectral \u00e9 feito utilizando bandpass adotando as imagens Landsat-8 como refer\u00eancia. Com o uso dessa biblioteca, esses m\u00e9todos podem ser aplicados em imagens dos sat\u00e9lites-sensores Landsat-4/TM, Landsat-5/TM, Landsat-7/ETM+, Landsat-8/OLI e Sentinel-2/MSI, al\u00e9m de permitir a harmoniza\u00e7\u00e3o entre esses diferentes dados. A biblioteca apresenta duas fun\u00e7\u00f5es principais, uma para harmonizar imagens provenientes de sensores a bordo dos sat\u00e9lites Landsat e outra para imagens provenientes dos sensores a bordo dos sat\u00e9lites Sentinel-2. Principais opera\u00e7\u00f5es dispon\u00edveis \u00b6 A tabela abaixo apresenta um resumo das principais opera\u00e7\u00f5es que est\u00e3o dispon\u00edveis na biblioteca sensor-harm . Function Description sensor_harm.landsat.landsat_harmonize Function to harmonize Landsat data sensor_harm.sentinel2.sentinel_harmonize Function to harmonize Sentinel-2 data Exemplo de utiliza\u00e7\u00e3o \u00b6 Para realizar a harmoniza\u00e7\u00e3o de dados, seja esse Landsat-4/TM, Landsat-5/TM, Landsat-7/ETM+, Landsat-8/OLI ou Sentinel-2, \u00e9 necess\u00e1rio definir o diret\u00f3rio onde os dados de entrada est\u00e3o armazenados, bem como o diret\u00f3rio de sa\u00edda. Para exemplificar o uso dessa fun\u00e7\u00e3o, o bloco de c\u00f3digo abaixo \u00e9 um exemplo de como a biblioteca sensor-harm pode ser utilizada para harmoniza\u00e7\u00e3o de dados Sentinel-2: sensor-harm example code 1 2 3 4 5 6 from sensor_harm.sentinel2 import sentinel_harmonize sentinel2_entry = '/path/to/S2/SR/images/' target_dir = '/path/to/output/NBAR/' sentinel_harmonize ( sentinel2_entry , target_dir , apply_bandpass = True ) O c\u00f3digo acima far\u00e1 a gera\u00e7\u00e3o das bandas de \u00e2ngulos da imagem definida na entrada. Exemplos de resultados podem ser visualizados nas figuras abaixo: Sentinel-2/MSI Harmonized Data NBAR Band 02 (10m) NBAR Band 03 (10m) NBAR Band 04 (10m) NBAR Band 08 (10m) NBAR Band 08A (20m) NBAR Band 11 (20m) NBAR Band 12 (20m) Sentinel-2/MSI Image (B02) NBAR 10m Sentinel-2/MSI Image (B03) NBAR 10m Sentinel-2/MSI Image (B04) NBAR 10m Sentinel-2/MSI Image (B08) NBAR 10m Sentinel-2/MSI Image (B8A) NBAR 20m Sentinel-2/MSI Image (B11) NBAR 20m Sentinel-2/MSI Image (B12) NBAR 20m Para mais informa\u00e7\u00f5es, por favor, consulte o reposit\u00f3rio oficial da biblioteca sensor-harm . Research Processing Python Library (research-processing) \u00b6 A metodologia de processamento realizada neste RC , conforme apresentado nas se\u00e7\u00f5es anteriores, possui diversas etapas, as quais dependem de diferentes ferramentas de software. Consequentemente, a realiza\u00e7\u00e3o de todo o fluxo de processamento pode exigir: Instala\u00e7\u00e3o de depend\u00eancias de software espec\u00edficas para cada etapa de processamento; Configura\u00e7\u00f5es espec\u00edficas no ambiente de software para cada etapa de processamento. Com essas exig\u00eancias, a execu\u00e7\u00e3o e reprodu\u00e7\u00e3o do fluxo de processamento poderiam ser problem\u00e1ticas para n\u00f3s e para aqueles que desejassem reproduzir ou mesmo aplicar o trabalho desenvolvido. Como forma de evitar esses poss\u00edveis problemas e facilitar a materializa\u00e7\u00e3o da metodologia de processamento deste RC , n\u00f3s desenvolvemos a biblioteca research-processing . Nessa biblioteca, todas as etapas da metodologia s\u00e3o modeladas como fun\u00e7\u00f5es Python que podem ser facilmente utilizadas na constru\u00e7\u00e3o de qualquer fluxo de processamento. Adicionalmente, parte das opera\u00e7\u00f5es que exigem configura\u00e7\u00f5es espec\u00edficas de ambiente, s\u00e3o executadas em Docker Containers , de forma transparente aos usu\u00e1rios da biblioteca. Ao todo, tem-se no research-processing , funcionalidades para a realiza\u00e7\u00e3o de a\u00e7\u00f5es como: Pr\u00e9-processamento Corre\u00e7\u00e3o atmosf\u00e9rica Sentinel-2/MSI (Sen2Cor e LaSRC); Gera\u00e7\u00e3o de bandas de \u00e2ngulos (Landsat-8/OLI e Sentinel-2/MSI); Gera\u00e7\u00e3o de produtos NBAR (Sentinel-2/MSI e Landsat-8/OLI). An\u00e1lise de dados Rotinas de valida\u00e7\u00e3o das corre\u00e7\u00f5es realizadas. Abordagem de execu\u00e7\u00e3o das fun\u00e7\u00f5es \u00b6 Assim como mencionado anteriormente, as funcionalidades do research-processing s\u00e3o modeladas como fun\u00e7\u00f5es reutiliz\u00e1veis com execu\u00e7\u00e3o amig\u00e1vel a reprodu\u00e7\u00e3o. Para que essas caracter\u00edsticas pudessem ser garantidas, adotou-se na biblioteca durante a implementa\u00e7\u00e3o das fun\u00e7\u00f5es, o conceito de modelo de execu\u00e7\u00e3o , o qual determina onde/como a fun\u00e7\u00e3o ser\u00e1 executada, de forma transparente ao usu\u00e1rio. Cada uma das fun\u00e7\u00f5es implementadas na biblioteca possui um modelo de execu\u00e7\u00e3o , permitindo que diferentes tecnologias e abordagens sejam utilizadas como base para a execu\u00e7\u00e3o da fun\u00e7\u00e3o. Partindo das necessidades das fun\u00e7\u00f5es desse RC , fez-se o uso de dois modelos de execu\u00e7\u00e3o nas implementa\u00e7\u00f5es realizadas, sendo eles: Local Fun\u00e7\u00f5es implementadas com o modelo de execu\u00e7\u00e3o Local , s\u00e3o fun\u00e7\u00f5es Python simples. Essas fun\u00e7\u00f5es n\u00e3o possuem nenhum formato de execu\u00e7\u00e3o especial, sendo executados diretamente no ambiente/interpretador que a invocou. Fun\u00e7\u00f5es com esse formato, dependem do ambiente adjacente, sendo necess\u00e1rio para esse, ter todas as depend\u00eancias instaladas e devidamente configuradas. Containerized Diferente de fun\u00e7\u00f5es Local , as fun\u00e7\u00f5es Containerized n\u00e3o dependem do ambiente adjacente. Isso porque fun\u00e7\u00f5es implementadas com esse formato, ao serem executadas, criam um Docker Container , com o ambiente completo necess\u00e1rio para a execu\u00e7\u00e3o da opera\u00e7\u00e3o associada a fun\u00e7\u00e3o. A fun\u00e7\u00e3o \u00e9 executada dentro do ambiente criado. Ambos modelos de execu\u00e7\u00e3o, s\u00e3o transparentes para o usu\u00e1rios no momento da execu\u00e7\u00e3o. Assim, independente da forma com que a implementa\u00e7\u00e3o foi realizada, o uso final \u00e9 o mesmo: Uma chamada a uma fun\u00e7\u00e3o Python. A diferen\u00e7a est\u00e1 nas exig\u00eancias que cada tipo de fun\u00e7\u00e3o far\u00e1 do ambiente onde est\u00e1 sendo executada. Para as fun\u00e7\u00f5es Local , como mencionado, ser\u00e1 necess\u00e1rio a instala\u00e7\u00e3o de todas as depend\u00eancias e configura\u00e7\u00f5es para que a fun\u00e7\u00e3o seja executada. Enquanto isso, as fun\u00e7\u00f5es Containerized exigir\u00e3o a disponibilidade do Docker no ambiente do usu\u00e1rio. Containerized - Permiss\u00f5es de usu\u00e1rio \u00c9 importante notar que, al\u00e9m do Docker instalado, \u00e9 necess\u00e1rio tamb\u00e9m que o usu\u00e1rio que est\u00e1 fazendo a execu\u00e7\u00e3o, tenha as devidas permiss\u00f5es para utilizar o Docker. Na Figura abaixo, faz-se uma representa\u00e7\u00e3o geral de cada um desses modelos utilizados. Note que, fun\u00e7\u00f5es Local , trabalham com o interpretador Python, enquanto fun\u00e7\u00f5es Containerized criam Docker Containers onde a execu\u00e7\u00e3o ser\u00e1 realizada. Research Processing Execution Models Com base nesses dois modos de opera\u00e7\u00e3o, a biblioteca implementa e disponibiliza as opera\u00e7\u00f5es de processamento e an\u00e1lise dos dados. Para as opera\u00e7\u00f5es de processamento, que utilizam ferramentas terceiras, como Sen2Cor e LaSRC , faz-se o uso do modo de opera\u00e7\u00e3o Containerized . J\u00e1 nas opera\u00e7\u00f5es de valida\u00e7\u00e3o, que dependem apenas de bibliotecas Python, usa-se o modo de opera\u00e7\u00e3o Local . Comunica\u00e7\u00e3o entre as fun\u00e7\u00f5es \u00b6 As funcionalidades da biblioteca research-processing podem ser utilizadas em conjunto, de modo a construir um fluxo de processamento que permita a materializa\u00e7\u00e3o da metodologia de experimento seguida nesse RC . Nesse contexto, um ponto a ser considerado sobre o funcionamento da biblioteca \u00e9 a forma com que as entradas e sa\u00eddas dessas fun\u00e7\u00f5es podem ser encadeadas, de modo que a sa\u00edda de uma fun\u00e7\u00e3o \u00e9 utilizada como entrada em outra fun\u00e7\u00e3o. No research-processing , as opera\u00e7\u00f5es implementadas s\u00e3o realizadas de modo a evitar a movimenta\u00e7\u00e3o de dado (Entradas e sa\u00eddas). Para isso, as fun\u00e7\u00f5es de processamento operam com base no caminho de dados . Com isso, as fun\u00e7\u00f5es recebem o caminho onde os dados est\u00e3o armazenados, bem como onde os resultados devem ser salvos. Tal modo de opera\u00e7\u00e3o permite melhores defini\u00e7\u00f5es em rela\u00e7\u00e3o ao local onde os dados ser\u00e3o carregados e salvos, evitando movimenta\u00e7\u00f5es desnecess\u00e1rias e armazenamento em locais que podem apresentar problemas com o limite de espa\u00e7o e baixo desempenho. Esse modo de funcionamento \u00e9 representado na figura abaixo. Modelo de comunica\u00e7\u00e3o entre fun\u00e7\u00f5es Com base nesse modo de funcionamento, as fun\u00e7\u00f5es s\u00e3o encadeadas e as sa\u00eddas de uma fun\u00e7\u00e3o, que representam o caminho para onde os dados foram salvos, s\u00e3o utilizados como entrada em outras fun\u00e7\u00f5es. Entradas como volumes Deve-se notar que essas informa\u00e7\u00f5es de caminho de dados, para as fun\u00e7\u00f5es Containerized , s\u00e3o utilizadas para a cria\u00e7\u00e3o de Docker Bind Mount Volumes . Assim, os Containers auxiliares tem acesso aos dados que devem ser processados. Ordem de execu\u00e7\u00e3o das opera\u00e7\u00f5es Com esse formato de entradas/sa\u00eddas das fun\u00e7\u00f5es, tem-se como premissa que a sa\u00edda de uma fun\u00e7\u00e3o ser\u00e1 entendida pela fun\u00e7\u00e3o seguinte. Sendo assim, as fun\u00e7\u00f5es dispon\u00edveis na biblioteca research-processing , devem ser encadeadas em uma ordem estrita, sendo nesse caso, a ordem descrita na metodologia. Para mais detalhes de como isso pode ser implementado, consulte a se\u00e7\u00e3o Scripts de processamento . Principais opera\u00e7\u00f5es dispon\u00edveis \u00b6 Para dar suporte a cria\u00e7\u00e3o de todo o fluxo de opera\u00e7\u00e3o implementado nesse RC , a biblioteca research-processing fornece diversas fun\u00e7\u00f5es e opera\u00e7\u00f5es auxiliares, as quais s\u00e3o listadas na tabela abaixo: Fun\u00e7\u00e3o Descri\u00e7\u00e3o Modelo de execu\u00e7\u00e3o research_processing.surface_reflectance.sen2cor Corre\u00e7\u00e3o Atmosf\u00e9rica Sen2Cor para Sentinel-2/MSI Containerized research_processing.surface_reflectance.lasrc Corre\u00e7\u00e3o atmosf\u00e9rica LaSRC para Sentinel-2/MSI Containerized research_processing.nbar.s2_sen2cor_nbar Gerador de produtos NBAR para dados Sentinel-2/MSI Reflect\u00e2ncia de superf\u00edcie com (Sen2Cor). Containerized research_processing.nbar.s2_lasrc_nbar Gerador de produtos NBAR para dados Sentinel-2/MSI Reflect\u00e2ncia de superf\u00edcie com (LaSRC). Containerized research_processing.nbar.lc8_nbar Gerador de produtos NBAR para dados Landsat-8/OLI Reflect\u00e2ncia de superf\u00edcie. Containerized research_processing.nbar.lc8_generate_angles Gerador de \u00e2ngulos para dados Landsat-8/OLI Containerized research_processing.validation.validation_routines An\u00e1lise de resultados (M\u00f3dulo com m\u00faltiplas fun\u00e7\u00f5es) Local Dentre essas fun\u00e7\u00f5es, h\u00e1 alguns detalhes que precisam ser considerados para o entendimento completo do motivo por tr\u00e1s de cada um dos modelos de execu\u00e7\u00e3o escolhidos para as fun\u00e7\u00f5es. Nos subt\u00f3picos abaixo, esses detalhes s\u00e3o apresentados: Corre\u00e7\u00e3o atmosf\u00e9rica Sentinel-2/MSI (Sen2Cor e LaSRC) O fluxo de processamento exige que as imagens utilizadas tenham corre\u00e7\u00e3o atmosf\u00e9rica. Para as imagens Landsat-8/OLI, n\u00e3o h\u00e1 a necessidade de fazer a corre\u00e7\u00e3o, uma vez que os produtos j\u00e1 s\u00e3o disponibilizados prontos para uso, com as devidas corre\u00e7\u00f5es geom\u00e9tricas e radiom\u00e9tricas realizadas. No entanto, para os dados Sentinel-2/MSI, isso n\u00e3o \u00e9 verdade. Assim, durante o desenvolvimento do artigo foi necess\u00e1rio realizar a corre\u00e7\u00e3o atmosf\u00e9rica desses dados. Para isso, adotou-se as ferramentas Sen2Cor e LaSRC . Essas ferramentas possuem suas necessidades espec\u00edficas no ambiente onde ser\u00e3o utilizadas, como depend\u00eancias e configura\u00e7\u00f5es para seu uso. Considerando isso, na biblioteca research-processing fez-se a cria\u00e7\u00e3o de fun\u00e7\u00f5es que operam essas ferramentas em um ambiente j\u00e1 configurado e pronto para uso. Esse ambiente \u00e9 executado em um Docker Container . Das fun\u00e7\u00f5es apresentadas na tabela acima, as listadas a seguir s\u00e3o utilizadas para aplicar a corre\u00e7\u00e3o atmosf\u00e9rica nos dados: research_processing.surface_reflectance.lasrc : Fun\u00e7\u00e3o que faz a aplica\u00e7\u00e3o de Corre\u00e7\u00e3o Atmosf\u00e9rica em dados Sentinel-2/MSI utilizando a ferramenta LaSRC. Todo o processamento \u00e9 realizado dentro de um Docker Container de forma transparente para o usu\u00e1rio; research_processing.surface_reflectance.sen2cor : Fun\u00e7\u00e3o que faz a aplica\u00e7\u00e3o de Corre\u00e7\u00e3o Atmosf\u00e9rica em dados Sentinel-2/MSI utilizando a ferramenta Sen2Cor. Todo o processamento \u00e9 realizado dentro de um Docker Container de forma transparente para o usu\u00e1rio. Entendendo os Containers A cria\u00e7\u00e3o de um Docker Container depende de uma Docker Image que define o ambiente e suas configura\u00e7\u00f5es. Isso n\u00e3o \u00e9 diferente no research-processing . Para a cria\u00e7\u00e3o dos Docker Containers de Corre\u00e7\u00e3o Atmosf\u00e9rica, s\u00e3o utilizadas as seguintes Docker Images : Corre\u00e7\u00e3o atmosf\u00e9rica com LaSRC: Docker Image LaSRC 2.0.1 Corre\u00e7\u00e3o atmosf\u00e9ria com Sen2COr: Docker Image Sen2Cor 2.9.0 Essas Docker Images foram criadas para execu\u00e7\u00e3o nesse RC . Para mais informa\u00e7\u00f5es, consulte a se\u00e7\u00e3o Ambientes computacionais . Gera\u00e7\u00e3o de bandas de \u00e2ngulos (Landsat-8/OLI e Sentinel-2/MSI) Para a gera\u00e7\u00e3o dos produtos NBAR, faz-se necess\u00e1rio que bandas de \u00e2ngulos (e.g., SAA , SZA , VAA and VZA ) sejam calculadas. Essas bandas s\u00e3o geradas de forma espec\u00edfica para cada dados/sensor que est\u00e1 sendo trabalhada. Com isso, faz-se necess\u00e1rio o uso de ferramentas especializadas para cada sensor: Landsat-8/OLI: A gera\u00e7\u00e3o de bandas de \u00e2ngulos para o Landsat-8/OLI \u00e9 feita atrav\u00e9s da ferramenta Landsat 8 Angles Creation Tools . Essa ferramenta possui suas pr\u00f3prias depend\u00eancias, exigindo tamb\u00e9m configura\u00e7\u00f5es espec\u00edficas no ambiente onde ser\u00e1 executada; Sentinel-2/MSI: Os \u00e2ngulos de dados Sentinel-2/MSI s\u00e3o gerados com a biblioteca Sentinel-2 Angle Generator Python Library (s2-angs) , desenvolvida nesse RC . Considerando essas caracter\u00edsticas, tem-se a seguinte fun\u00e7\u00e3o para executar essas opera\u00e7\u00f5es: research_processing.nbar.lc8_generate_angles : Fun\u00e7\u00e3o que atrav\u00e9s da ferramenta Landsat 8 Angles Creation Tools , realiza o c\u00e1lculo das bandas de \u00e2ngulos para dados Landsat-8/OLI. O processamento realizado por essa fun\u00e7\u00e3o \u00e9 feito dentro de um Docker Container . Na lista de fun\u00e7\u00f5es acima, n\u00e3o h\u00e1 nenhuma fun\u00e7\u00e3o espec\u00edfica para o Sentinel-2/MSI. Isso ocorre j\u00e1 que, durante a implementa\u00e7\u00e3o da biblioteca research-processing , foi decidido que a gera\u00e7\u00e3o de \u00e2ngulos para os dados Sentinel-2/MSI seria uma opera\u00e7\u00e3o integrada a gera\u00e7\u00e3o dos produtos NBAR. Assim, o c\u00e1lculo dos \u00e2ngulos necess\u00e1rios para a gera\u00e7\u00e3o dos produtos NBAR com dados Sentinel-2/MSI, feitos com a biblioteca s2-angs, s\u00e3o parte das seguintes fun\u00e7\u00f5es: research_processing.nbar.s2_sen2cor_nbar research_processing.nbar.s2_lasrc_nbar O processamento realizado por ambas as fun\u00e7\u00f5es listadas acima tamb\u00e9m \u00e9 feito dentro de um Docker Container . Entendendo os Containers A cria\u00e7\u00e3o de um Docker Container depende de uma Docker Image que define o ambiente e suas configura\u00e7\u00f5es. Isso n\u00e3o \u00e9 diferente no research-processing . Para a cria\u00e7\u00e3o do Docker Container de gera\u00e7\u00e3o de banda de \u00e2ngulos, \u00e9 feito o uso da seguinte Docker Image : Gera\u00e7\u00e3o de bandas de \u00e2ngulos para dados Landsat-8/OLI: Docker Image L8Angs Essas imagens foram criadas para execu\u00e7\u00e3o nesse RC . Para mais informa\u00e7\u00f5es, consulte a se\u00e7\u00e3o Ambientes computacionais . Gera\u00e7\u00e3o de produtos NBAR (Sentinel-2/MSI e Landsat-8/OLI) A base para a gera\u00e7\u00e3o dos produtos NBAR nesse RC , conforme apresentado nas se\u00e7\u00f5es anteriores, \u00e9 a biblioteca sensor-harm . Essa biblioteca, permite a gera\u00e7\u00e3o de produtos NBAR para dados Sentinel-2/MSI e Landsat-8/OLI. Como forma de facilitar a utiliza\u00e7\u00e3o da biblioteca sensor-harm e evitar que os utilizadores tenham de fazer instala\u00e7\u00f5es e configura\u00e7\u00f5es espec\u00edficas, na biblioteca research-processing foi realizada a implementa\u00e7\u00e3o das seguintes fun\u00e7\u00f5es: research_processing.nbar.lc8_nbar : Faz a gera\u00e7\u00e3o de produtos NBAR para dados Landsat-8/OLI research_processing.nbar.s2_lasrc_nbar : Faz a gera\u00e7\u00e3o de produtos NBAR para dados Sentinel-2/MSI com corre\u00e7\u00e3o atmosf\u00e9rica feita com a ferramenta LaSRC; research_processing.nbar.s2_sen2cor_nbar : Faz a gera\u00e7\u00e3o de produtos NBAR para dados Sentinel-2/MSI com corre\u00e7\u00e3o atmosf\u00e9rica feita com a ferramenta Sen2Cor. Essas fun\u00e7\u00f5es, s\u00e3o implementadas com o modelo de execu\u00e7\u00e3o Containerized. Assim, no momento em que o usu\u00e1rio faz a execu\u00e7\u00e3o dessas fun\u00e7\u00f5es, um Docker Container , com as devidas depend\u00eancias \u00e9 criado para executar a fun\u00e7\u00e3o. Entendendo os Containers A cria\u00e7\u00e3o de um Docker Container depende de uma Docker Image que define o ambiente e suas configura\u00e7\u00f5es. Isso n\u00e3o \u00e9 diferente no research-processing . Para a cria\u00e7\u00e3o do Docker Container de gera\u00e7\u00e3o de produtos NBAR (todas as fun\u00e7\u00f5es), \u00e9 feito o uso da seguinte Docker Image : Gera\u00e7\u00e3o de produtos NBAR (Sentinel-2/MSI e Landsat-8/OLI): Docker Image NBAR Essas imagens foram criadas para execu\u00e7\u00e3o nesse RC . Para mais informa\u00e7\u00f5es, consulte a se\u00e7\u00e3o Ambientes computacionais . Rotinas de valida\u00e7\u00e3o das corre\u00e7\u00f5es realizadas Para finalizar as funcionalidades, tem-se o m\u00f3dulo de valida\u00e7\u00e3o de corre\u00e7\u00f5es. Esse m\u00f3dulo ( research_processing.validation.validation_routines ), \u00e9 o respons\u00e1vel em fazer todas as compara\u00e7\u00f5es e c\u00e1lculos utilizados para avaliar os resultados gerados neste RC . De modo a tornar a depura\u00e7\u00e3o mais simples e direta, sua implementa\u00e7\u00e3o \u00e9 feita com o modelo de execu\u00e7\u00e3o Local. Com isso, o usu\u00e1rio que utilizar as fun\u00e7\u00f5es desse m\u00f3dulo, deve configurar o ambiente onde a execu\u00e7\u00e3o ser\u00e1 feita. Em resumo, tem-se apenas que instalar as depend\u00eancias da pr\u00f3pria biblioteca research-processing e ent\u00e3o o ambiente j\u00e1 estar\u00e1 pronto para executar essas fun\u00e7\u00f5es. Exemplo de utiliza\u00e7\u00e3o \u00b6 Para exemplificar a forma de utiliza\u00e7\u00e3o da biblioteca research-processing , abaixo \u00e9 apresentada a forma com que a biblioteca realiza a corre\u00e7\u00e3o atmosf\u00e9rica de imagens Sentinel-2/MSI, utilizando a ferramenta Sen2Cor . Corre\u00e7\u00e3o atmosf\u00e9rica Sen2Cor Sen2Cor atmosphere correction example with research-processign library 1 2 3 4 5 6 7 8 9 10 11 12 13 from research_processing.surface_reflectance import sen2cor # sen2cor( # input_dir = \"<path to directory where .safe is>\", # output_dir = \"<path where result will be saved>\" , # scene_ids = [\"<scene ids of `input_dir` that will be processed>\"] #) # For example: sen2cor ( input_dir = \"/data/input\" , output_dir = \"/data/output\" , scene_ids = [ \"S2B_MSIL1C_20171119T133209_N0206_R081_T22JBM_20171120T175608.SAFE\" ] ) O c\u00f3digo acima far\u00e1 a gera\u00e7\u00e3o de imagens com corre\u00e7\u00e3o atmosf\u00e9rica. Exemplos de resultados podem ser visualizados nas figuras abaixo: Atmospheric Corrected Images with Sen2Cor 10m resolution 20m resolution 60m resolution Sentinel-2/MSI Image with Atmospheric Correction (10m resolution) Sentinel-2/MSI Image with Atmospheric Correction (20m resolution) Sentinel-2/MSI Image with Atmospheric Correction (60m resolution)","title":"Bibliotecas de software"},{"location":"tools/libraries/#bibliotecas-de-software","text":"A reprodutibilidade nos permite refazer o caminho que foi trilhado originalmente para a obten\u00e7\u00e3o dos resultados de uma pesquisa cient\u00edfica. Com isso, outros pesquisadores e mesmo nossos pr\u00f3prios \"eus\" do futuro podem se beneficiar e entender o que fizemos. Isso da margem para que poss\u00edveis erros sejam facilmente detectados e corrigidos. Dentre as v\u00e1rias a\u00e7\u00f5es que podem ser realizadas para atingir a reprodutibilidade, est\u00e1 a capacidade de automatizar os passos j\u00e1 realizado. Isso tr\u00e1s uma s\u00e9rie de benef\u00edcios, dentre eles: Evita poss\u00edveis \"erros\" por leitura de resultados incorretos que n\u00e3o s\u00e3o mais v\u00e1lidos para a execu\u00e7\u00e3o corrente; Permite a verifica\u00e7\u00e3o de todo o fluxo de processamento (O que tamb\u00e9m ajuda na escrita do relat\u00f3rio / artigo); Diminui\u00e7\u00e3o do overhead de execu\u00e7\u00e3o do trabalho. H\u00e1 alguns anos atr\u00e1s, essa caracter\u00edstica, era limitada nas pesquisas, por conta do uso de software com interfaces gr\u00e1ficas que acabam n\u00e3o permitindo a automa\u00e7\u00e3o do trabalho realizado atrav\u00e9s dos \"bot\u00f5es e clicks\" na tela. Hoje em dia, com o aumento e dissemina\u00e7\u00e3o de linguagens de alto n\u00edvel para o desenvolvimento de pesquisas, como R e Python , isso mudou por completo. Podemos automatizar as tarefas de forma mais f\u00e1cil. Al\u00e9m disso, a partir do momento que um script de processamento \u00e9 criado, toda a l\u00f3gica realmente aplicada nos dados \u00e9 descrita de forma clara e direta. Neste RC , para que todos as etapas pudessem ser modeladas atrav\u00e9s de scripts , fez-se a cria\u00e7\u00e3o de diversas bibliotecas de c\u00f3digo Python. Cada biblioteca possui uma responsabilidade \u00fanica, o que nos ajudou a manter o trabalho organizado durante seu desenvolvimento. Ao mesmo tempo, essa abordagem facilita a reutiliza\u00e7\u00e3o dos c\u00f3digos criados. Aqueles que desejam reutilizar o trabalho desenvolvido, podem faz\u00ea-lo com a importa\u00e7\u00e3o dessas bibliotecas em seu projeto Python.","title":"Bibliotecas de software"},{"location":"tools/libraries/#bibliotecas-de-codigo-disponiveis","text":"Para a ado\u00e7\u00e3o dessa abordagem de desenvolvimento, baseada em bibliotecas de responsabilidade \u00fanica, que pudessem ser reutilizadas, fez-se necess\u00e1rio a ado\u00e7\u00e3o de alguns crit\u00e9rios que nos ajudasse a manter tudo organizado e reutiliz\u00e1vel. Al\u00e9m disso, nossa vis\u00e3o com a cria\u00e7\u00e3o de v\u00e1rias bibliotecas para a composi\u00e7\u00e3o dos scripts de processamento do trabalho \u00e9 que essas, devem ser modeladas de modo a possibilitar seu uso conjunto. Como blocos que podem ser juntados para construir um muro. Considerando essas caracter\u00edsticas, inicialmente foi definido que neste RC , dois tipos de bibliotecas seriam desenvolvidas: Base Bibliotecas que fornecem os recursos e funcionalidades base para a realiza\u00e7\u00e3o de uma a\u00e7\u00e3o (e.g., Gera\u00e7\u00e3o de bandas de \u00e2ngulos) Aplica\u00e7\u00e3o Bibliotecas que atrav\u00e9s da uni\u00e3o de bibliotecas Base , disponibiliza funcionalidades que permitem a aplica\u00e7\u00e3o de diferentes metodologias e fluxos de processamento (e.g., Gera\u00e7\u00e3o de produtos harmonizados). Partindo dessas defini\u00e7\u00f5es, para a produ\u00e7\u00e3o deste RC e a gera\u00e7\u00e3o de seus resultados, fez-se o desenvolvimento de duas bibliotecas Base : s2-angs Disponibiliza funcionalidades para a gera\u00e7\u00e3o de bandas \u00e2ngulos de imagens Sentinel-2; sensor-harm Permite a gera\u00e7\u00e3o de produtos harmonizados. Essas bibliotecas est\u00e3o dispon\u00edveis e podem ser instaladas como qualquer outra biblioteca da linguagem Python e utilizada em diferentes projetos. Assim, caso seja de seu interesse, \u00e9 poss\u00edvel, por exemplo, instalar a biblioteca sensor-harm em seu projeto Python e utilizar as funcionalidades disponibilizadas para a gera\u00e7\u00e3o de produtos harmonizados. Para ambas as bibliotecas a \u00fanica restri\u00e7\u00e3o est\u00e1 no respeito aos formatos de entradas esperados pelas fun\u00e7\u00f5es dessas bibliotecas. Ao seguir corretamente, voc\u00ea n\u00e3o dever\u00e1 ter problemas para fazer sua utiliza\u00e7\u00e3o. Com base nessas duas bibliotecas, fez-se a cria\u00e7\u00e3o de uma biblioteca Aplica\u00e7\u00e3o : research-processing Fornece funcionalidades que permitem a aplica\u00e7\u00e3o da metodologia de processamento de dados utilizadas na gera\u00e7\u00e3o dos resultados deste RC . Parte de suas funcionalidades \u00e9 criada com base nas bibliotecas s2-angs e sensor-harm . Para resumir, a rela\u00e7\u00e3o entre essas bibliotecas \u00e9 sumarizada na Figura abaixo. Libraries organization Detalhes do funcionamento e funcionalidades de cada uma dessas bibliotecas s\u00e3o apresentados nas se\u00e7\u00f5es a seguir.","title":"Bibliotecas de c\u00f3digo dispon\u00edveis"},{"location":"tools/libraries/#bibliotecas-na-metodologia-dos-experimentos","text":"Para que se tenha uma ideia onde cada uma das bibliotecas apresentadas anteriormente s\u00e3o utilizadas na metodologia de processamento realizada neste RC , tem-se na Figura abaixo a rela\u00e7\u00e3o de cada uma das etapas e a biblioteca utilizada. Processing workflow with libraries","title":"Bibliotecas na metodologia dos experimentos"},{"location":"tools/libraries/#especificacao-das-bibliotecas","text":"Nesta se\u00e7\u00e3o, de forma complementar a vis\u00e3o geral apresentada at\u00e9 aqui, \u00e9 feita a especifica\u00e7\u00e3o das funcionalidades e forma de uso de cada uma das bibliotecas mencionadas anteriormente.","title":"Especifica\u00e7\u00e3o das bibliotecas"},{"location":"tools/libraries/#sentinel-2-angle-generator-python-library-s2-angs","text":"A biblioteca s2-angs , como mencionado anteriormente, \u00e9 respons\u00e1vel pela gera\u00e7\u00e3o de bandas de \u00e2ngulos para imagens Sentinel-2. Essas bandas cont\u00e9m informa\u00e7\u00f5es, por pixel, de \u00e2ngulos solar azimutal ( SAA ), solar zenital ( SZA ), sensor azimutal ( VAA ) e sensor zenital ( VZA ). Essas informa\u00e7\u00f5es s\u00e3o extra\u00eddas dos metadados de imagens Sentinel-2. Inicialmente esses dados s\u00e3o fornecidos em forma de matrizes de 23x23 (linhas X colunas), ou seja, em uma resolu\u00e7\u00e3o espacial de aproximadamente 5000 metros. Entretanto, essa informa\u00e7\u00e3o precisa estar em resolu\u00e7\u00e3o espacial equivalente \u00e0 das bandas espectrais do sensor ( 10 , 20 ou 60 metros) para que possam ser aproveitadas em corre\u00e7\u00f5es por pixel. Assim, a biblioteca s2-angs \u00e9 capaz de estimar os \u00e2ngulos e salv\u00e1-slos arquivos .tif , tanto em sua resolu\u00e7\u00e3o espacial original, quanto reamostrados para a resolu\u00e7\u00e3o espacial das bandas do sensor. Com isso, pode-se listar como funcionalidades principais dessa biblioteca: Gera\u00e7\u00e3o de bandas de \u00e2ngulos ( SAA , SZA , VAA e VZA ); Reamostragem das bandas de \u00e2ngulos para resolu\u00e7\u00e3o de bandas do pr\u00f3prio sensor.","title":"Sentinel-2 Angle Generator Python Library (s2-angs)"},{"location":"tools/libraries/#principais-operacoes-disponiveis","text":"A tabela abaixo apresenta um resumo das principais opera\u00e7\u00f5es que est\u00e3o dispon\u00edveis na biblioteca s2-angs . Function Description s2_angs.gen_s2_ang Function to generate the Sentinel-2 Angle bands","title":"Principais opera\u00e7\u00f5es dispon\u00edveis"},{"location":"tools/libraries/#exemplo-de-utilizacao","text":"Para realizar a gera\u00e7\u00e3o de banda de \u00e2ngulo utilizando a biblioteca s2_angs , basta fazer o uso da fun\u00e7\u00e3o gen_s2_ang . Essa fun\u00e7\u00e3o aceita como entrada .zip , .SAFE directory ou diret\u00f3rio com imagens Sentinel-2 . No bloco de c\u00f3digo abaixo, tem-se um exemplo em que um arquivo .zip \u00e9 utilizado como entrada na fun\u00e7\u00e3o: s2-angs example code 1 2 3 4 5 import s2_angs s2_angs . gen_s2_ang ( \"S2B_MSIL1C_20191223T131239_N0208_R138_T23KMR_20191223T135458.zip\" ) O c\u00f3digo acima far\u00e1 a gera\u00e7\u00e3o das bandas de \u00e2ngulos da imagem definida na entrada. Exemplos de resultados podem ser visualizados nas figuras abaixo: Intermediary results (matrix 23x23) Solar Azimuth Solar Zenith View Azimuth View Zenith Solar Azimuth intermediary result Solar Zenith intermediary result View Azimuth intermediary result View Zenith intermediary result Resampled results Solar Azimuth Solar Zenith View Azimuth View Zenith Solar Azimuth resampled result Solar Zenith resampled result View Azimuth resampled result View Zenith resampled result Para mais informa\u00e7\u00f5es, por favor, consulte o reposit\u00f3rio oficial da biblioteca s2-angs .","title":"Exemplo de utiliza\u00e7\u00e3o"},{"location":"tools/libraries/#sensor-harmonization-python-library-sensor-harm","text":"Neste RC , parte dos resultados consiste em produtos harmonizados, ou seja, produtos de n\u00edvel de reflect\u00e2ncia de superf\u00edcie com corre\u00e7\u00e3o para os efeitos de Fun\u00e7\u00e3o de distribui\u00e7\u00e3o de reflect\u00e2ncia bidirecional (BRDF) e ajustes espectrais. Para isso, fez-se a cria\u00e7\u00e3o da biblioteca sensor-harm . Nessa biblioteca, a corre\u00e7\u00e3o BRDF \u00e9 feita utilizando o m\u00e9todo do c-factor para gerar produtos Nadir BRDF-Adjusted Reflectance (NBAR), enquanto que o ajuste espectral \u00e9 feito utilizando bandpass adotando as imagens Landsat-8 como refer\u00eancia. Com o uso dessa biblioteca, esses m\u00e9todos podem ser aplicados em imagens dos sat\u00e9lites-sensores Landsat-4/TM, Landsat-5/TM, Landsat-7/ETM+, Landsat-8/OLI e Sentinel-2/MSI, al\u00e9m de permitir a harmoniza\u00e7\u00e3o entre esses diferentes dados. A biblioteca apresenta duas fun\u00e7\u00f5es principais, uma para harmonizar imagens provenientes de sensores a bordo dos sat\u00e9lites Landsat e outra para imagens provenientes dos sensores a bordo dos sat\u00e9lites Sentinel-2.","title":"Sensor Harmonization Python Library (sensor-harm)"},{"location":"tools/libraries/#principais-operacoes-disponiveis_1","text":"A tabela abaixo apresenta um resumo das principais opera\u00e7\u00f5es que est\u00e3o dispon\u00edveis na biblioteca sensor-harm . Function Description sensor_harm.landsat.landsat_harmonize Function to harmonize Landsat data sensor_harm.sentinel2.sentinel_harmonize Function to harmonize Sentinel-2 data","title":"Principais opera\u00e7\u00f5es dispon\u00edveis"},{"location":"tools/libraries/#exemplo-de-utilizacao_1","text":"Para realizar a harmoniza\u00e7\u00e3o de dados, seja esse Landsat-4/TM, Landsat-5/TM, Landsat-7/ETM+, Landsat-8/OLI ou Sentinel-2, \u00e9 necess\u00e1rio definir o diret\u00f3rio onde os dados de entrada est\u00e3o armazenados, bem como o diret\u00f3rio de sa\u00edda. Para exemplificar o uso dessa fun\u00e7\u00e3o, o bloco de c\u00f3digo abaixo \u00e9 um exemplo de como a biblioteca sensor-harm pode ser utilizada para harmoniza\u00e7\u00e3o de dados Sentinel-2: sensor-harm example code 1 2 3 4 5 6 from sensor_harm.sentinel2 import sentinel_harmonize sentinel2_entry = '/path/to/S2/SR/images/' target_dir = '/path/to/output/NBAR/' sentinel_harmonize ( sentinel2_entry , target_dir , apply_bandpass = True ) O c\u00f3digo acima far\u00e1 a gera\u00e7\u00e3o das bandas de \u00e2ngulos da imagem definida na entrada. Exemplos de resultados podem ser visualizados nas figuras abaixo: Sentinel-2/MSI Harmonized Data NBAR Band 02 (10m) NBAR Band 03 (10m) NBAR Band 04 (10m) NBAR Band 08 (10m) NBAR Band 08A (20m) NBAR Band 11 (20m) NBAR Band 12 (20m) Sentinel-2/MSI Image (B02) NBAR 10m Sentinel-2/MSI Image (B03) NBAR 10m Sentinel-2/MSI Image (B04) NBAR 10m Sentinel-2/MSI Image (B08) NBAR 10m Sentinel-2/MSI Image (B8A) NBAR 20m Sentinel-2/MSI Image (B11) NBAR 20m Sentinel-2/MSI Image (B12) NBAR 20m Para mais informa\u00e7\u00f5es, por favor, consulte o reposit\u00f3rio oficial da biblioteca sensor-harm .","title":"Exemplo de utiliza\u00e7\u00e3o"},{"location":"tools/libraries/#research-processing-python-library-research-processing","text":"A metodologia de processamento realizada neste RC , conforme apresentado nas se\u00e7\u00f5es anteriores, possui diversas etapas, as quais dependem de diferentes ferramentas de software. Consequentemente, a realiza\u00e7\u00e3o de todo o fluxo de processamento pode exigir: Instala\u00e7\u00e3o de depend\u00eancias de software espec\u00edficas para cada etapa de processamento; Configura\u00e7\u00f5es espec\u00edficas no ambiente de software para cada etapa de processamento. Com essas exig\u00eancias, a execu\u00e7\u00e3o e reprodu\u00e7\u00e3o do fluxo de processamento poderiam ser problem\u00e1ticas para n\u00f3s e para aqueles que desejassem reproduzir ou mesmo aplicar o trabalho desenvolvido. Como forma de evitar esses poss\u00edveis problemas e facilitar a materializa\u00e7\u00e3o da metodologia de processamento deste RC , n\u00f3s desenvolvemos a biblioteca research-processing . Nessa biblioteca, todas as etapas da metodologia s\u00e3o modeladas como fun\u00e7\u00f5es Python que podem ser facilmente utilizadas na constru\u00e7\u00e3o de qualquer fluxo de processamento. Adicionalmente, parte das opera\u00e7\u00f5es que exigem configura\u00e7\u00f5es espec\u00edficas de ambiente, s\u00e3o executadas em Docker Containers , de forma transparente aos usu\u00e1rios da biblioteca. Ao todo, tem-se no research-processing , funcionalidades para a realiza\u00e7\u00e3o de a\u00e7\u00f5es como: Pr\u00e9-processamento Corre\u00e7\u00e3o atmosf\u00e9rica Sentinel-2/MSI (Sen2Cor e LaSRC); Gera\u00e7\u00e3o de bandas de \u00e2ngulos (Landsat-8/OLI e Sentinel-2/MSI); Gera\u00e7\u00e3o de produtos NBAR (Sentinel-2/MSI e Landsat-8/OLI). An\u00e1lise de dados Rotinas de valida\u00e7\u00e3o das corre\u00e7\u00f5es realizadas.","title":"Research Processing Python Library (research-processing)"},{"location":"tools/libraries/#abordagem-de-execucao-das-funcoes","text":"Assim como mencionado anteriormente, as funcionalidades do research-processing s\u00e3o modeladas como fun\u00e7\u00f5es reutiliz\u00e1veis com execu\u00e7\u00e3o amig\u00e1vel a reprodu\u00e7\u00e3o. Para que essas caracter\u00edsticas pudessem ser garantidas, adotou-se na biblioteca durante a implementa\u00e7\u00e3o das fun\u00e7\u00f5es, o conceito de modelo de execu\u00e7\u00e3o , o qual determina onde/como a fun\u00e7\u00e3o ser\u00e1 executada, de forma transparente ao usu\u00e1rio. Cada uma das fun\u00e7\u00f5es implementadas na biblioteca possui um modelo de execu\u00e7\u00e3o , permitindo que diferentes tecnologias e abordagens sejam utilizadas como base para a execu\u00e7\u00e3o da fun\u00e7\u00e3o. Partindo das necessidades das fun\u00e7\u00f5es desse RC , fez-se o uso de dois modelos de execu\u00e7\u00e3o nas implementa\u00e7\u00f5es realizadas, sendo eles: Local Fun\u00e7\u00f5es implementadas com o modelo de execu\u00e7\u00e3o Local , s\u00e3o fun\u00e7\u00f5es Python simples. Essas fun\u00e7\u00f5es n\u00e3o possuem nenhum formato de execu\u00e7\u00e3o especial, sendo executados diretamente no ambiente/interpretador que a invocou. Fun\u00e7\u00f5es com esse formato, dependem do ambiente adjacente, sendo necess\u00e1rio para esse, ter todas as depend\u00eancias instaladas e devidamente configuradas. Containerized Diferente de fun\u00e7\u00f5es Local , as fun\u00e7\u00f5es Containerized n\u00e3o dependem do ambiente adjacente. Isso porque fun\u00e7\u00f5es implementadas com esse formato, ao serem executadas, criam um Docker Container , com o ambiente completo necess\u00e1rio para a execu\u00e7\u00e3o da opera\u00e7\u00e3o associada a fun\u00e7\u00e3o. A fun\u00e7\u00e3o \u00e9 executada dentro do ambiente criado. Ambos modelos de execu\u00e7\u00e3o, s\u00e3o transparentes para o usu\u00e1rios no momento da execu\u00e7\u00e3o. Assim, independente da forma com que a implementa\u00e7\u00e3o foi realizada, o uso final \u00e9 o mesmo: Uma chamada a uma fun\u00e7\u00e3o Python. A diferen\u00e7a est\u00e1 nas exig\u00eancias que cada tipo de fun\u00e7\u00e3o far\u00e1 do ambiente onde est\u00e1 sendo executada. Para as fun\u00e7\u00f5es Local , como mencionado, ser\u00e1 necess\u00e1rio a instala\u00e7\u00e3o de todas as depend\u00eancias e configura\u00e7\u00f5es para que a fun\u00e7\u00e3o seja executada. Enquanto isso, as fun\u00e7\u00f5es Containerized exigir\u00e3o a disponibilidade do Docker no ambiente do usu\u00e1rio. Containerized - Permiss\u00f5es de usu\u00e1rio \u00c9 importante notar que, al\u00e9m do Docker instalado, \u00e9 necess\u00e1rio tamb\u00e9m que o usu\u00e1rio que est\u00e1 fazendo a execu\u00e7\u00e3o, tenha as devidas permiss\u00f5es para utilizar o Docker. Na Figura abaixo, faz-se uma representa\u00e7\u00e3o geral de cada um desses modelos utilizados. Note que, fun\u00e7\u00f5es Local , trabalham com o interpretador Python, enquanto fun\u00e7\u00f5es Containerized criam Docker Containers onde a execu\u00e7\u00e3o ser\u00e1 realizada. Research Processing Execution Models Com base nesses dois modos de opera\u00e7\u00e3o, a biblioteca implementa e disponibiliza as opera\u00e7\u00f5es de processamento e an\u00e1lise dos dados. Para as opera\u00e7\u00f5es de processamento, que utilizam ferramentas terceiras, como Sen2Cor e LaSRC , faz-se o uso do modo de opera\u00e7\u00e3o Containerized . J\u00e1 nas opera\u00e7\u00f5es de valida\u00e7\u00e3o, que dependem apenas de bibliotecas Python, usa-se o modo de opera\u00e7\u00e3o Local .","title":"Abordagem de execu\u00e7\u00e3o das fun\u00e7\u00f5es"},{"location":"tools/libraries/#comunicacao-entre-as-funcoes","text":"As funcionalidades da biblioteca research-processing podem ser utilizadas em conjunto, de modo a construir um fluxo de processamento que permita a materializa\u00e7\u00e3o da metodologia de experimento seguida nesse RC . Nesse contexto, um ponto a ser considerado sobre o funcionamento da biblioteca \u00e9 a forma com que as entradas e sa\u00eddas dessas fun\u00e7\u00f5es podem ser encadeadas, de modo que a sa\u00edda de uma fun\u00e7\u00e3o \u00e9 utilizada como entrada em outra fun\u00e7\u00e3o. No research-processing , as opera\u00e7\u00f5es implementadas s\u00e3o realizadas de modo a evitar a movimenta\u00e7\u00e3o de dado (Entradas e sa\u00eddas). Para isso, as fun\u00e7\u00f5es de processamento operam com base no caminho de dados . Com isso, as fun\u00e7\u00f5es recebem o caminho onde os dados est\u00e3o armazenados, bem como onde os resultados devem ser salvos. Tal modo de opera\u00e7\u00e3o permite melhores defini\u00e7\u00f5es em rela\u00e7\u00e3o ao local onde os dados ser\u00e3o carregados e salvos, evitando movimenta\u00e7\u00f5es desnecess\u00e1rias e armazenamento em locais que podem apresentar problemas com o limite de espa\u00e7o e baixo desempenho. Esse modo de funcionamento \u00e9 representado na figura abaixo. Modelo de comunica\u00e7\u00e3o entre fun\u00e7\u00f5es Com base nesse modo de funcionamento, as fun\u00e7\u00f5es s\u00e3o encadeadas e as sa\u00eddas de uma fun\u00e7\u00e3o, que representam o caminho para onde os dados foram salvos, s\u00e3o utilizados como entrada em outras fun\u00e7\u00f5es. Entradas como volumes Deve-se notar que essas informa\u00e7\u00f5es de caminho de dados, para as fun\u00e7\u00f5es Containerized , s\u00e3o utilizadas para a cria\u00e7\u00e3o de Docker Bind Mount Volumes . Assim, os Containers auxiliares tem acesso aos dados que devem ser processados. Ordem de execu\u00e7\u00e3o das opera\u00e7\u00f5es Com esse formato de entradas/sa\u00eddas das fun\u00e7\u00f5es, tem-se como premissa que a sa\u00edda de uma fun\u00e7\u00e3o ser\u00e1 entendida pela fun\u00e7\u00e3o seguinte. Sendo assim, as fun\u00e7\u00f5es dispon\u00edveis na biblioteca research-processing , devem ser encadeadas em uma ordem estrita, sendo nesse caso, a ordem descrita na metodologia. Para mais detalhes de como isso pode ser implementado, consulte a se\u00e7\u00e3o Scripts de processamento .","title":"Comunica\u00e7\u00e3o entre as fun\u00e7\u00f5es"},{"location":"tools/libraries/#principais-operacoes-disponiveis_2","text":"Para dar suporte a cria\u00e7\u00e3o de todo o fluxo de opera\u00e7\u00e3o implementado nesse RC , a biblioteca research-processing fornece diversas fun\u00e7\u00f5es e opera\u00e7\u00f5es auxiliares, as quais s\u00e3o listadas na tabela abaixo: Fun\u00e7\u00e3o Descri\u00e7\u00e3o Modelo de execu\u00e7\u00e3o research_processing.surface_reflectance.sen2cor Corre\u00e7\u00e3o Atmosf\u00e9rica Sen2Cor para Sentinel-2/MSI Containerized research_processing.surface_reflectance.lasrc Corre\u00e7\u00e3o atmosf\u00e9rica LaSRC para Sentinel-2/MSI Containerized research_processing.nbar.s2_sen2cor_nbar Gerador de produtos NBAR para dados Sentinel-2/MSI Reflect\u00e2ncia de superf\u00edcie com (Sen2Cor). Containerized research_processing.nbar.s2_lasrc_nbar Gerador de produtos NBAR para dados Sentinel-2/MSI Reflect\u00e2ncia de superf\u00edcie com (LaSRC). Containerized research_processing.nbar.lc8_nbar Gerador de produtos NBAR para dados Landsat-8/OLI Reflect\u00e2ncia de superf\u00edcie. Containerized research_processing.nbar.lc8_generate_angles Gerador de \u00e2ngulos para dados Landsat-8/OLI Containerized research_processing.validation.validation_routines An\u00e1lise de resultados (M\u00f3dulo com m\u00faltiplas fun\u00e7\u00f5es) Local Dentre essas fun\u00e7\u00f5es, h\u00e1 alguns detalhes que precisam ser considerados para o entendimento completo do motivo por tr\u00e1s de cada um dos modelos de execu\u00e7\u00e3o escolhidos para as fun\u00e7\u00f5es. Nos subt\u00f3picos abaixo, esses detalhes s\u00e3o apresentados: Corre\u00e7\u00e3o atmosf\u00e9rica Sentinel-2/MSI (Sen2Cor e LaSRC) O fluxo de processamento exige que as imagens utilizadas tenham corre\u00e7\u00e3o atmosf\u00e9rica. Para as imagens Landsat-8/OLI, n\u00e3o h\u00e1 a necessidade de fazer a corre\u00e7\u00e3o, uma vez que os produtos j\u00e1 s\u00e3o disponibilizados prontos para uso, com as devidas corre\u00e7\u00f5es geom\u00e9tricas e radiom\u00e9tricas realizadas. No entanto, para os dados Sentinel-2/MSI, isso n\u00e3o \u00e9 verdade. Assim, durante o desenvolvimento do artigo foi necess\u00e1rio realizar a corre\u00e7\u00e3o atmosf\u00e9rica desses dados. Para isso, adotou-se as ferramentas Sen2Cor e LaSRC . Essas ferramentas possuem suas necessidades espec\u00edficas no ambiente onde ser\u00e3o utilizadas, como depend\u00eancias e configura\u00e7\u00f5es para seu uso. Considerando isso, na biblioteca research-processing fez-se a cria\u00e7\u00e3o de fun\u00e7\u00f5es que operam essas ferramentas em um ambiente j\u00e1 configurado e pronto para uso. Esse ambiente \u00e9 executado em um Docker Container . Das fun\u00e7\u00f5es apresentadas na tabela acima, as listadas a seguir s\u00e3o utilizadas para aplicar a corre\u00e7\u00e3o atmosf\u00e9rica nos dados: research_processing.surface_reflectance.lasrc : Fun\u00e7\u00e3o que faz a aplica\u00e7\u00e3o de Corre\u00e7\u00e3o Atmosf\u00e9rica em dados Sentinel-2/MSI utilizando a ferramenta LaSRC. Todo o processamento \u00e9 realizado dentro de um Docker Container de forma transparente para o usu\u00e1rio; research_processing.surface_reflectance.sen2cor : Fun\u00e7\u00e3o que faz a aplica\u00e7\u00e3o de Corre\u00e7\u00e3o Atmosf\u00e9rica em dados Sentinel-2/MSI utilizando a ferramenta Sen2Cor. Todo o processamento \u00e9 realizado dentro de um Docker Container de forma transparente para o usu\u00e1rio. Entendendo os Containers A cria\u00e7\u00e3o de um Docker Container depende de uma Docker Image que define o ambiente e suas configura\u00e7\u00f5es. Isso n\u00e3o \u00e9 diferente no research-processing . Para a cria\u00e7\u00e3o dos Docker Containers de Corre\u00e7\u00e3o Atmosf\u00e9rica, s\u00e3o utilizadas as seguintes Docker Images : Corre\u00e7\u00e3o atmosf\u00e9rica com LaSRC: Docker Image LaSRC 2.0.1 Corre\u00e7\u00e3o atmosf\u00e9ria com Sen2COr: Docker Image Sen2Cor 2.9.0 Essas Docker Images foram criadas para execu\u00e7\u00e3o nesse RC . Para mais informa\u00e7\u00f5es, consulte a se\u00e7\u00e3o Ambientes computacionais . Gera\u00e7\u00e3o de bandas de \u00e2ngulos (Landsat-8/OLI e Sentinel-2/MSI) Para a gera\u00e7\u00e3o dos produtos NBAR, faz-se necess\u00e1rio que bandas de \u00e2ngulos (e.g., SAA , SZA , VAA and VZA ) sejam calculadas. Essas bandas s\u00e3o geradas de forma espec\u00edfica para cada dados/sensor que est\u00e1 sendo trabalhada. Com isso, faz-se necess\u00e1rio o uso de ferramentas especializadas para cada sensor: Landsat-8/OLI: A gera\u00e7\u00e3o de bandas de \u00e2ngulos para o Landsat-8/OLI \u00e9 feita atrav\u00e9s da ferramenta Landsat 8 Angles Creation Tools . Essa ferramenta possui suas pr\u00f3prias depend\u00eancias, exigindo tamb\u00e9m configura\u00e7\u00f5es espec\u00edficas no ambiente onde ser\u00e1 executada; Sentinel-2/MSI: Os \u00e2ngulos de dados Sentinel-2/MSI s\u00e3o gerados com a biblioteca Sentinel-2 Angle Generator Python Library (s2-angs) , desenvolvida nesse RC . Considerando essas caracter\u00edsticas, tem-se a seguinte fun\u00e7\u00e3o para executar essas opera\u00e7\u00f5es: research_processing.nbar.lc8_generate_angles : Fun\u00e7\u00e3o que atrav\u00e9s da ferramenta Landsat 8 Angles Creation Tools , realiza o c\u00e1lculo das bandas de \u00e2ngulos para dados Landsat-8/OLI. O processamento realizado por essa fun\u00e7\u00e3o \u00e9 feito dentro de um Docker Container . Na lista de fun\u00e7\u00f5es acima, n\u00e3o h\u00e1 nenhuma fun\u00e7\u00e3o espec\u00edfica para o Sentinel-2/MSI. Isso ocorre j\u00e1 que, durante a implementa\u00e7\u00e3o da biblioteca research-processing , foi decidido que a gera\u00e7\u00e3o de \u00e2ngulos para os dados Sentinel-2/MSI seria uma opera\u00e7\u00e3o integrada a gera\u00e7\u00e3o dos produtos NBAR. Assim, o c\u00e1lculo dos \u00e2ngulos necess\u00e1rios para a gera\u00e7\u00e3o dos produtos NBAR com dados Sentinel-2/MSI, feitos com a biblioteca s2-angs, s\u00e3o parte das seguintes fun\u00e7\u00f5es: research_processing.nbar.s2_sen2cor_nbar research_processing.nbar.s2_lasrc_nbar O processamento realizado por ambas as fun\u00e7\u00f5es listadas acima tamb\u00e9m \u00e9 feito dentro de um Docker Container . Entendendo os Containers A cria\u00e7\u00e3o de um Docker Container depende de uma Docker Image que define o ambiente e suas configura\u00e7\u00f5es. Isso n\u00e3o \u00e9 diferente no research-processing . Para a cria\u00e7\u00e3o do Docker Container de gera\u00e7\u00e3o de banda de \u00e2ngulos, \u00e9 feito o uso da seguinte Docker Image : Gera\u00e7\u00e3o de bandas de \u00e2ngulos para dados Landsat-8/OLI: Docker Image L8Angs Essas imagens foram criadas para execu\u00e7\u00e3o nesse RC . Para mais informa\u00e7\u00f5es, consulte a se\u00e7\u00e3o Ambientes computacionais . Gera\u00e7\u00e3o de produtos NBAR (Sentinel-2/MSI e Landsat-8/OLI) A base para a gera\u00e7\u00e3o dos produtos NBAR nesse RC , conforme apresentado nas se\u00e7\u00f5es anteriores, \u00e9 a biblioteca sensor-harm . Essa biblioteca, permite a gera\u00e7\u00e3o de produtos NBAR para dados Sentinel-2/MSI e Landsat-8/OLI. Como forma de facilitar a utiliza\u00e7\u00e3o da biblioteca sensor-harm e evitar que os utilizadores tenham de fazer instala\u00e7\u00f5es e configura\u00e7\u00f5es espec\u00edficas, na biblioteca research-processing foi realizada a implementa\u00e7\u00e3o das seguintes fun\u00e7\u00f5es: research_processing.nbar.lc8_nbar : Faz a gera\u00e7\u00e3o de produtos NBAR para dados Landsat-8/OLI research_processing.nbar.s2_lasrc_nbar : Faz a gera\u00e7\u00e3o de produtos NBAR para dados Sentinel-2/MSI com corre\u00e7\u00e3o atmosf\u00e9rica feita com a ferramenta LaSRC; research_processing.nbar.s2_sen2cor_nbar : Faz a gera\u00e7\u00e3o de produtos NBAR para dados Sentinel-2/MSI com corre\u00e7\u00e3o atmosf\u00e9rica feita com a ferramenta Sen2Cor. Essas fun\u00e7\u00f5es, s\u00e3o implementadas com o modelo de execu\u00e7\u00e3o Containerized. Assim, no momento em que o usu\u00e1rio faz a execu\u00e7\u00e3o dessas fun\u00e7\u00f5es, um Docker Container , com as devidas depend\u00eancias \u00e9 criado para executar a fun\u00e7\u00e3o. Entendendo os Containers A cria\u00e7\u00e3o de um Docker Container depende de uma Docker Image que define o ambiente e suas configura\u00e7\u00f5es. Isso n\u00e3o \u00e9 diferente no research-processing . Para a cria\u00e7\u00e3o do Docker Container de gera\u00e7\u00e3o de produtos NBAR (todas as fun\u00e7\u00f5es), \u00e9 feito o uso da seguinte Docker Image : Gera\u00e7\u00e3o de produtos NBAR (Sentinel-2/MSI e Landsat-8/OLI): Docker Image NBAR Essas imagens foram criadas para execu\u00e7\u00e3o nesse RC . Para mais informa\u00e7\u00f5es, consulte a se\u00e7\u00e3o Ambientes computacionais . Rotinas de valida\u00e7\u00e3o das corre\u00e7\u00f5es realizadas Para finalizar as funcionalidades, tem-se o m\u00f3dulo de valida\u00e7\u00e3o de corre\u00e7\u00f5es. Esse m\u00f3dulo ( research_processing.validation.validation_routines ), \u00e9 o respons\u00e1vel em fazer todas as compara\u00e7\u00f5es e c\u00e1lculos utilizados para avaliar os resultados gerados neste RC . De modo a tornar a depura\u00e7\u00e3o mais simples e direta, sua implementa\u00e7\u00e3o \u00e9 feita com o modelo de execu\u00e7\u00e3o Local. Com isso, o usu\u00e1rio que utilizar as fun\u00e7\u00f5es desse m\u00f3dulo, deve configurar o ambiente onde a execu\u00e7\u00e3o ser\u00e1 feita. Em resumo, tem-se apenas que instalar as depend\u00eancias da pr\u00f3pria biblioteca research-processing e ent\u00e3o o ambiente j\u00e1 estar\u00e1 pronto para executar essas fun\u00e7\u00f5es.","title":"Principais opera\u00e7\u00f5es dispon\u00edveis"},{"location":"tools/libraries/#exemplo-de-utilizacao_2","text":"Para exemplificar a forma de utiliza\u00e7\u00e3o da biblioteca research-processing , abaixo \u00e9 apresentada a forma com que a biblioteca realiza a corre\u00e7\u00e3o atmosf\u00e9rica de imagens Sentinel-2/MSI, utilizando a ferramenta Sen2Cor . Corre\u00e7\u00e3o atmosf\u00e9rica Sen2Cor Sen2Cor atmosphere correction example with research-processign library 1 2 3 4 5 6 7 8 9 10 11 12 13 from research_processing.surface_reflectance import sen2cor # sen2cor( # input_dir = \"<path to directory where .safe is>\", # output_dir = \"<path where result will be saved>\" , # scene_ids = [\"<scene ids of `input_dir` that will be processed>\"] #) # For example: sen2cor ( input_dir = \"/data/input\" , output_dir = \"/data/output\" , scene_ids = [ \"S2B_MSIL1C_20171119T133209_N0206_R081_T22JBM_20171120T175608.SAFE\" ] ) O c\u00f3digo acima far\u00e1 a gera\u00e7\u00e3o de imagens com corre\u00e7\u00e3o atmosf\u00e9rica. Exemplos de resultados podem ser visualizados nas figuras abaixo: Atmospheric Corrected Images with Sen2Cor 10m resolution 20m resolution 60m resolution Sentinel-2/MSI Image with Atmospheric Correction (10m resolution) Sentinel-2/MSI Image with Atmospheric Correction (20m resolution) Sentinel-2/MSI Image with Atmospheric Correction (60m resolution)","title":"Exemplo de utiliza\u00e7\u00e3o"},{"location":"tools/processing/","text":"Scripts de processamento \u00b6 Para a implementa\u00e7\u00e3o da metodologia dos experimentos realizados neste RC , inicialmente fez-se a cria\u00e7\u00e3o de um conjunto de bibliotecas de software . Com base nessas bibliotecas e todas as funcionalidades que essas disponibilizam, n\u00f3s realizamos a implementa\u00e7\u00e3o de scripts de processamento. Esses scripts representam a materializa\u00e7\u00e3o da metodologia dos experimentos realizados neste RC . Esses scripts de processamento, foram implementados utilizando duas ferramentas: Jupyter Notebook : Ambiente de computa\u00e7\u00e3o interativa, que permitem a cria\u00e7\u00e3o de notebooks computacionais que implementam os conceitos de computa\u00e7\u00e3o letrada, o qual permite a mescla de textos e c\u00f3digo em um mesmo documento; Dagster : Plataforma que permite a cria\u00e7\u00e3o, gerenciamento e execu\u00e7\u00e3o de fluxos de processamento e manipula\u00e7\u00e3o de dados. Ambas implementa\u00e7\u00f5es listadas acima, utilizam as mesmas bibliotecas e vers\u00f5es para a realiza\u00e7\u00e3o do processamento, garantindo a produ\u00e7\u00e3o de resultados reprodut\u00edveis e compar\u00e1veis. A escolha pela implementa\u00e7\u00e3o do fluxo de processamento em duas ferramentas distintas foi realizada por alguns motivos, sendo os principais deles listados abaixo: Apresenta\u00e7\u00e3o das capacidades de uso das biblioteca criadas neste RC ; Abrang\u00eancia nos cen\u00e1rios de uso da metodologia implementada neste RC ; Dupla implementa\u00e7\u00e3o, o que permite a compara\u00e7\u00e3o dos resultados gerados de diferentes formas, partindo das mesmas ferramentas. De forma geral, a liga\u00e7\u00e3o de cada um desses scripts de processamento e as bibliotecas de software desenvolvidas nesse RC , s\u00e3o apresentadas na Figura abaixo: Rela\u00e7\u00e3o entre as bibliotecas de funcionalidades e scripts de processamento. Jupyter Notebook \u00b6 Por vezes, quando a metodologia implementada est\u00e1 sendo explorado, por exemplo, em cen\u00e1rios de desenvolvimento, \u00e9 necess\u00e1rio que todos os passos sejam bem documentos e organizados de forma clara. Para esse tipo de cen\u00e1rio, foi realizada a implementa\u00e7\u00e3o da metodologia deste RC utilizando o Jupyter Notebook. No Jupyter Notebook implementado, todas as etapas s\u00e3o documentadas e descritas, de modo que todos possam explorar e enteder o que est\u00e1 sendo realizado. No v\u00eddeo abaixo, tem-se a apresenta\u00e7\u00e3o de algumas partes desse notebook. Exemplo de parte do Jupyter Notebook de processamento. Configura\u00e7\u00f5es \u00b6 Para a utiliza\u00e7\u00e3o desse Jupyter Notebook \u00e9 necess\u00e1rio apenas uma configura\u00e7\u00e3o: A defini\u00e7\u00e3o da vari\u00e1vel de ambiente DATA_DIRECTORY . Essa vari\u00e1vel de ambiente, determina o local onde est\u00e1 o diret\u00f3rio de dados que dever\u00e1 ser utilizado pelo notebook para carregar os dados de entrada e salvar os resultados gerados. Sobre o diret\u00f3rio de dados Os detalhes de como o diret\u00f3rio de dados definido na vari\u00e1vel de ambiente DATA_DIRECTORY deve estar organizado s\u00e3o apresentados na se\u00e7\u00e3o Diret\u00f3rio de dados . Dagster \u00b6 Sobre a vers\u00e3o do Dagster No momento em que esse RC estava sendo constru\u00eddo, a vers\u00e3o oficial do Dagste era 0.12.15 . Atualmente, as vers\u00f5es mudaram, o que trouxe novas nomenclaturas e defini\u00e7\u00f5es ao software . Para manter o conte\u00fado consistente, as explica\u00e7\u00f5es utilizadas nessa documenta\u00e7\u00e3o, foram criadas com base na vers\u00e3o 0.12.15 . Nomenclaturas No Dagster, todo fluxo de processamento criado \u00e9 chamado de Pipeline . Neste documento, para manter a consist\u00eancia com as explica\u00e7\u00f5es realizadas com o Jupyter Notebook, os pipelines ser\u00e3o genericamente tratados como scripts de processamento. Caso voc\u00ea deseje, pode consultar a documenta\u00e7\u00e3o oficial do Dagster (v0.12.15) onde tem-se uma explica\u00e7\u00e3o detalhada do que s\u00e3o os Pipelines . Com a metodologia implementada e pronta para ser utilizada, sua execu\u00e7\u00e3o com o uso de grandes volumes de dados pode exigir diversos controles e a\u00e7\u00f5es como: Execu\u00e7\u00e3o paralela das opera\u00e7\u00f5es; Controle de falhas; Reexecu\u00e7\u00e3o. Com base nessa necessidade, foi criado o script de processamento utilizando a ferramenta Dagster . Com essa ferramenta, toda a orquestra\u00e7\u00e3o da opera\u00e7\u00e3o pode ser feita de maneira simples e direta. Al\u00e9m disso, quando necess\u00e1rio e devidamente configurado, diferentes formas de execu\u00e7\u00e3o podem ser adotados, como execu\u00e7\u00f5es distribu\u00eddas, paralelas. Outro ponto positivo ao uso do Dagster est\u00e1 no controle de falhas e reexecu\u00e7\u00e3o. A cria\u00e7\u00e3o do script de processamento \u00e9 feita utilizando uma interface de programa\u00e7\u00e3o (API) em linguagem Python, no entanto, a manipula\u00e7\u00e3o e uso do script \u00e9 feita atrav\u00e9s de uma interface web . Nessa interface, tem-se dispon\u00edvel op\u00e7\u00f5es para executar o processamento, fazer seu gerenciamento, bem como consultar a documenta\u00e7\u00e3o de cada uma das etapas e ver o fluxo geral que \u00e9 realizado. O v\u00eddeo abaixo, apresenta uma vis\u00e3o geral dessa interface: Exemplo de parte da interface do Dagster com a vis\u00e3o do fluxo de processamento. Configura\u00e7\u00f5es \u00b6 Para realizar a execu\u00e7\u00e3o do script de processamento criado com o Dagster, \u00e9 necess\u00e1rio realizar a configura\u00e7\u00e3o da ferramenta. Essa configura\u00e7\u00e3o \u00e9 feita atrav\u00e9s da defini\u00e7\u00e3o de par\u00e2metros em um arquivo no formato YAML , o qual especifica: Onde os dados de entrada est\u00e3o armazenados; Qual ser\u00e1 o diret\u00f3rio de sa\u00edda; Quais imagens e bandas espectrais devem ser utilizadas; Especifica\u00e7\u00e3o de recursos computacionais (e.g., Quantidade de CPU utilizada). Esse arquivo, uma vez criado, \u00e9 inserido na interface do Dagster, onde \u00e9 feita sua valida\u00e7\u00e3o e uso para a cria\u00e7\u00e3o de uma nova execu\u00e7\u00e3o do script . Sobre execu\u00e7\u00f5es no Dagster Para saber mais detalhes sobre como o Dagster realiza a cria\u00e7\u00e3o de suas execu\u00e7\u00f5es na vers\u00e3o 0.12.15 , por favor, consulte a documenta\u00e7\u00e3o oficial da ferramenta Para que voc\u00ea seja capaz de criar configura\u00e7\u00f5es do Dagster para realizar execu\u00e7\u00f5es com seus pr\u00f3prios dados e necessidades, nas subse\u00e7\u00f5es abaixo faz-se a apresenta\u00e7\u00e3o da estrutura do arquivo de configura\u00e7\u00e3o YAML do Dagster. Arquivo de configura\u00e7\u00e3o Dagster \u00b6 O arquivo de configura\u00e7\u00e3o Dagster, como mencionado anteriormente, \u00e9 utilizado para definir os recursos que ser\u00e3o utilizados durante a execu\u00e7\u00e3o do script de processamento. Esse arquivo de configura\u00e7\u00e3o, no formato YAML , \u00e9 dividido em duas principais se\u00e7\u00f5es: (i) Resources; (ii) Solids. Cada uma dessas se\u00e7\u00f5es, \u00e9 respons\u00e1vel por realizar uma parte da configura\u00e7\u00e3o, de modo a permitir que as defini\u00e7\u00f5es mencionadas anteriormente possam ser realizadas. Cada se\u00e7\u00e3o e as configura\u00e7\u00f5es definidas s\u00e3o apresentadas nos t\u00f3picos a seguir Sobre o arquivo de configura\u00e7\u00e3o Ao utilizar o Jupyter Notebook, toda a configura\u00e7\u00e3o \u00e9 feita atrav\u00e9s de uma \u00fanica vari\u00e1vel de ambiente ( DATA_DIRECTORY ). No Dagster, o arquivo de configura\u00e7\u00e3o tem exatamente o mesmo papel. Sendo assim, \u00e9 recomendado que antes de voc\u00ea acompanhar os t\u00f3picos de defini\u00e7\u00e3o do arquivo de configura\u00e7\u00e3o, voc\u00ea consulte a se\u00e7\u00e3o Diret\u00f3rio de dados , na qual \u00e9 especificado a estrutura de diret\u00f3rios que deve ser seguida, bem como o conte\u00fado que deve estar dispon\u00edvel em cada se\u00e7\u00e3o. Resources \u00b6 No Dagster, resources s\u00e3o utilizados para representar elementos e recursos do ambiente computacional que devem estar dispon\u00edveis para a execu\u00e7\u00e3o do script de processamento. No caso do script de processamento desse RC , os resources representam os dados que devem ser utilizados durante o processamento. Sendo assim, a um resource , pode-se ter associado um ou mais diret\u00f3rios de dados. Ao todo, o script de processamento necessita de dois resources : lasrc_data Resource onde tem-se a disponibiliza\u00e7\u00e3o de dados relacionados a ferramenta LaSRC. A defini\u00e7\u00e3o de um resource lasrc_data requer a especifica\u00e7\u00e3o da seguinte vari\u00e1vel de configura\u00e7\u00e3o: lasrc_auxiliary_data_dir : Vari\u00e1vel onde faz-se a defini\u00e7\u00e3o do caminho para o diret\u00f3rio de dados auxiliares da ferramente LaSRC, requeridos para a realiza\u00e7\u00e3o da corre\u00e7\u00e3o atmosf\u00e9rica (usado neste RC em imagens Sentinel-2/MSI). Organiza\u00e7\u00e3o do diret\u00f3rio de dados Para mais detalhes de como esse diret\u00f3rio de dados auxiliares deve ser organizado, por favor, consulte a se\u00e7\u00e3o Diret\u00f3rio de dados . Um exemplo completo de defini\u00e7\u00e3o de um resource lasrc_data \u00e9 apresentado abaixo: resources : lasrc_data : config : lasrc_auxiliary_data_dir : /path/to/lasrc/auxiliary/data Esse bloco de c\u00f3digo deve ser definido dentro do arquivo de configura\u00e7\u00e3o. Para um exemplo completo, consulte o arquivo de configura\u00e7\u00e3o do dagster . repository Resource onde faz-se a defini\u00e7\u00e3o dos diret\u00f3rios de entrada e sa\u00edda do script de processamento. A defini\u00e7\u00e3o de um resource repository requer a especifica\u00e7\u00e3o das seguintes vari\u00e1veis de configura\u00e7\u00e3o: landsat8_input_dir : Diret\u00f3rio onde est\u00e3o dispon\u00edveis os dados Landsat-8/OLI, que podem ser utilizados como entrada para o script de processamento; sentinel2_input_dir : Diret\u00f3rio onde est\u00e3o dispon\u00edveis os dados Sentinel-2/MSI, que podem ser utilizados como entrada para o script de processamento; derived_data_dir : Diret\u00f3rio onde os dados gerados durante as etapas de processamento ser\u00e3o salvos. Organiza\u00e7\u00e3o do diret\u00f3rio de dados Para mais detalhes de como cada um desses diret\u00f3rios devem ser organizados, por favor, consulte a se\u00e7\u00e3o Diret\u00f3rio de dados . Um exemplo completo de defini\u00e7\u00e3o de um resource repository \u00e9 apresentado abaixo: resources : repository : config : derived_data_dir : /path/to/derived/data/dir landsat8_input_dir : /path/to/input/landsat8/data/dir sentinel2_input_dir : /path/to/input/sentinel2/data/dir Esse bloco de c\u00f3digo deve ser definido dentro do arquivo de configura\u00e7\u00e3o. Para um exemplo completo, consulte o arquivo de configura\u00e7\u00e3o do dagster . Solids \u00b6 No Dagster, solids representam a unidade funcional de trabalho que far\u00e1 a execu\u00e7\u00e3o de uma opera\u00e7\u00e3o de fato. Esses elementos s\u00e3o os respons\u00e1veis por receber as entradas, realizar um processamento e gerar as sa\u00eddas. Assim, no script de processamento, eles foram utilizados para representar cada uma das etapas de processamento. Os solids podem ou n\u00e3o requerer algum tipo de configura\u00e7\u00e3o para sua defini\u00e7\u00e3o. No caso do script de processamento deste RC , apenas um deles necessita de configura\u00e7\u00e3o, sendo ele: load_and_standardize_sceneids_input : solid respons\u00e1vel em receber os arquivos com a defini\u00e7\u00e3o das cenas Landsat-8/OLI e Sentinel-2/MSI que dever\u00e3o ser processadas. Ao receber esses arquivos, o solid faz a leitura, valida\u00e7\u00e3o e ent\u00e3o passa as informa\u00e7\u00f5es carregadas para as etapas subsequentes, que as utilizam para a realiza\u00e7\u00e3o das atividades de processamento. Na configura\u00e7\u00e3o desse solid , faz-se necess\u00e1rio a especifica\u00e7\u00e3o das seguintes vari\u00e1veis de configura\u00e7\u00e3o: landsat8_sceneid_list : Arquivo .txt com o nome das cenas Landsat-8/OLI que dever\u00e3o ser consideradas no fluxo de processamento. Esses nomes devem ser o mesmo dispon\u00edvel no diret\u00f3rio de dados Landsat-8/OLI ( landsat8_input_dir ) especificado no resource repository ; sentinel2_sceneid_list : Arquivo .txt com o nome das cenas Sentinel-2/MSI que dever\u00e3o ser consideradas no fluxo de processamento. Esses nomes devem ser o mesmo dispon\u00edvel no diret\u00f3rio de dados Sentinel-2/MSI ( sentinel2_input_dir ) especificado no resource repository ; Organiza\u00e7\u00e3o do diret\u00f3rio de dados Para mais detalhes de como os arquivos com os nomes das cenas devem estar organizados, por favor, consulte a se\u00e7\u00e3o Diret\u00f3rio de dados . Um exemplo completo de defini\u00e7\u00e3o de um solid load_and_standardize_sceneids_input \u00e9 apresentado abaixo: solids : load_and_standardize_sceneids_input : config : landsat8_sceneid_list : /path/to/input/landsat8/data/landsat.txt sentinel2_sceneid_list : /path/to/input/sentinel2/data/sentinel.txt Exemplo completo \u00b6 Abaixo, faz-se a apresenta\u00e7\u00e3o de um arquivo completo de configura\u00e7\u00e3o Dagster, que utiliza todos os resources e solids especificados nos t\u00f3picos anteriores: Para exemplificar todos os elementos citados anteriormente em um arquivo de configura\u00e7\u00e3o, abaixo \u00e9 apresentado a configura\u00e7\u00e3o m\u00ednima necess\u00e1ria para a execu\u00e7\u00e3o do pipeline deste RC . resources : lasrc_data : config : lasrc_auxiliary_data_dir : /path/to/lasrc/auxiliary/data repository : config : derived_data_dir : /path/to/derived/data/dir landsat8_input_dir : /path/to/input/landsat8/data/dir sentinel2_input_dir : /path/to/input/sentinel2/data/dir solids : load_and_standardize_sceneids_input : config : landsat8_sceneid_list : /path/to/input/landsat8/data/landsat.txt sentinel2_sceneid_list : /path/to/input/sentinel2/data/sentinel.txt Diret\u00f3rio de dados \u00b6 Em ambas as implementa\u00e7\u00f5es dos scripts de processamento, na etapa de configura\u00e7\u00e3o, faz-se necess\u00e1rio a defini\u00e7\u00e3o do caminho para o diret\u00f3rio de dados. Esse diret\u00f3rio possui uma organiza\u00e7\u00e3o padr\u00e3o, de modo que, independente da forma com que a execu\u00e7\u00e3o ser\u00e1 realizada, a mesma organiza\u00e7\u00e3o de dados pode ser seguida. Nesta se\u00e7\u00e3o, ser\u00e1 apresentado o formato de organiza\u00e7\u00e3o desse diret\u00f3rio de dados. Para come\u00e7ar, primeiro \u00e9 importante entender que: O diret\u00f3rio de dados, utilizado em ambos scripts de processamento, representam o diret\u00f3rio de entradas e sa\u00eddas do script . Todas as entradas s\u00e3o lidas desse diret\u00f3rios, n\u00e3o sendo poss\u00edvel ler dados de diferentes diret\u00f3rios. O mesmo vale para as sa\u00eddas, os resultados produzidos sempre ser\u00e3o armazenados em nesse diret\u00f3rio. A l\u00f3gica por tr\u00e1s da defini\u00e7\u00e3o desse diret\u00f3rio \u00e9 a da organiza\u00e7\u00e3o: Se o pesquisador precisa centralizar e manter organizado em uma estrutura l\u00f3gica todos os materiais que ele est\u00e1 utilizando para produzir os resultados, bem como os resultados propriamente ditos, ele minimamente ter\u00e1 de manter seus objetos de pesquisa organizados. Para que esse diret\u00f3rio suporte toda essas utilidades, diferentes subdiret\u00f3rios s\u00e3o criados abaixo dele. Nesses diret\u00f3rios tem-se a defini\u00e7\u00e3o dos dados de entrada e sa\u00edda. Abaixo, a estrutura do diret\u00f3rio de dados, esperada por ambos os scripts de processamento ser\u00e3o apresentados: data directory \u251c\u2500\u2500 derived_data \u2514\u2500\u2500 raw_data \u251c\u2500\u2500 landsat8_data \u251c\u2500\u2500 sentinel2_data \u251c\u2500\u2500 scene_id_list \u2514\u2500\u2500 lasrc_auxiliary_data Onde: raw_data Diret\u00f3rio onde os dados de entrada devem ser armazenados. Os subdiret\u00f3rio desse, s\u00e3o explicados abaixo. raw_data/landsat8_data Nesse diret\u00f3rio, devem ser colocados todos os dados Landsat-8/OLI que podem ser utilizados como entrada nos scripts de processamento. Para o caso do Landsat-8/OLI, \u00e9 esperado que as imagens estejam separadas por diret\u00f3rios. Abaixo, \u00e9 apresentado a estrutura desse diret\u00f3rio: landsat8_data \u251c\u2500\u2500 LC08_L2SP_222081_20171120_20200902_02_T1 \u2514\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1 A organiza\u00e7\u00e3o das imagens dentro de cada diret\u00f3rio, deve seguir o formato utilizado pela U nited S tates G eological S urvey (USGS) na distribui\u00e7\u00e3o dos dados L2 (com corre\u00e7\u00e3o atmosf\u00e9rica) da cole\u00e7\u00e3o 2 (C2). Al\u00e9m disso, a nomenclatura do diret\u00f3rio, tamb\u00e9m deve seguir o padr\u00e3o de dissemina\u00e7\u00e3o da USGS. Caso esse padr\u00e3o n\u00e3o seja seguido, erros podem surgir durante as etapas de processamento. Abaixo, tem um exemplo de como esse diret\u00f3rio deve estar organizado internamente: landsat8_data \u2514\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1 \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ANG.txt \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_MTL.json \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_MTL.txt \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_MTL.xml \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_QA_PIXEL.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_QA_RADSAT.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_SR_B1.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_SR_B2.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_SR_B3.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_SR_B4.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_SR_B5.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_SR_B6.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_SR_B7.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_SR_QA_AEROSOL.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_SR_stac.json \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_ATRAN.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_B10.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_CDIST.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_DRAD.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_EMIS.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_EMSD.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_QA.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_stac.json \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_TRAD.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_URAD.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_thumb_large.jpeg \u2514\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_thumb_small.jpeg raw_data/sentinel2_data Nesse diret\u00f3rio, devem ser colocados todos os dados Sentinel-2/MSI que podem ser utilizados como entrada nos scripts de processamento. Para o caso do Sentinel-2/MSI, \u00e9 esperado que as imagens estejam separadas por diret\u00f3rios. Abaixo, \u00e9 apresentado a estrutura desse diret\u00f3rio: sentinel2_data \u251c\u2500\u2500 S2B_MSIL1C_20171119T133209_N0206_R081_T22JBM_20171120T175608.SAFE \u2514\u2500\u2500 S2B_MSIL1C_20171122T134159_N0206_R124_T22JBM_20171122T200800.SAFE A organiza\u00e7\u00e3o das imagens dentro de cada diret\u00f3rio, deve seguir o formato dos diret\u00f3rios .SAFE disponibilizados pela ESA. Deve-se notar que, a nomenclatura do diret\u00f3rio, tamb\u00e9m deve seguir o padr\u00e3o de dissemina\u00e7\u00e3o da ESA (Incluindo o .SAFE ). Caso esse padr\u00e3o n\u00e3o seja seguido, erros podem surgir durante as etapas de processamento. Abaixo, tem um exemplo de como esse diret\u00f3rio deve estar organizado internamente: sentinel2_data \u2514\u2500\u2500 S2B_MSIL1C_20171119T133209_N0206_R081_T22JBM_20171120T175608.SAFE \u251c\u2500\u2500 AUX_DATA \u251c\u2500\u2500 DATASTRIP \u251c\u2500\u2500 GRANULE \u251c\u2500\u2500 HTML \u251c\u2500\u2500 INSPIRE.xml \u251c\u2500\u2500 manifest.safe \u251c\u2500\u2500 MTD_MSIL1C.xml \u251c\u2500\u2500 rep_info \u2514\u2500\u2500 S2B_MSIL1C_20171119T133209_N0206_R081_T22JBM_20171120T175608-ql.jpg raw_data/scene_id_list Diret\u00f3rio onde est\u00e3o os arquivos de defini\u00e7\u00e3o de quais dados est\u00e3o dispon\u00edveis nos diret\u00f3rios landsat8_data e sentinel2_data , devem ser considerados nas etapas de processamento. Para isso, nesse diret\u00f3rio tem-se dois arquivos: scene_ids_lc8.txt : Arquivo de defini\u00e7\u00e3o de quais imagens Landsat-8/OLI do diret\u00f3rio ( landsat8_data ) devem ser processados; scene_ids_s2.txt : Arquivo de defini\u00e7\u00e3o de quais imagens Sentinel-2/MSI do diret\u00f3rio ( sentinel2_data ) devem ser processados. Em ambos os arquivos, deve-se definir o nome dos diret\u00f3rios das imagens que devem ser consideradas nas etapas de processamento. Para exemplificar sua utiliza\u00e7\u00e3o, considere os seguintes diret\u00f3rios landsat8_data e sentinel2_data : landsat8_data \u251c\u2500\u2500 LC08_L2SP_222081_20171120_20200902_02_T1 \u2514\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1 sentinel2_data \u251c\u2500\u2500 S2B_MSIL1C_20171119T133209_N0206_R081_T22JBM_20171120T175608.SAFE \u2514\u2500\u2500 S2B_MSIL1C_20171122T134159_N0206_R124_T22JBM_20171122T200800.SAFE Para que esses dados sejam considerados nos scripts de processamento, os arquivos de defini\u00e7\u00e3o devem ser preenchidos da seguinte forma: scene_ids_lc8.txt LC08_L2SP_222081_20171120_20200902_02_T1 LC08_L2SP_223081_20171111_20200902_02_T1 scene_ids_s2.txt S2B_MSIL1C_20171119T133209_N0206_R081_T22JBM_20171120T175608.SAFE S2B_MSIL1C_20171122T134159_N0206_R124_T22JBM_20171122T200800.SAFE Os diret\u00f3rios que n\u00e3o s\u00e3o listados nesses arquivos, n\u00e3o s\u00e3o considerados nas etapas de processamento. raw_data/lasrc_auxiliary_data Nesse diret\u00f3rio, devem estar armazenados os dados auxiliares utilizados na aplica\u00e7\u00e3o do LaSRC. A organiza\u00e7\u00e3o do diret\u00f3rio deve ser a mesma utilizada no FTP de dissemina\u00e7\u00e3o USGS da USGS. Abaixo, tem-se um exemplo de organiza\u00e7\u00e3o: lasrc_auxiliary_data \u251c\u2500\u2500 CMGDEM.hdf \u251c\u2500\u2500 LADS \u251c\u2500\u2500 LDCMLUT \u251c\u2500\u2500 MSILUT \u2514\u2500\u2500 ratiomapndwiexp.hdf Deve-se notar que para o fluxo de processamento, apenas os arquivos do diret\u00f3rio lasrc_auxiliary_data/LADS devem ser alterados. Nesse diret\u00f3rio, os arquivos equivalentes as imagens Sentinel-2/MSI que ser\u00e3o processadas devem estar dispon\u00edveis. Para exemplificar, considere o diret\u00f3rio sentinel2_data abaixo: sentinel2_data \u251c\u2500\u2500 S2B_MSIL1C_20171119T133209_N0206_R081_T22JBM_20171120T175608.SAFE \u2514\u2500\u2500 S2B_MSIL1C_20171122T134159_N0206_R124_T22JBM_20171122T200800.SAFE Para que todas as imagens desse diret\u00f3rio sejam processadas com o LaSRC durante a execu\u00e7\u00e3o dos scripts de processamento, os seus respectivos arquivos LADS devem estar definidos. Nesse caso: lasrc_auxiliary_data \u251c\u2500\u2500 CMGDEM.hdf \u2514\u2500\u2500 LADS \u2514\u2500\u2500 2017 \u251c\u2500\u2500 L8ANC2017323.hdf_fused \u2514\u2500\u2500 L8ANC2017326.hdf_fused derived_data Diret\u00f3rio onde os resultados gerados ser\u00e3o armazenados. Ao final da execu\u00e7\u00e3o do script de processamento, esse diret\u00f3rio ter\u00e1 a seguinte estrutura: derived_data \u251c\u2500\u2500 l8 \u2502 \u251c\u2500\u2500 lc8_nbar_angles \u2502 \u2514\u2500\u2500 lc8_nbar \u2514\u2500\u2500 s2 \u251c\u2500\u2500 s2_lasrc_sr \u251c\u2500\u2500 s2_lasrc_nbar \u251c\u2500\u2500 s2_sen2cor_sr \u2514\u2500\u2500 s2_sen2cor_nbar Onde: Landsat-8/OLI data l8/lc8_nbar_angles : Bandas de \u00e2ngulos geradas para os dados Landsat-8/OLI; l8/lc8_nbar : Produtos NBAR gerados com dados Landsat-8/OLI. Sentinel-2/MSI LaSRC data s2/s2_lasrc_sr : Dados Sentinel-2/MSI com corre\u00e7\u00e3o atmosf\u00e9rica feita atrav\u00e9s da ferramenta LaSRC; s2/s2_lasrc_nbar : Produtos NBAR gerados com dados Sentinel-2/MSI com corre\u00e7\u00e3o atmosf\u00e9rica feita atrav\u00e9s da ferramenta LaSRC (O mesmo diret\u00f3rio s2/s2_lasrc_sr ). Sentinel-2/MSI Sen2Cor data s2/s2_sen2cor_sr : Dados Sentinel-2/MSI com corre\u00e7\u00e3o atmosf\u00e9rica feita atrav\u00e9s da ferramenta Sen2Cor; s2/s2_sen2cor_nbar : Produtos NBAR gerados com dados Sentinel-2/MSI com corre\u00e7\u00e3o atmosf\u00e9rica feita atrav\u00e9s da ferramenta Sen2Cor (O mesmo diret\u00f3rio s2/s2_sen2cor_sr ).","title":"Scripts de processamento"},{"location":"tools/processing/#scripts-de-processamento","text":"Para a implementa\u00e7\u00e3o da metodologia dos experimentos realizados neste RC , inicialmente fez-se a cria\u00e7\u00e3o de um conjunto de bibliotecas de software . Com base nessas bibliotecas e todas as funcionalidades que essas disponibilizam, n\u00f3s realizamos a implementa\u00e7\u00e3o de scripts de processamento. Esses scripts representam a materializa\u00e7\u00e3o da metodologia dos experimentos realizados neste RC . Esses scripts de processamento, foram implementados utilizando duas ferramentas: Jupyter Notebook : Ambiente de computa\u00e7\u00e3o interativa, que permitem a cria\u00e7\u00e3o de notebooks computacionais que implementam os conceitos de computa\u00e7\u00e3o letrada, o qual permite a mescla de textos e c\u00f3digo em um mesmo documento; Dagster : Plataforma que permite a cria\u00e7\u00e3o, gerenciamento e execu\u00e7\u00e3o de fluxos de processamento e manipula\u00e7\u00e3o de dados. Ambas implementa\u00e7\u00f5es listadas acima, utilizam as mesmas bibliotecas e vers\u00f5es para a realiza\u00e7\u00e3o do processamento, garantindo a produ\u00e7\u00e3o de resultados reprodut\u00edveis e compar\u00e1veis. A escolha pela implementa\u00e7\u00e3o do fluxo de processamento em duas ferramentas distintas foi realizada por alguns motivos, sendo os principais deles listados abaixo: Apresenta\u00e7\u00e3o das capacidades de uso das biblioteca criadas neste RC ; Abrang\u00eancia nos cen\u00e1rios de uso da metodologia implementada neste RC ; Dupla implementa\u00e7\u00e3o, o que permite a compara\u00e7\u00e3o dos resultados gerados de diferentes formas, partindo das mesmas ferramentas. De forma geral, a liga\u00e7\u00e3o de cada um desses scripts de processamento e as bibliotecas de software desenvolvidas nesse RC , s\u00e3o apresentadas na Figura abaixo: Rela\u00e7\u00e3o entre as bibliotecas de funcionalidades e scripts de processamento.","title":"Scripts de processamento"},{"location":"tools/processing/#jupyter-notebook","text":"Por vezes, quando a metodologia implementada est\u00e1 sendo explorado, por exemplo, em cen\u00e1rios de desenvolvimento, \u00e9 necess\u00e1rio que todos os passos sejam bem documentos e organizados de forma clara. Para esse tipo de cen\u00e1rio, foi realizada a implementa\u00e7\u00e3o da metodologia deste RC utilizando o Jupyter Notebook. No Jupyter Notebook implementado, todas as etapas s\u00e3o documentadas e descritas, de modo que todos possam explorar e enteder o que est\u00e1 sendo realizado. No v\u00eddeo abaixo, tem-se a apresenta\u00e7\u00e3o de algumas partes desse notebook. Exemplo de parte do Jupyter Notebook de processamento.","title":"Jupyter Notebook"},{"location":"tools/processing/#configuracoes","text":"Para a utiliza\u00e7\u00e3o desse Jupyter Notebook \u00e9 necess\u00e1rio apenas uma configura\u00e7\u00e3o: A defini\u00e7\u00e3o da vari\u00e1vel de ambiente DATA_DIRECTORY . Essa vari\u00e1vel de ambiente, determina o local onde est\u00e1 o diret\u00f3rio de dados que dever\u00e1 ser utilizado pelo notebook para carregar os dados de entrada e salvar os resultados gerados. Sobre o diret\u00f3rio de dados Os detalhes de como o diret\u00f3rio de dados definido na vari\u00e1vel de ambiente DATA_DIRECTORY deve estar organizado s\u00e3o apresentados na se\u00e7\u00e3o Diret\u00f3rio de dados .","title":"Configura\u00e7\u00f5es"},{"location":"tools/processing/#dagster","text":"Sobre a vers\u00e3o do Dagster No momento em que esse RC estava sendo constru\u00eddo, a vers\u00e3o oficial do Dagste era 0.12.15 . Atualmente, as vers\u00f5es mudaram, o que trouxe novas nomenclaturas e defini\u00e7\u00f5es ao software . Para manter o conte\u00fado consistente, as explica\u00e7\u00f5es utilizadas nessa documenta\u00e7\u00e3o, foram criadas com base na vers\u00e3o 0.12.15 . Nomenclaturas No Dagster, todo fluxo de processamento criado \u00e9 chamado de Pipeline . Neste documento, para manter a consist\u00eancia com as explica\u00e7\u00f5es realizadas com o Jupyter Notebook, os pipelines ser\u00e3o genericamente tratados como scripts de processamento. Caso voc\u00ea deseje, pode consultar a documenta\u00e7\u00e3o oficial do Dagster (v0.12.15) onde tem-se uma explica\u00e7\u00e3o detalhada do que s\u00e3o os Pipelines . Com a metodologia implementada e pronta para ser utilizada, sua execu\u00e7\u00e3o com o uso de grandes volumes de dados pode exigir diversos controles e a\u00e7\u00f5es como: Execu\u00e7\u00e3o paralela das opera\u00e7\u00f5es; Controle de falhas; Reexecu\u00e7\u00e3o. Com base nessa necessidade, foi criado o script de processamento utilizando a ferramenta Dagster . Com essa ferramenta, toda a orquestra\u00e7\u00e3o da opera\u00e7\u00e3o pode ser feita de maneira simples e direta. Al\u00e9m disso, quando necess\u00e1rio e devidamente configurado, diferentes formas de execu\u00e7\u00e3o podem ser adotados, como execu\u00e7\u00f5es distribu\u00eddas, paralelas. Outro ponto positivo ao uso do Dagster est\u00e1 no controle de falhas e reexecu\u00e7\u00e3o. A cria\u00e7\u00e3o do script de processamento \u00e9 feita utilizando uma interface de programa\u00e7\u00e3o (API) em linguagem Python, no entanto, a manipula\u00e7\u00e3o e uso do script \u00e9 feita atrav\u00e9s de uma interface web . Nessa interface, tem-se dispon\u00edvel op\u00e7\u00f5es para executar o processamento, fazer seu gerenciamento, bem como consultar a documenta\u00e7\u00e3o de cada uma das etapas e ver o fluxo geral que \u00e9 realizado. O v\u00eddeo abaixo, apresenta uma vis\u00e3o geral dessa interface: Exemplo de parte da interface do Dagster com a vis\u00e3o do fluxo de processamento.","title":"Dagster"},{"location":"tools/processing/#configuracoes_1","text":"Para realizar a execu\u00e7\u00e3o do script de processamento criado com o Dagster, \u00e9 necess\u00e1rio realizar a configura\u00e7\u00e3o da ferramenta. Essa configura\u00e7\u00e3o \u00e9 feita atrav\u00e9s da defini\u00e7\u00e3o de par\u00e2metros em um arquivo no formato YAML , o qual especifica: Onde os dados de entrada est\u00e3o armazenados; Qual ser\u00e1 o diret\u00f3rio de sa\u00edda; Quais imagens e bandas espectrais devem ser utilizadas; Especifica\u00e7\u00e3o de recursos computacionais (e.g., Quantidade de CPU utilizada). Esse arquivo, uma vez criado, \u00e9 inserido na interface do Dagster, onde \u00e9 feita sua valida\u00e7\u00e3o e uso para a cria\u00e7\u00e3o de uma nova execu\u00e7\u00e3o do script . Sobre execu\u00e7\u00f5es no Dagster Para saber mais detalhes sobre como o Dagster realiza a cria\u00e7\u00e3o de suas execu\u00e7\u00f5es na vers\u00e3o 0.12.15 , por favor, consulte a documenta\u00e7\u00e3o oficial da ferramenta Para que voc\u00ea seja capaz de criar configura\u00e7\u00f5es do Dagster para realizar execu\u00e7\u00f5es com seus pr\u00f3prios dados e necessidades, nas subse\u00e7\u00f5es abaixo faz-se a apresenta\u00e7\u00e3o da estrutura do arquivo de configura\u00e7\u00e3o YAML do Dagster.","title":"Configura\u00e7\u00f5es"},{"location":"tools/processing/#arquivo-de-configuracao-dagster","text":"O arquivo de configura\u00e7\u00e3o Dagster, como mencionado anteriormente, \u00e9 utilizado para definir os recursos que ser\u00e3o utilizados durante a execu\u00e7\u00e3o do script de processamento. Esse arquivo de configura\u00e7\u00e3o, no formato YAML , \u00e9 dividido em duas principais se\u00e7\u00f5es: (i) Resources; (ii) Solids. Cada uma dessas se\u00e7\u00f5es, \u00e9 respons\u00e1vel por realizar uma parte da configura\u00e7\u00e3o, de modo a permitir que as defini\u00e7\u00f5es mencionadas anteriormente possam ser realizadas. Cada se\u00e7\u00e3o e as configura\u00e7\u00f5es definidas s\u00e3o apresentadas nos t\u00f3picos a seguir Sobre o arquivo de configura\u00e7\u00e3o Ao utilizar o Jupyter Notebook, toda a configura\u00e7\u00e3o \u00e9 feita atrav\u00e9s de uma \u00fanica vari\u00e1vel de ambiente ( DATA_DIRECTORY ). No Dagster, o arquivo de configura\u00e7\u00e3o tem exatamente o mesmo papel. Sendo assim, \u00e9 recomendado que antes de voc\u00ea acompanhar os t\u00f3picos de defini\u00e7\u00e3o do arquivo de configura\u00e7\u00e3o, voc\u00ea consulte a se\u00e7\u00e3o Diret\u00f3rio de dados , na qual \u00e9 especificado a estrutura de diret\u00f3rios que deve ser seguida, bem como o conte\u00fado que deve estar dispon\u00edvel em cada se\u00e7\u00e3o.","title":"Arquivo de configura\u00e7\u00e3o Dagster"},{"location":"tools/processing/#resources","text":"No Dagster, resources s\u00e3o utilizados para representar elementos e recursos do ambiente computacional que devem estar dispon\u00edveis para a execu\u00e7\u00e3o do script de processamento. No caso do script de processamento desse RC , os resources representam os dados que devem ser utilizados durante o processamento. Sendo assim, a um resource , pode-se ter associado um ou mais diret\u00f3rios de dados. Ao todo, o script de processamento necessita de dois resources : lasrc_data Resource onde tem-se a disponibiliza\u00e7\u00e3o de dados relacionados a ferramenta LaSRC. A defini\u00e7\u00e3o de um resource lasrc_data requer a especifica\u00e7\u00e3o da seguinte vari\u00e1vel de configura\u00e7\u00e3o: lasrc_auxiliary_data_dir : Vari\u00e1vel onde faz-se a defini\u00e7\u00e3o do caminho para o diret\u00f3rio de dados auxiliares da ferramente LaSRC, requeridos para a realiza\u00e7\u00e3o da corre\u00e7\u00e3o atmosf\u00e9rica (usado neste RC em imagens Sentinel-2/MSI). Organiza\u00e7\u00e3o do diret\u00f3rio de dados Para mais detalhes de como esse diret\u00f3rio de dados auxiliares deve ser organizado, por favor, consulte a se\u00e7\u00e3o Diret\u00f3rio de dados . Um exemplo completo de defini\u00e7\u00e3o de um resource lasrc_data \u00e9 apresentado abaixo: resources : lasrc_data : config : lasrc_auxiliary_data_dir : /path/to/lasrc/auxiliary/data Esse bloco de c\u00f3digo deve ser definido dentro do arquivo de configura\u00e7\u00e3o. Para um exemplo completo, consulte o arquivo de configura\u00e7\u00e3o do dagster . repository Resource onde faz-se a defini\u00e7\u00e3o dos diret\u00f3rios de entrada e sa\u00edda do script de processamento. A defini\u00e7\u00e3o de um resource repository requer a especifica\u00e7\u00e3o das seguintes vari\u00e1veis de configura\u00e7\u00e3o: landsat8_input_dir : Diret\u00f3rio onde est\u00e3o dispon\u00edveis os dados Landsat-8/OLI, que podem ser utilizados como entrada para o script de processamento; sentinel2_input_dir : Diret\u00f3rio onde est\u00e3o dispon\u00edveis os dados Sentinel-2/MSI, que podem ser utilizados como entrada para o script de processamento; derived_data_dir : Diret\u00f3rio onde os dados gerados durante as etapas de processamento ser\u00e3o salvos. Organiza\u00e7\u00e3o do diret\u00f3rio de dados Para mais detalhes de como cada um desses diret\u00f3rios devem ser organizados, por favor, consulte a se\u00e7\u00e3o Diret\u00f3rio de dados . Um exemplo completo de defini\u00e7\u00e3o de um resource repository \u00e9 apresentado abaixo: resources : repository : config : derived_data_dir : /path/to/derived/data/dir landsat8_input_dir : /path/to/input/landsat8/data/dir sentinel2_input_dir : /path/to/input/sentinel2/data/dir Esse bloco de c\u00f3digo deve ser definido dentro do arquivo de configura\u00e7\u00e3o. Para um exemplo completo, consulte o arquivo de configura\u00e7\u00e3o do dagster .","title":"Resources"},{"location":"tools/processing/#solids","text":"No Dagster, solids representam a unidade funcional de trabalho que far\u00e1 a execu\u00e7\u00e3o de uma opera\u00e7\u00e3o de fato. Esses elementos s\u00e3o os respons\u00e1veis por receber as entradas, realizar um processamento e gerar as sa\u00eddas. Assim, no script de processamento, eles foram utilizados para representar cada uma das etapas de processamento. Os solids podem ou n\u00e3o requerer algum tipo de configura\u00e7\u00e3o para sua defini\u00e7\u00e3o. No caso do script de processamento deste RC , apenas um deles necessita de configura\u00e7\u00e3o, sendo ele: load_and_standardize_sceneids_input : solid respons\u00e1vel em receber os arquivos com a defini\u00e7\u00e3o das cenas Landsat-8/OLI e Sentinel-2/MSI que dever\u00e3o ser processadas. Ao receber esses arquivos, o solid faz a leitura, valida\u00e7\u00e3o e ent\u00e3o passa as informa\u00e7\u00f5es carregadas para as etapas subsequentes, que as utilizam para a realiza\u00e7\u00e3o das atividades de processamento. Na configura\u00e7\u00e3o desse solid , faz-se necess\u00e1rio a especifica\u00e7\u00e3o das seguintes vari\u00e1veis de configura\u00e7\u00e3o: landsat8_sceneid_list : Arquivo .txt com o nome das cenas Landsat-8/OLI que dever\u00e3o ser consideradas no fluxo de processamento. Esses nomes devem ser o mesmo dispon\u00edvel no diret\u00f3rio de dados Landsat-8/OLI ( landsat8_input_dir ) especificado no resource repository ; sentinel2_sceneid_list : Arquivo .txt com o nome das cenas Sentinel-2/MSI que dever\u00e3o ser consideradas no fluxo de processamento. Esses nomes devem ser o mesmo dispon\u00edvel no diret\u00f3rio de dados Sentinel-2/MSI ( sentinel2_input_dir ) especificado no resource repository ; Organiza\u00e7\u00e3o do diret\u00f3rio de dados Para mais detalhes de como os arquivos com os nomes das cenas devem estar organizados, por favor, consulte a se\u00e7\u00e3o Diret\u00f3rio de dados . Um exemplo completo de defini\u00e7\u00e3o de um solid load_and_standardize_sceneids_input \u00e9 apresentado abaixo: solids : load_and_standardize_sceneids_input : config : landsat8_sceneid_list : /path/to/input/landsat8/data/landsat.txt sentinel2_sceneid_list : /path/to/input/sentinel2/data/sentinel.txt","title":"Solids"},{"location":"tools/processing/#exemplo-completo","text":"Abaixo, faz-se a apresenta\u00e7\u00e3o de um arquivo completo de configura\u00e7\u00e3o Dagster, que utiliza todos os resources e solids especificados nos t\u00f3picos anteriores: Para exemplificar todos os elementos citados anteriormente em um arquivo de configura\u00e7\u00e3o, abaixo \u00e9 apresentado a configura\u00e7\u00e3o m\u00ednima necess\u00e1ria para a execu\u00e7\u00e3o do pipeline deste RC . resources : lasrc_data : config : lasrc_auxiliary_data_dir : /path/to/lasrc/auxiliary/data repository : config : derived_data_dir : /path/to/derived/data/dir landsat8_input_dir : /path/to/input/landsat8/data/dir sentinel2_input_dir : /path/to/input/sentinel2/data/dir solids : load_and_standardize_sceneids_input : config : landsat8_sceneid_list : /path/to/input/landsat8/data/landsat.txt sentinel2_sceneid_list : /path/to/input/sentinel2/data/sentinel.txt","title":"Exemplo completo"},{"location":"tools/processing/#diretorio-de-dados","text":"Em ambas as implementa\u00e7\u00f5es dos scripts de processamento, na etapa de configura\u00e7\u00e3o, faz-se necess\u00e1rio a defini\u00e7\u00e3o do caminho para o diret\u00f3rio de dados. Esse diret\u00f3rio possui uma organiza\u00e7\u00e3o padr\u00e3o, de modo que, independente da forma com que a execu\u00e7\u00e3o ser\u00e1 realizada, a mesma organiza\u00e7\u00e3o de dados pode ser seguida. Nesta se\u00e7\u00e3o, ser\u00e1 apresentado o formato de organiza\u00e7\u00e3o desse diret\u00f3rio de dados. Para come\u00e7ar, primeiro \u00e9 importante entender que: O diret\u00f3rio de dados, utilizado em ambos scripts de processamento, representam o diret\u00f3rio de entradas e sa\u00eddas do script . Todas as entradas s\u00e3o lidas desse diret\u00f3rios, n\u00e3o sendo poss\u00edvel ler dados de diferentes diret\u00f3rios. O mesmo vale para as sa\u00eddas, os resultados produzidos sempre ser\u00e3o armazenados em nesse diret\u00f3rio. A l\u00f3gica por tr\u00e1s da defini\u00e7\u00e3o desse diret\u00f3rio \u00e9 a da organiza\u00e7\u00e3o: Se o pesquisador precisa centralizar e manter organizado em uma estrutura l\u00f3gica todos os materiais que ele est\u00e1 utilizando para produzir os resultados, bem como os resultados propriamente ditos, ele minimamente ter\u00e1 de manter seus objetos de pesquisa organizados. Para que esse diret\u00f3rio suporte toda essas utilidades, diferentes subdiret\u00f3rios s\u00e3o criados abaixo dele. Nesses diret\u00f3rios tem-se a defini\u00e7\u00e3o dos dados de entrada e sa\u00edda. Abaixo, a estrutura do diret\u00f3rio de dados, esperada por ambos os scripts de processamento ser\u00e3o apresentados: data directory \u251c\u2500\u2500 derived_data \u2514\u2500\u2500 raw_data \u251c\u2500\u2500 landsat8_data \u251c\u2500\u2500 sentinel2_data \u251c\u2500\u2500 scene_id_list \u2514\u2500\u2500 lasrc_auxiliary_data Onde: raw_data Diret\u00f3rio onde os dados de entrada devem ser armazenados. Os subdiret\u00f3rio desse, s\u00e3o explicados abaixo. raw_data/landsat8_data Nesse diret\u00f3rio, devem ser colocados todos os dados Landsat-8/OLI que podem ser utilizados como entrada nos scripts de processamento. Para o caso do Landsat-8/OLI, \u00e9 esperado que as imagens estejam separadas por diret\u00f3rios. Abaixo, \u00e9 apresentado a estrutura desse diret\u00f3rio: landsat8_data \u251c\u2500\u2500 LC08_L2SP_222081_20171120_20200902_02_T1 \u2514\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1 A organiza\u00e7\u00e3o das imagens dentro de cada diret\u00f3rio, deve seguir o formato utilizado pela U nited S tates G eological S urvey (USGS) na distribui\u00e7\u00e3o dos dados L2 (com corre\u00e7\u00e3o atmosf\u00e9rica) da cole\u00e7\u00e3o 2 (C2). Al\u00e9m disso, a nomenclatura do diret\u00f3rio, tamb\u00e9m deve seguir o padr\u00e3o de dissemina\u00e7\u00e3o da USGS. Caso esse padr\u00e3o n\u00e3o seja seguido, erros podem surgir durante as etapas de processamento. Abaixo, tem um exemplo de como esse diret\u00f3rio deve estar organizado internamente: landsat8_data \u2514\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1 \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ANG.txt \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_MTL.json \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_MTL.txt \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_MTL.xml \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_QA_PIXEL.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_QA_RADSAT.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_SR_B1.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_SR_B2.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_SR_B3.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_SR_B4.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_SR_B5.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_SR_B6.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_SR_B7.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_SR_QA_AEROSOL.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_SR_stac.json \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_ATRAN.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_B10.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_CDIST.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_DRAD.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_EMIS.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_EMSD.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_QA.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_stac.json \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_TRAD.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_URAD.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_thumb_large.jpeg \u2514\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_thumb_small.jpeg raw_data/sentinel2_data Nesse diret\u00f3rio, devem ser colocados todos os dados Sentinel-2/MSI que podem ser utilizados como entrada nos scripts de processamento. Para o caso do Sentinel-2/MSI, \u00e9 esperado que as imagens estejam separadas por diret\u00f3rios. Abaixo, \u00e9 apresentado a estrutura desse diret\u00f3rio: sentinel2_data \u251c\u2500\u2500 S2B_MSIL1C_20171119T133209_N0206_R081_T22JBM_20171120T175608.SAFE \u2514\u2500\u2500 S2B_MSIL1C_20171122T134159_N0206_R124_T22JBM_20171122T200800.SAFE A organiza\u00e7\u00e3o das imagens dentro de cada diret\u00f3rio, deve seguir o formato dos diret\u00f3rios .SAFE disponibilizados pela ESA. Deve-se notar que, a nomenclatura do diret\u00f3rio, tamb\u00e9m deve seguir o padr\u00e3o de dissemina\u00e7\u00e3o da ESA (Incluindo o .SAFE ). Caso esse padr\u00e3o n\u00e3o seja seguido, erros podem surgir durante as etapas de processamento. Abaixo, tem um exemplo de como esse diret\u00f3rio deve estar organizado internamente: sentinel2_data \u2514\u2500\u2500 S2B_MSIL1C_20171119T133209_N0206_R081_T22JBM_20171120T175608.SAFE \u251c\u2500\u2500 AUX_DATA \u251c\u2500\u2500 DATASTRIP \u251c\u2500\u2500 GRANULE \u251c\u2500\u2500 HTML \u251c\u2500\u2500 INSPIRE.xml \u251c\u2500\u2500 manifest.safe \u251c\u2500\u2500 MTD_MSIL1C.xml \u251c\u2500\u2500 rep_info \u2514\u2500\u2500 S2B_MSIL1C_20171119T133209_N0206_R081_T22JBM_20171120T175608-ql.jpg raw_data/scene_id_list Diret\u00f3rio onde est\u00e3o os arquivos de defini\u00e7\u00e3o de quais dados est\u00e3o dispon\u00edveis nos diret\u00f3rios landsat8_data e sentinel2_data , devem ser considerados nas etapas de processamento. Para isso, nesse diret\u00f3rio tem-se dois arquivos: scene_ids_lc8.txt : Arquivo de defini\u00e7\u00e3o de quais imagens Landsat-8/OLI do diret\u00f3rio ( landsat8_data ) devem ser processados; scene_ids_s2.txt : Arquivo de defini\u00e7\u00e3o de quais imagens Sentinel-2/MSI do diret\u00f3rio ( sentinel2_data ) devem ser processados. Em ambos os arquivos, deve-se definir o nome dos diret\u00f3rios das imagens que devem ser consideradas nas etapas de processamento. Para exemplificar sua utiliza\u00e7\u00e3o, considere os seguintes diret\u00f3rios landsat8_data e sentinel2_data : landsat8_data \u251c\u2500\u2500 LC08_L2SP_222081_20171120_20200902_02_T1 \u2514\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1 sentinel2_data \u251c\u2500\u2500 S2B_MSIL1C_20171119T133209_N0206_R081_T22JBM_20171120T175608.SAFE \u2514\u2500\u2500 S2B_MSIL1C_20171122T134159_N0206_R124_T22JBM_20171122T200800.SAFE Para que esses dados sejam considerados nos scripts de processamento, os arquivos de defini\u00e7\u00e3o devem ser preenchidos da seguinte forma: scene_ids_lc8.txt LC08_L2SP_222081_20171120_20200902_02_T1 LC08_L2SP_223081_20171111_20200902_02_T1 scene_ids_s2.txt S2B_MSIL1C_20171119T133209_N0206_R081_T22JBM_20171120T175608.SAFE S2B_MSIL1C_20171122T134159_N0206_R124_T22JBM_20171122T200800.SAFE Os diret\u00f3rios que n\u00e3o s\u00e3o listados nesses arquivos, n\u00e3o s\u00e3o considerados nas etapas de processamento. raw_data/lasrc_auxiliary_data Nesse diret\u00f3rio, devem estar armazenados os dados auxiliares utilizados na aplica\u00e7\u00e3o do LaSRC. A organiza\u00e7\u00e3o do diret\u00f3rio deve ser a mesma utilizada no FTP de dissemina\u00e7\u00e3o USGS da USGS. Abaixo, tem-se um exemplo de organiza\u00e7\u00e3o: lasrc_auxiliary_data \u251c\u2500\u2500 CMGDEM.hdf \u251c\u2500\u2500 LADS \u251c\u2500\u2500 LDCMLUT \u251c\u2500\u2500 MSILUT \u2514\u2500\u2500 ratiomapndwiexp.hdf Deve-se notar que para o fluxo de processamento, apenas os arquivos do diret\u00f3rio lasrc_auxiliary_data/LADS devem ser alterados. Nesse diret\u00f3rio, os arquivos equivalentes as imagens Sentinel-2/MSI que ser\u00e3o processadas devem estar dispon\u00edveis. Para exemplificar, considere o diret\u00f3rio sentinel2_data abaixo: sentinel2_data \u251c\u2500\u2500 S2B_MSIL1C_20171119T133209_N0206_R081_T22JBM_20171120T175608.SAFE \u2514\u2500\u2500 S2B_MSIL1C_20171122T134159_N0206_R124_T22JBM_20171122T200800.SAFE Para que todas as imagens desse diret\u00f3rio sejam processadas com o LaSRC durante a execu\u00e7\u00e3o dos scripts de processamento, os seus respectivos arquivos LADS devem estar definidos. Nesse caso: lasrc_auxiliary_data \u251c\u2500\u2500 CMGDEM.hdf \u2514\u2500\u2500 LADS \u2514\u2500\u2500 2017 \u251c\u2500\u2500 L8ANC2017323.hdf_fused \u2514\u2500\u2500 L8ANC2017326.hdf_fused derived_data Diret\u00f3rio onde os resultados gerados ser\u00e3o armazenados. Ao final da execu\u00e7\u00e3o do script de processamento, esse diret\u00f3rio ter\u00e1 a seguinte estrutura: derived_data \u251c\u2500\u2500 l8 \u2502 \u251c\u2500\u2500 lc8_nbar_angles \u2502 \u2514\u2500\u2500 lc8_nbar \u2514\u2500\u2500 s2 \u251c\u2500\u2500 s2_lasrc_sr \u251c\u2500\u2500 s2_lasrc_nbar \u251c\u2500\u2500 s2_sen2cor_sr \u2514\u2500\u2500 s2_sen2cor_nbar Onde: Landsat-8/OLI data l8/lc8_nbar_angles : Bandas de \u00e2ngulos geradas para os dados Landsat-8/OLI; l8/lc8_nbar : Produtos NBAR gerados com dados Landsat-8/OLI. Sentinel-2/MSI LaSRC data s2/s2_lasrc_sr : Dados Sentinel-2/MSI com corre\u00e7\u00e3o atmosf\u00e9rica feita atrav\u00e9s da ferramenta LaSRC; s2/s2_lasrc_nbar : Produtos NBAR gerados com dados Sentinel-2/MSI com corre\u00e7\u00e3o atmosf\u00e9rica feita atrav\u00e9s da ferramenta LaSRC (O mesmo diret\u00f3rio s2/s2_lasrc_sr ). Sentinel-2/MSI Sen2Cor data s2/s2_sen2cor_sr : Dados Sentinel-2/MSI com corre\u00e7\u00e3o atmosf\u00e9rica feita atrav\u00e9s da ferramenta Sen2Cor; s2/s2_sen2cor_nbar : Produtos NBAR gerados com dados Sentinel-2/MSI com corre\u00e7\u00e3o atmosf\u00e9rica feita atrav\u00e9s da ferramenta Sen2Cor (O mesmo diret\u00f3rio s2/s2_sen2cor_sr ).","title":"Diret\u00f3rio de dados"},{"location":"tools/utilitary/","text":"Scripts auxiliares \u00b6 A constru\u00e7\u00e3o e publica\u00e7\u00e3o deste RC foi realizada em diversas etapas. Em cada etapa, al\u00e9m dos c\u00f3digos que fazem a gera\u00e7\u00e3o dos resultados propriamente ditos, foram desenvolvidos tamb\u00e9m diversos scripts auxiliares. Nesta se\u00e7\u00e3o, faz-se a apresenta\u00e7\u00e3o desses scripts , seus detalhes de uso e forma de configura\u00e7\u00e3o. Calculate Checksum e GitHub Asset Upload \u00b6 Para o compartilhamento deste RC , foi adotada uma estrat\u00e9gia em que todos os materiais que formam a vers\u00e3o completa do RC , foram organizados em um reposit\u00f3rio do GitHub. Nesse reposit\u00f3rio, tem-se todo o hist\u00f3rico de modifica\u00e7\u00f5es realizadas nos materiais, c\u00f3digos, documenta\u00e7\u00e3o e os dados. O armazenamento de materiais como os scripts e a documenta\u00e7\u00e3o n\u00e3o requerem muito espa\u00e7o em disco. Com isso, o uso do GitHub foi feito diretamente, sem a necessidade de modifica\u00e7\u00f5es. No entanto, os dados auxiliares, que s\u00e3o utilizadas nos exemplos de execu\u00e7\u00e3o e replica\u00e7\u00e3o dispon\u00edveis neste RC , possuem tamanhos que os tornam invi\u00e1veis para versionamento em reposit\u00f3rios git comum. Como alternativa, adotou-se a abordagem da publica\u00e7\u00e3o desses dados atrav\u00e9s dos Release Assets . Com essa alternativa, arquivos com at\u00e9 2 GB podem ser publicados e mantidos associados no reposit\u00f3rio. Com isso, fez-se necess\u00e1rio a prepara\u00e7\u00e3o e organiza\u00e7\u00e3o dos dados, de modo que eles pudessem ser compartilhados no GitHub Release Assets. Para essa prepara\u00e7\u00e3o, fez-se a cria\u00e7\u00e3o de dois scripts auxiliares: Calculate Checksum Calculate Checksum \u00e9 um script Python que realiza a cria\u00e7\u00e3o de BagIt e faz seu armazenamento em arquivos zip . GitHub Asset Upload Uma vez que os BagIt s\u00e3o criados, o script R GitHub Asset Upload faz o envio desses arquivos para os servidores do GitHub. O processo \u00e9 feito com o aux\u00edlio do pacote piggyback . Tendo como base esses dois pacotes, no reposit\u00f3rio onde foram armazenados os scripts e a documenta\u00e7\u00e3o, tamb\u00e9m pode-se disponibilizar os dados. Example toolkit \u00b6 Conforme apresentado na se\u00e7\u00e3o anterior, os dados disponibilizados junto a este RC , foram publicados utilizando o GitHub Release Assets . Para isso, s\u00e3o criados BagIt dos dados em arquivos no formato zip . Esses, por sua vez, s\u00e3o armazenados nos servidores do GitHub. Com essa estrat\u00e9gia, todos tem acesso aos dados e podem fazer seu uso para executar os exemplos disponibilizados neste RC . No entanto, para que os exemplos dispon\u00edveis neste RC possam ser utilizados, \u00e9 necess\u00e1rio que os dados estejam organizados nos diret\u00f3rios corretos do RC . Para resolver esse problema, e evitar o download e organiza\u00e7\u00e3o manual desses dados na estrutura do RC , fez-se a cria\u00e7\u00e3o de um script Python, o Example toolkit . Com essa ferramenta, toda as etapas de download e organiza\u00e7\u00e3o s\u00e3o completamente automatizadas, sendo necess\u00e1rio para o usu\u00e1rio, apenas definir os diret\u00f3rios que devem ser considerados no momento do download . Opera\u00e7\u00e3o \u00b6 Ao realizar a utiliza\u00e7\u00e3o do Example toolkit , o script far\u00e1 a execu\u00e7\u00e3o de quatro etapas principais. Cada uma dessas etapas s\u00e3o representadas na figura abaixo: Fluxo de opera\u00e7\u00e3o do Example Toolkit Conforme pode ser visto na figura acima, inicialmente, o Example toolkit realiza o download dos dados associados ao RC que est\u00e3o armazenados no GitHub Release Assets . Em seguida, para garantir que os dados baixados n\u00e3o sofreram mudan\u00e7as ou est\u00e3o corrompidos, \u00e9 feita a valida\u00e7\u00e3o do BagIts baixados, estrutura na qual os arquivos est\u00e3o salvos. Ap\u00f3s a valida\u00e7\u00e3o, a ferramenta faz a extra\u00e7\u00e3o dos dados para os diret\u00f3rios corretos. Por fim, com base nas defini\u00e7\u00f5es do usu\u00e1rio e onde os dados foram salvos, o Example toolkit faz a gera\u00e7\u00e3o de um arquivo de configura\u00e7\u00e3o Dagster que pode ser utilizado para iniciar o processamento dos dados baixados. Dagster e arquivo de configura\u00e7\u00e3o Caso voc\u00ea deseje saber mais sobre como o Dagster \u00e9 utilizado neste RC e onde o arquivo de configura\u00e7\u00e3o deve ser utilizado, consulte a Se\u00e7\u00e3o Scripts de processamento - Dagster . Na pr\u00f3xima se\u00e7\u00e3o, \u00e9 feita a apresenta\u00e7\u00e3o de como o Example toolkit \u00e9 configurado e executado. Utiliza\u00e7\u00e3o \u00b6 Por se tratar de um script Python, a utiliza\u00e7\u00e3o do Example toolkit , requer apenas que voc\u00ea tenha o ambiente Python com as devidas depend\u00eancias configuradas. Com o ambiente pronto, o c\u00f3digo pode ser utilizado. Example toolkit com Docker Para utilizar o Example toolkit sem realizar nenhuma configura\u00e7\u00e3o de ambiente, consulte a se\u00e7\u00e3o Example toolkit environment para mais explica\u00e7\u00f5es de como voc\u00ea pode utilizar a vers\u00e3o em Docker do script. Configura\u00e7\u00e3o manual do ambiente Para realizar a configura\u00e7\u00e3o manual do ambiente Python junto as depend\u00eancias necess\u00e1rias a utiliza\u00e7\u00e3o do Example toolkit , voc\u00ea pode utilizar o conda . Com esse gerenciador de pacote, voc\u00ea pode criar um novo ambiente que possui todas as depend\u00eancias exigidas pelo Example toolkit . Para criar esse ambiente, utilize o arquivo environment.yml que est\u00e1 dispon\u00edvel no diret\u00f3rio tools/example-toolkit : conda env create -f environment.yml A primeira etapa necess\u00e1ria para a execu\u00e7\u00e3o do Example toolkit \u00e9 a defini\u00e7\u00e3o das configura\u00e7\u00f5es que devem ser consideradas. Essas configura\u00e7\u00f5es, ser\u00e3o utilizadas para determinar o local onde os dados baixados ser\u00e3o salvos. S\u00e3o definidos tamb\u00e9m alguns par\u00e2metros que auxiliam o script na gera\u00e7\u00e3o do arquivo de configura\u00e7\u00e3o Dagster. A defini\u00e7\u00e3o dessas configura\u00e7\u00f5es \u00e9 feita atrav\u00e9s de vari\u00e1veis de ambiente. Ao todo, as seguintes vari\u00e1veis de ambiente devem ser declaradas: DOWNLOAD_REFERENCE_FILE Vari\u00e1vel de ambiente para determinar o caminho absoluto para o arquivo JSON que define o endere\u00e7o dos dados que ser\u00e3o baixados do GitHub Assets Release. Exemplos desse arquivo podem ser encontrados no diret\u00f3rio tools/example-toolkit/config deste RC . Exemplo de valor : /compendium/config/example-toolkit.json DOWNLOAD_OUTPUT_DIRECTORY Vari\u00e1vel de ambiente para determinar o diret\u00f3rio onde os dados baixados devem ser armazenados. Os dados ser\u00e3o organizados no formato do Diret\u00f3rio de dados , requerido pelos scripts de processamento deste RC . Exemplo de valor : /compendium/data PIPELINE_DIR (Configura\u00e7\u00e3o Dagster) Vari\u00e1vel de ambiente para determinar o diret\u00f3rio onde o arquivo de configura\u00e7\u00e3o do Dagster ser\u00e1 salvo. Exemplo de valor : /compendium/config/config.yml RAW_DATA_DIR (Configura\u00e7\u00e3o Dagster) Vari\u00e1vel de ambiente para determinar o diret\u00f3rio da m\u00e1quina que deve ser considerado o input dir do processamento Dagster. Exemplo de valor : /compendium/data/raw_data DERIVED_DATA_DIR (Configura\u00e7\u00e3o Dagster) Vari\u00e1vel de ambiente para determinar o diret\u00f3rio da m\u00e1quina que deve ser considerado o output dir no arquivo de configura\u00e7\u00e3o Dagster. Exemplo de valor : /compendium/data/derived_data Consist\u00eancia de defini\u00e7\u00e3o das vari\u00e1veis Deve-se notar que as vari\u00e1veis de configura\u00e7\u00e3o possuem uma depend\u00eancia l\u00f3gica que deve ser seguida para que nenhum problema seja gerado. Para apresentar essa depend\u00eancia, vamos considerar o exemplo abaixo: Supondo que voc\u00ea queira fazer o download dos dados no diret\u00f3rio /opt/my-data . Neste caso, voc\u00ea definir\u00e1 a vari\u00e1vel DOWNLOAD_OUTPUT_DIRECTORY da seguinte forma: DOWNLOAD_OUTPUT_DIRECTORY = /opt/my-data Sabendo que os dados ser\u00e3o organizados seguindo o padr\u00e3o de Diret\u00f3rio de dados , os dados baixados ser\u00e3o armazenados guardado da seguinte forma: /opt/my-data \u251c\u2500\u2500 derived_data \u2514\u2500\u2500 raw_data Considerando essa organiza\u00e7\u00e3o, caso voc\u00ea queira que o arquivo de configura\u00e7\u00e3o Dagster seja gerado para processar os dados no diret\u00f3rio /opt/my-data , ser\u00e1 necess\u00e1rio definir as vari\u00e1veis de ambiente associadas a configura\u00e7\u00e3o do Dagster da seguinte forma: # 1. Input dir RAW_DATA_DIR = /opt/my-data/raw_data # 2. Output dir DERIVED_DATA_DIR = /opt/my-data/derived_data Feita a defini\u00e7\u00e3o de cada uma dessas vari\u00e1veis de ambiente, o Example toolkit pode ser executado. Para isso, o script dispon\u00edvel no diret\u00f3rio tools/example-toolkit/scripts/pipeline.py deve ser executado. Considerando que voc\u00ea est\u00e1 no diret\u00f3rio raiz deste RC , a execu\u00e7\u00e3o desse script pode ser feita da seguinte forma: 1. Mudando o diret\u00f3rio cd tools/example-toolkit/ 2. Execu\u00e7\u00e3o python3 scripts/pipeline.py Ao final da execu\u00e7\u00e3o, tem-se como resultado nos diret\u00f3rios de sa\u00edda, os seguintes conte\u00fados: Dados O diret\u00f3rio definido na vari\u00e1vel DOWNLOAD_OUTPUT_DIRECTORY , como mencionado, seguir\u00e1 a organiza\u00e7\u00e3o do Diret\u00f3rio de dados exigido pelos scripts de processamento deste RC . Assim, os dados estar\u00e3o organizados da seguinte forma: DOWNLOAD_OUTPUT_DIRECTORY \u251c\u2500\u2500 derived_data \u2514\u2500\u2500 raw_data \u251c\u2500\u2500 landsat8_data \u251c\u2500\u2500 sentinel2_data \u251c\u2500\u2500 scene_id_list \u2514\u2500\u2500 lasrc_auxiliary_data Configura\u00e7\u00e3o do Dagster O diret\u00f3rio definido na vari\u00e1vel PIPELINE_DIR ser\u00e1 populado com um arquivo de configura\u00e7\u00e3o Dagster nomeado config.yaml . Neste arquivo, o seguinte conte\u00fado estar\u00e1 dispon\u00edvel: config.yaml: Arquivo de configura\u00e7\u00e3o Dagster resources : lasrc_data : config : lasrc_auxiliary_data_dir : { RAW_DATA_DIR } /lasrc_auxiliary_data repository : config : derived_data_dir : { DERIVED_DATA_DIR } landsat8_input_dir : { RAW_DATA_DIR } /landsat8_data sentinel2_input_dir : { RAW_DATA_DIR } /sentinel2_data solids : load_and_standardize_sceneids_input : config : landsat8_sceneid_list : { RAW_DATA_DIR } /scene_id_list/l8-sceneids.txt sentinel2_sceneid_list : { RAW_DATA_DIR } /scene_id_list/s2-sceneids.txt Para um exemplo de uso completo e funcional do Example toolkit , consulte a se\u00e7\u00e3o Pesquisa reprodut\u00edvel - Exemplo m\u00ednimo .","title":"Scripts auxiliares"},{"location":"tools/utilitary/#scripts-auxiliares","text":"A constru\u00e7\u00e3o e publica\u00e7\u00e3o deste RC foi realizada em diversas etapas. Em cada etapa, al\u00e9m dos c\u00f3digos que fazem a gera\u00e7\u00e3o dos resultados propriamente ditos, foram desenvolvidos tamb\u00e9m diversos scripts auxiliares. Nesta se\u00e7\u00e3o, faz-se a apresenta\u00e7\u00e3o desses scripts , seus detalhes de uso e forma de configura\u00e7\u00e3o.","title":"Scripts auxiliares"},{"location":"tools/utilitary/#calculate-checksum-e-github-asset-upload","text":"Para o compartilhamento deste RC , foi adotada uma estrat\u00e9gia em que todos os materiais que formam a vers\u00e3o completa do RC , foram organizados em um reposit\u00f3rio do GitHub. Nesse reposit\u00f3rio, tem-se todo o hist\u00f3rico de modifica\u00e7\u00f5es realizadas nos materiais, c\u00f3digos, documenta\u00e7\u00e3o e os dados. O armazenamento de materiais como os scripts e a documenta\u00e7\u00e3o n\u00e3o requerem muito espa\u00e7o em disco. Com isso, o uso do GitHub foi feito diretamente, sem a necessidade de modifica\u00e7\u00f5es. No entanto, os dados auxiliares, que s\u00e3o utilizadas nos exemplos de execu\u00e7\u00e3o e replica\u00e7\u00e3o dispon\u00edveis neste RC , possuem tamanhos que os tornam invi\u00e1veis para versionamento em reposit\u00f3rios git comum. Como alternativa, adotou-se a abordagem da publica\u00e7\u00e3o desses dados atrav\u00e9s dos Release Assets . Com essa alternativa, arquivos com at\u00e9 2 GB podem ser publicados e mantidos associados no reposit\u00f3rio. Com isso, fez-se necess\u00e1rio a prepara\u00e7\u00e3o e organiza\u00e7\u00e3o dos dados, de modo que eles pudessem ser compartilhados no GitHub Release Assets. Para essa prepara\u00e7\u00e3o, fez-se a cria\u00e7\u00e3o de dois scripts auxiliares: Calculate Checksum Calculate Checksum \u00e9 um script Python que realiza a cria\u00e7\u00e3o de BagIt e faz seu armazenamento em arquivos zip . GitHub Asset Upload Uma vez que os BagIt s\u00e3o criados, o script R GitHub Asset Upload faz o envio desses arquivos para os servidores do GitHub. O processo \u00e9 feito com o aux\u00edlio do pacote piggyback . Tendo como base esses dois pacotes, no reposit\u00f3rio onde foram armazenados os scripts e a documenta\u00e7\u00e3o, tamb\u00e9m pode-se disponibilizar os dados.","title":"Calculate Checksum e GitHub Asset Upload"},{"location":"tools/utilitary/#example-toolkit","text":"Conforme apresentado na se\u00e7\u00e3o anterior, os dados disponibilizados junto a este RC , foram publicados utilizando o GitHub Release Assets . Para isso, s\u00e3o criados BagIt dos dados em arquivos no formato zip . Esses, por sua vez, s\u00e3o armazenados nos servidores do GitHub. Com essa estrat\u00e9gia, todos tem acesso aos dados e podem fazer seu uso para executar os exemplos disponibilizados neste RC . No entanto, para que os exemplos dispon\u00edveis neste RC possam ser utilizados, \u00e9 necess\u00e1rio que os dados estejam organizados nos diret\u00f3rios corretos do RC . Para resolver esse problema, e evitar o download e organiza\u00e7\u00e3o manual desses dados na estrutura do RC , fez-se a cria\u00e7\u00e3o de um script Python, o Example toolkit . Com essa ferramenta, toda as etapas de download e organiza\u00e7\u00e3o s\u00e3o completamente automatizadas, sendo necess\u00e1rio para o usu\u00e1rio, apenas definir os diret\u00f3rios que devem ser considerados no momento do download .","title":"Example toolkit"},{"location":"tools/utilitary/#operacao","text":"Ao realizar a utiliza\u00e7\u00e3o do Example toolkit , o script far\u00e1 a execu\u00e7\u00e3o de quatro etapas principais. Cada uma dessas etapas s\u00e3o representadas na figura abaixo: Fluxo de opera\u00e7\u00e3o do Example Toolkit Conforme pode ser visto na figura acima, inicialmente, o Example toolkit realiza o download dos dados associados ao RC que est\u00e3o armazenados no GitHub Release Assets . Em seguida, para garantir que os dados baixados n\u00e3o sofreram mudan\u00e7as ou est\u00e3o corrompidos, \u00e9 feita a valida\u00e7\u00e3o do BagIts baixados, estrutura na qual os arquivos est\u00e3o salvos. Ap\u00f3s a valida\u00e7\u00e3o, a ferramenta faz a extra\u00e7\u00e3o dos dados para os diret\u00f3rios corretos. Por fim, com base nas defini\u00e7\u00f5es do usu\u00e1rio e onde os dados foram salvos, o Example toolkit faz a gera\u00e7\u00e3o de um arquivo de configura\u00e7\u00e3o Dagster que pode ser utilizado para iniciar o processamento dos dados baixados. Dagster e arquivo de configura\u00e7\u00e3o Caso voc\u00ea deseje saber mais sobre como o Dagster \u00e9 utilizado neste RC e onde o arquivo de configura\u00e7\u00e3o deve ser utilizado, consulte a Se\u00e7\u00e3o Scripts de processamento - Dagster . Na pr\u00f3xima se\u00e7\u00e3o, \u00e9 feita a apresenta\u00e7\u00e3o de como o Example toolkit \u00e9 configurado e executado.","title":"Opera\u00e7\u00e3o"},{"location":"tools/utilitary/#utilizacao","text":"Por se tratar de um script Python, a utiliza\u00e7\u00e3o do Example toolkit , requer apenas que voc\u00ea tenha o ambiente Python com as devidas depend\u00eancias configuradas. Com o ambiente pronto, o c\u00f3digo pode ser utilizado. Example toolkit com Docker Para utilizar o Example toolkit sem realizar nenhuma configura\u00e7\u00e3o de ambiente, consulte a se\u00e7\u00e3o Example toolkit environment para mais explica\u00e7\u00f5es de como voc\u00ea pode utilizar a vers\u00e3o em Docker do script. Configura\u00e7\u00e3o manual do ambiente Para realizar a configura\u00e7\u00e3o manual do ambiente Python junto as depend\u00eancias necess\u00e1rias a utiliza\u00e7\u00e3o do Example toolkit , voc\u00ea pode utilizar o conda . Com esse gerenciador de pacote, voc\u00ea pode criar um novo ambiente que possui todas as depend\u00eancias exigidas pelo Example toolkit . Para criar esse ambiente, utilize o arquivo environment.yml que est\u00e1 dispon\u00edvel no diret\u00f3rio tools/example-toolkit : conda env create -f environment.yml A primeira etapa necess\u00e1ria para a execu\u00e7\u00e3o do Example toolkit \u00e9 a defini\u00e7\u00e3o das configura\u00e7\u00f5es que devem ser consideradas. Essas configura\u00e7\u00f5es, ser\u00e3o utilizadas para determinar o local onde os dados baixados ser\u00e3o salvos. S\u00e3o definidos tamb\u00e9m alguns par\u00e2metros que auxiliam o script na gera\u00e7\u00e3o do arquivo de configura\u00e7\u00e3o Dagster. A defini\u00e7\u00e3o dessas configura\u00e7\u00f5es \u00e9 feita atrav\u00e9s de vari\u00e1veis de ambiente. Ao todo, as seguintes vari\u00e1veis de ambiente devem ser declaradas: DOWNLOAD_REFERENCE_FILE Vari\u00e1vel de ambiente para determinar o caminho absoluto para o arquivo JSON que define o endere\u00e7o dos dados que ser\u00e3o baixados do GitHub Assets Release. Exemplos desse arquivo podem ser encontrados no diret\u00f3rio tools/example-toolkit/config deste RC . Exemplo de valor : /compendium/config/example-toolkit.json DOWNLOAD_OUTPUT_DIRECTORY Vari\u00e1vel de ambiente para determinar o diret\u00f3rio onde os dados baixados devem ser armazenados. Os dados ser\u00e3o organizados no formato do Diret\u00f3rio de dados , requerido pelos scripts de processamento deste RC . Exemplo de valor : /compendium/data PIPELINE_DIR (Configura\u00e7\u00e3o Dagster) Vari\u00e1vel de ambiente para determinar o diret\u00f3rio onde o arquivo de configura\u00e7\u00e3o do Dagster ser\u00e1 salvo. Exemplo de valor : /compendium/config/config.yml RAW_DATA_DIR (Configura\u00e7\u00e3o Dagster) Vari\u00e1vel de ambiente para determinar o diret\u00f3rio da m\u00e1quina que deve ser considerado o input dir do processamento Dagster. Exemplo de valor : /compendium/data/raw_data DERIVED_DATA_DIR (Configura\u00e7\u00e3o Dagster) Vari\u00e1vel de ambiente para determinar o diret\u00f3rio da m\u00e1quina que deve ser considerado o output dir no arquivo de configura\u00e7\u00e3o Dagster. Exemplo de valor : /compendium/data/derived_data Consist\u00eancia de defini\u00e7\u00e3o das vari\u00e1veis Deve-se notar que as vari\u00e1veis de configura\u00e7\u00e3o possuem uma depend\u00eancia l\u00f3gica que deve ser seguida para que nenhum problema seja gerado. Para apresentar essa depend\u00eancia, vamos considerar o exemplo abaixo: Supondo que voc\u00ea queira fazer o download dos dados no diret\u00f3rio /opt/my-data . Neste caso, voc\u00ea definir\u00e1 a vari\u00e1vel DOWNLOAD_OUTPUT_DIRECTORY da seguinte forma: DOWNLOAD_OUTPUT_DIRECTORY = /opt/my-data Sabendo que os dados ser\u00e3o organizados seguindo o padr\u00e3o de Diret\u00f3rio de dados , os dados baixados ser\u00e3o armazenados guardado da seguinte forma: /opt/my-data \u251c\u2500\u2500 derived_data \u2514\u2500\u2500 raw_data Considerando essa organiza\u00e7\u00e3o, caso voc\u00ea queira que o arquivo de configura\u00e7\u00e3o Dagster seja gerado para processar os dados no diret\u00f3rio /opt/my-data , ser\u00e1 necess\u00e1rio definir as vari\u00e1veis de ambiente associadas a configura\u00e7\u00e3o do Dagster da seguinte forma: # 1. Input dir RAW_DATA_DIR = /opt/my-data/raw_data # 2. Output dir DERIVED_DATA_DIR = /opt/my-data/derived_data Feita a defini\u00e7\u00e3o de cada uma dessas vari\u00e1veis de ambiente, o Example toolkit pode ser executado. Para isso, o script dispon\u00edvel no diret\u00f3rio tools/example-toolkit/scripts/pipeline.py deve ser executado. Considerando que voc\u00ea est\u00e1 no diret\u00f3rio raiz deste RC , a execu\u00e7\u00e3o desse script pode ser feita da seguinte forma: 1. Mudando o diret\u00f3rio cd tools/example-toolkit/ 2. Execu\u00e7\u00e3o python3 scripts/pipeline.py Ao final da execu\u00e7\u00e3o, tem-se como resultado nos diret\u00f3rios de sa\u00edda, os seguintes conte\u00fados: Dados O diret\u00f3rio definido na vari\u00e1vel DOWNLOAD_OUTPUT_DIRECTORY , como mencionado, seguir\u00e1 a organiza\u00e7\u00e3o do Diret\u00f3rio de dados exigido pelos scripts de processamento deste RC . Assim, os dados estar\u00e3o organizados da seguinte forma: DOWNLOAD_OUTPUT_DIRECTORY \u251c\u2500\u2500 derived_data \u2514\u2500\u2500 raw_data \u251c\u2500\u2500 landsat8_data \u251c\u2500\u2500 sentinel2_data \u251c\u2500\u2500 scene_id_list \u2514\u2500\u2500 lasrc_auxiliary_data Configura\u00e7\u00e3o do Dagster O diret\u00f3rio definido na vari\u00e1vel PIPELINE_DIR ser\u00e1 populado com um arquivo de configura\u00e7\u00e3o Dagster nomeado config.yaml . Neste arquivo, o seguinte conte\u00fado estar\u00e1 dispon\u00edvel: config.yaml: Arquivo de configura\u00e7\u00e3o Dagster resources : lasrc_data : config : lasrc_auxiliary_data_dir : { RAW_DATA_DIR } /lasrc_auxiliary_data repository : config : derived_data_dir : { DERIVED_DATA_DIR } landsat8_input_dir : { RAW_DATA_DIR } /landsat8_data sentinel2_input_dir : { RAW_DATA_DIR } /sentinel2_data solids : load_and_standardize_sceneids_input : config : landsat8_sceneid_list : { RAW_DATA_DIR } /scene_id_list/l8-sceneids.txt sentinel2_sceneid_list : { RAW_DATA_DIR } /scene_id_list/s2-sceneids.txt Para um exemplo de uso completo e funcional do Example toolkit , consulte a se\u00e7\u00e3o Pesquisa reprodut\u00edvel - Exemplo m\u00ednimo .","title":"Utiliza\u00e7\u00e3o"},{"location":"en/","text":"Evaluating Landsat-8 and Sentinel-2 Nadir BRDF Adjusted Reflectance (NBAR) on South of Brazil through a Reproducible and Replicable Workflow \u00b6 This is the official Research Compendium ( RC ) documentation, with all the materials (Codes, data, and computing environments) needed for the reproduction, replication, and evaluation of the results presented in the paper: Marujo et al (2022). Evaluating Landsat-8 and Sentinel-2 Nadir BRDF Adjusted Reflectance (NBAR) on South of Brazil through a Reproducible and Replicable Workflow . This Paper will be submitted in June 2022. Research Compendium Content \u00b6 The organization defined for this RC , aims to facilitate the use of the codes implemented to generate the results presented in the article. The processing codes are made available in a structure of examples that allow the execution without difficulties, making it possible for others to reproduce and replicate the study performed. These codes are stored in the analysis directory, which has three subdirectories: :file_folder: analysis/notebook : Directory with the Jupyter Notebook version of the processing flow implemented in the article associated with this RC . For more information, see the Reference Section Processing Scripts ; :file_folder: analysis/pipeline : Directory with the Dagster version of the processing flow implemented in the article associated with this RC . For more information, see the Reference Section Processing Scripts ; :file_folder: analysis/data : Directory for storing the generated input and output data. It contains the following subdirectories: :file_folder: examples : Directory with the data (Input/Output) of the examples provided in this RC . For more information about the examples, see Chapter Data Processing ; :file_folder: original_scene_ids : Directory for storing the original scene id index files used to produce the article results. This data can be applied to the codes provided in the analysis/notebook and analysis/pipeline directories for reproducing the article results. By default, the input data, because of the size of the files, is not stored directly in the data directory ( analysis/data/ ). Instead, as described in detail in the Reference Section Helper scripts , they are made available in the GitHub Release Assets of the RC repository. To build the processing scripts available in the analysis directory, we have created several software libraries and scripts auxiliary . The source code for some of these tools is available in the tools directory. In this directory there are four subdirectories, namely: :file_folder: tools/auxiliary-library : Source code for the research-processing library , which provides the high-level operations for processing the data in this RC ; :file_folder: tools/calculate-checksum : Source code of script calculate-checksum , created to calculate the checksum of the files in this RC before sharing; :file_folder: tools/example-toolkit : Source code of the script example-toolkit , created to facilitate the download and validation of example data from the GitHub Release Assets; :file_folder: tools/github-asset-upload : Source code of the script github-asset-upload , created to facilitate the upload of example data to the GitHub Release Assets. Another directory available in this RC is composes . In this directory are Docker Compose configuration files for the computing environments needed to run the examples available in this RC . For more information about the RC computing environments, see the Reference Section Computing Environments . In the composes directory, there are two subdirectories: :file_folder: composes/minimal : Directory with the Docker Composes to run the Minimal example provided in this RC ; :file_folder: composes/replication : Directory with the Docker Composes to run the Replication example provided in this RC . For more information about the examples, see Section Data Processing . Complementary to the composes directory is the docker directory. This directory holds the Dockerfile files used to build the environments used in Docker Composes. This directory has two subdirectories: :file_folder: docker/notebook : Directory with the Dockerfile of the environment required for running the Jupyter Notebook version of this RC process stream. :file_folder: docker/pipeline : Directory with the Dockerfile of the environment needed for running the Dagster version of this RC process stream. In addition to these directories, some files are fundamental to using the materials in this RC : Vagrantfile and bootstrap.sh : Vagrant files used to build a virtual machine with the complete environment for running the Processing scripts available in the analysis directory. For more information, see the reference section Computing Environments - Virtual Machine with Vagrant ; Makefile : GNU Make definition file to make the use of the materials available in the analysis and composes directories easier. The setenv.sh file is used by Makefile to define the user who will run Jupyter Notebook environment. More information is provided in Section Data Processing .","title":"Home"},{"location":"en/#evaluating-landsat-8-and-sentinel-2-nadir-brdf-adjusted-reflectance-nbar-on-south-of-brazil-through-a-reproducible-and-replicable-workflow","text":"This is the official Research Compendium ( RC ) documentation, with all the materials (Codes, data, and computing environments) needed for the reproduction, replication, and evaluation of the results presented in the paper: Marujo et al (2022). Evaluating Landsat-8 and Sentinel-2 Nadir BRDF Adjusted Reflectance (NBAR) on South of Brazil through a Reproducible and Replicable Workflow . This Paper will be submitted in June 2022.","title":"Evaluating Landsat-8 and Sentinel-2 Nadir BRDF Adjusted Reflectance (NBAR) on South of Brazil through a Reproducible and Replicable Workflow"},{"location":"en/#research-compendium-content","text":"The organization defined for this RC , aims to facilitate the use of the codes implemented to generate the results presented in the article. The processing codes are made available in a structure of examples that allow the execution without difficulties, making it possible for others to reproduce and replicate the study performed. These codes are stored in the analysis directory, which has three subdirectories: :file_folder: analysis/notebook : Directory with the Jupyter Notebook version of the processing flow implemented in the article associated with this RC . For more information, see the Reference Section Processing Scripts ; :file_folder: analysis/pipeline : Directory with the Dagster version of the processing flow implemented in the article associated with this RC . For more information, see the Reference Section Processing Scripts ; :file_folder: analysis/data : Directory for storing the generated input and output data. It contains the following subdirectories: :file_folder: examples : Directory with the data (Input/Output) of the examples provided in this RC . For more information about the examples, see Chapter Data Processing ; :file_folder: original_scene_ids : Directory for storing the original scene id index files used to produce the article results. This data can be applied to the codes provided in the analysis/notebook and analysis/pipeline directories for reproducing the article results. By default, the input data, because of the size of the files, is not stored directly in the data directory ( analysis/data/ ). Instead, as described in detail in the Reference Section Helper scripts , they are made available in the GitHub Release Assets of the RC repository. To build the processing scripts available in the analysis directory, we have created several software libraries and scripts auxiliary . The source code for some of these tools is available in the tools directory. In this directory there are four subdirectories, namely: :file_folder: tools/auxiliary-library : Source code for the research-processing library , which provides the high-level operations for processing the data in this RC ; :file_folder: tools/calculate-checksum : Source code of script calculate-checksum , created to calculate the checksum of the files in this RC before sharing; :file_folder: tools/example-toolkit : Source code of the script example-toolkit , created to facilitate the download and validation of example data from the GitHub Release Assets; :file_folder: tools/github-asset-upload : Source code of the script github-asset-upload , created to facilitate the upload of example data to the GitHub Release Assets. Another directory available in this RC is composes . In this directory are Docker Compose configuration files for the computing environments needed to run the examples available in this RC . For more information about the RC computing environments, see the Reference Section Computing Environments . In the composes directory, there are two subdirectories: :file_folder: composes/minimal : Directory with the Docker Composes to run the Minimal example provided in this RC ; :file_folder: composes/replication : Directory with the Docker Composes to run the Replication example provided in this RC . For more information about the examples, see Section Data Processing . Complementary to the composes directory is the docker directory. This directory holds the Dockerfile files used to build the environments used in Docker Composes. This directory has two subdirectories: :file_folder: docker/notebook : Directory with the Dockerfile of the environment required for running the Jupyter Notebook version of this RC process stream. :file_folder: docker/pipeline : Directory with the Dockerfile of the environment needed for running the Dagster version of this RC process stream. In addition to these directories, some files are fundamental to using the materials in this RC : Vagrantfile and bootstrap.sh : Vagrant files used to build a virtual machine with the complete environment for running the Processing scripts available in the analysis directory. For more information, see the reference section Computing Environments - Virtual Machine with Vagrant ; Makefile : GNU Make definition file to make the use of the materials available in the analysis and composes directories easier. The setenv.sh file is used by Makefile to define the user who will run Jupyter Notebook environment. More information is provided in Section Data Processing .","title":"Research Compendium Content"},{"location":"en/reproducible-research/","text":"Data Processing \u00b6 Info This Chapter describes how to apply the available material of this RC . If you are intereseted in the concepts behind the materials, please, consult the Reference Chapter Refer\u00eancia do Compendium . To produce the results of this RC , a large Earth Observation data colection was used ( ~2TB ). With such volume, manage and process these data can require a lot of resources from researchers, which isn't always possible or available. Due to that, verify , validate , reproduce and replicate the materials can be difficult to perform. To solve this problem and allow everyone to use the material we develop, understanding its implementation, organization and and used techniques, we created use examples. In these examples, all the RC materials are used alongside to build a processing chain. The data used in these examples are, subsets of the original set of data used to produce the article results. We also applied these examples on extra data, not presented on the article, to allow the possibility of replicating the materials of this RC . The available examples are generic and can be used as a base to reproduce the article, as well as replicate it, only changing the input data. Generic and customizable examples These characteristics were associated to the available examples, since during the development of the article, they were the first components to be developed. Once with the tools finished and tested, the results were generated. To generate the final results, the only required action was to change the input data from the examples to the complete set. With this we can say that the examples are generic and customizable to allow others datasets and regions to be processed. Example flux \u00b6 Two examples are available on this RC : Minimal example Complete processing example, with all the article methodology steps. For this example, a small subset of data is used, from the data used in the article. Replication example Following the same steps of the Minimal example , in this example, a region different from the one used in the article is used to show the possibility of replication of the article processing chain. Both examples consists in the same proessing steps. These, as can be seen in the Figure beloow are divided in three parts: Data download ; Processing (Jupyter) ; Processing (Dagster) Example Flux In Data download , download of the required data to run the examples is performed. The data is made available through the GitHub Release Assets . After that, the available data can be used as input to the processing flux, which is implemented in two different technologies , Jupyter Notebook and Dagster . Both implementation results are the same. The difference only consists in the environment the tools are make available. With the Jupyter Notebook, one can interactively execute the processing steps, while with Dagster it is executed in batch . The execution of these steps is performed through Docker Composes. Each step has its own Docker Compose, so each step can be executed independently and isolated, being the data the only shared resource between them. Automation \u00b6 All the logic behind the configuration of the Docker Composes was inserted in a Makefile , which contains all the commands to execute it through GNU Make . Make and reproducible research The idea of using Make came from the magnific The Turing Way handbook to reproducible, ethical and collaborative data science . For more details on GNU Make and reproducible search, consult Reproducibility with Make . When using the GNU Make , as can be seen in the Figure bellow, the Docker Compose interaction and possible configurations are performed by ready and tested code, avoiding several errors. Besides that, since it is a simple text document, the ones more interested in details, can open and verify the file. Example flux with Make In the examples the GNU Make will be used alongside the Makefile to automatically make the configurations, to make the use of the materials simplier and more direct. Makefile available commands \u00b6 To facilitate the use of the Makefile commands, this subsectiion has a reference of each available command. These commands are divided in two groups: Example and Replication . The Example commands facilitates the Minimal example operations, while Replication commands facilitates replication. Note that, both commands execute the same environments being the only change the input directory. So, if you wish to adapt the codes for your data, these commands can also be used. Alternatively, the Docker Compose used by the Makefile are also available and can be modified. The following topics show the presented commands for each of these groups: Example Command Description example_cleanup_data Removes all data (Input and output) used in the minimal example example_download_data Downloads the data used by the minimal example example_pipeline Creates the Container to execute the minimal example through Dagster example_notebook Creates the Container to execute the minimal example through Jupyter Notebook Replication Command Description replication_cleanup_data Removes all data (Input and output) used in the replication replication_ download_data Downloads the data used by the replication replication_pipeline Creates the Container to execute the replication example through Dagster replication_notebook Creates the Container to execute the replication example through Jupyter Notebook As can be seen, the commands for both examples are the same, changing only the command call. As for their functionality, it is the same only changing the input data. requirements \u00b6 Before starting the examples, ensure that you have the required tools configured in your work environment. Bellow, these tools are listed and described: Git Version control system (Documentation created with git version 2.25.1 . Posterior versions should support the used commands); Docker Virtualization software based on Containers (Documentation created using version 0.10.12 . Posterior versions should support the used commands); Docker Compose Orchestrator and manager tool for Docker Containers (Documentation created with Docker Compose vers\u00e3o 1.29.2 . Posterior versions should support the used commands)). GNU Make Automatization and tool to control build flux and execution (Documentation created with GNU Make vers\u00e3o 4.2.1 . Posterior versions should support the used commands)). Use the links above to access the official documentation of each tool and install them (case you don't have it installed). Operational System The operational system used to create this RC materias was Ubuntu 20.04. It is expected that the presented steps work on similar or equivalent distributions (e.g., MacOS , FreeBSD , openSUSE ). Regarding Windows, adaptations may be required. If you use Windows and does not want to modify this RC , we also provided a virtual machine that can be used. For more information, please consult the Section Computational Environments - Vagrant Virtual Machine . After installing and configuring all the listed tools above, you are ready to start the examples.","title":"Introduction"},{"location":"en/reproducible-research/#data-processing","text":"Info This Chapter describes how to apply the available material of this RC . If you are intereseted in the concepts behind the materials, please, consult the Reference Chapter Refer\u00eancia do Compendium . To produce the results of this RC , a large Earth Observation data colection was used ( ~2TB ). With such volume, manage and process these data can require a lot of resources from researchers, which isn't always possible or available. Due to that, verify , validate , reproduce and replicate the materials can be difficult to perform. To solve this problem and allow everyone to use the material we develop, understanding its implementation, organization and and used techniques, we created use examples. In these examples, all the RC materials are used alongside to build a processing chain. The data used in these examples are, subsets of the original set of data used to produce the article results. We also applied these examples on extra data, not presented on the article, to allow the possibility of replicating the materials of this RC . The available examples are generic and can be used as a base to reproduce the article, as well as replicate it, only changing the input data. Generic and customizable examples These characteristics were associated to the available examples, since during the development of the article, they were the first components to be developed. Once with the tools finished and tested, the results were generated. To generate the final results, the only required action was to change the input data from the examples to the complete set. With this we can say that the examples are generic and customizable to allow others datasets and regions to be processed.","title":"Data Processing"},{"location":"en/reproducible-research/#example-flux","text":"Two examples are available on this RC : Minimal example Complete processing example, with all the article methodology steps. For this example, a small subset of data is used, from the data used in the article. Replication example Following the same steps of the Minimal example , in this example, a region different from the one used in the article is used to show the possibility of replication of the article processing chain. Both examples consists in the same proessing steps. These, as can be seen in the Figure beloow are divided in three parts: Data download ; Processing (Jupyter) ; Processing (Dagster) Example Flux In Data download , download of the required data to run the examples is performed. The data is made available through the GitHub Release Assets . After that, the available data can be used as input to the processing flux, which is implemented in two different technologies , Jupyter Notebook and Dagster . Both implementation results are the same. The difference only consists in the environment the tools are make available. With the Jupyter Notebook, one can interactively execute the processing steps, while with Dagster it is executed in batch . The execution of these steps is performed through Docker Composes. Each step has its own Docker Compose, so each step can be executed independently and isolated, being the data the only shared resource between them.","title":"Example flux"},{"location":"en/reproducible-research/#automation","text":"All the logic behind the configuration of the Docker Composes was inserted in a Makefile , which contains all the commands to execute it through GNU Make . Make and reproducible research The idea of using Make came from the magnific The Turing Way handbook to reproducible, ethical and collaborative data science . For more details on GNU Make and reproducible search, consult Reproducibility with Make . When using the GNU Make , as can be seen in the Figure bellow, the Docker Compose interaction and possible configurations are performed by ready and tested code, avoiding several errors. Besides that, since it is a simple text document, the ones more interested in details, can open and verify the file. Example flux with Make In the examples the GNU Make will be used alongside the Makefile to automatically make the configurations, to make the use of the materials simplier and more direct.","title":"Automation"},{"location":"en/reproducible-research/#makefile-available-commands","text":"To facilitate the use of the Makefile commands, this subsectiion has a reference of each available command. These commands are divided in two groups: Example and Replication . The Example commands facilitates the Minimal example operations, while Replication commands facilitates replication. Note that, both commands execute the same environments being the only change the input directory. So, if you wish to adapt the codes for your data, these commands can also be used. Alternatively, the Docker Compose used by the Makefile are also available and can be modified. The following topics show the presented commands for each of these groups: Example Command Description example_cleanup_data Removes all data (Input and output) used in the minimal example example_download_data Downloads the data used by the minimal example example_pipeline Creates the Container to execute the minimal example through Dagster example_notebook Creates the Container to execute the minimal example through Jupyter Notebook Replication Command Description replication_cleanup_data Removes all data (Input and output) used in the replication replication_ download_data Downloads the data used by the replication replication_pipeline Creates the Container to execute the replication example through Dagster replication_notebook Creates the Container to execute the replication example through Jupyter Notebook As can be seen, the commands for both examples are the same, changing only the command call. As for their functionality, it is the same only changing the input data.","title":"Makefile available commands"},{"location":"en/reproducible-research/#requirements","text":"Before starting the examples, ensure that you have the required tools configured in your work environment. Bellow, these tools are listed and described: Git Version control system (Documentation created with git version 2.25.1 . Posterior versions should support the used commands); Docker Virtualization software based on Containers (Documentation created using version 0.10.12 . Posterior versions should support the used commands); Docker Compose Orchestrator and manager tool for Docker Containers (Documentation created with Docker Compose vers\u00e3o 1.29.2 . Posterior versions should support the used commands)). GNU Make Automatization and tool to control build flux and execution (Documentation created with GNU Make vers\u00e3o 4.2.1 . Posterior versions should support the used commands)). Use the links above to access the official documentation of each tool and install them (case you don't have it installed). Operational System The operational system used to create this RC materias was Ubuntu 20.04. It is expected that the presented steps work on similar or equivalent distributions (e.g., MacOS , FreeBSD , openSUSE ). Regarding Windows, adaptations may be required. If you use Windows and does not want to modify this RC , we also provided a virtual machine that can be used. For more information, please consult the Section Computational Environments - Vagrant Virtual Machine . After installing and configuring all the listed tools above, you are ready to start the examples.","title":"requirements"},{"location":"en/reproducible-research/minimal-example/","text":"Minimal example \u00b6 Requirements Before starting this example, certify that the requirements are installed in your work environment. Alongside this RC , there is an article in which the harmonization experiments were conducted using Landsat-8/OLI and Sentinel-2/MSI images. As presented in the Reference Chapter , the materials and tools of this RC , represent the effort of generating the mentioned article results. In this section, these materials and a reproducible example are presented, going through all the processing chain used in the article to generate the harmonized products. This minimal example consists in a small subset containing 4 scenes ( 2x Landsat-8/OLI and 2x Sentinel-2/MSI ) extracted from the original dataset. With this minimal example it is expected to allow researchers to explore the produced material and the chain implementation of the article. Reference This is a practical section, in which the RC materials are used. If you need more info regarding the used tools, please consult the Reference Chapter . Research Compendium Download \u00b6 This example first step is to download this RC and all its materials. To do this, in a terminal, use git to clone : git clone https://github.com/brazil-data-cube/compendium-harmonization After the clone , a new directory will be created in your current directory. Its name is compendium-harmonization : ls -ls . #> 4 drwxrwxr-x 3 ubuntu ubuntu 4096 May 2 00:44 compendium-harmonization Now, access the directory compendium-harmonization and list its content: Changing directory cd compendium-harmonization Listing its content ls -ls . #> total 76K #> drwxrwxr-x 9 ubuntu ubuntu 4.0K May 1 23:29 . #> drwxrwxr-x 4 ubuntu ubuntu 4.0K May 2 00:44 .. #> drwxrwxr-x 5 ubuntu ubuntu 4.0K Apr 14 17:00 analysis #> -rw-rw-r-- 1 ubuntu ubuntu 1.4K May 1 16:36 bootstrap.sh #> drwxrwxr-x 4 ubuntu ubuntu 4.0K Apr 14 17:00 composes #> drwxrwxr-x 4 ubuntu ubuntu 4.0K Apr 14 17:00 docker #> -rw-rw-r-- 1 ubuntu ubuntu 375 May 1 16:36 .dockerignore #> drwxrwxr-x 3 ubuntu ubuntu 4.0K May 1 16:44 docs #> drwxrwxr-x 7 ubuntu ubuntu 4.0K Apr 14 17:00 .git #> drwxrwxr-x 3 ubuntu ubuntu 4.0K Apr 15 22:53 .github #> -rw-rw-r-- 1 ubuntu ubuntu 4.6K May 1 16:36 .gitignore #> -rw-rw-r-- 1 ubuntu ubuntu 1.1K May 1 16:35 LICENSE #> -rw-rw-r-- 1 ubuntu ubuntu 2.7K May 1 16:36 Makefile #> -rw-rw-r-- 1 ubuntu ubuntu 4.5K Apr 9 20:01 README.md #> -rw-rw-r-- 1 ubuntu ubuntu 392 May 1 16:36 setenv.sh #> drwxrwxr-x 6 ubuntu ubuntu 4.0K Apr 14 17:00 tools #> -rw-rw-r-- 1 ubuntu ubuntu 3.4K May 1 16:36 Vagrantfile As you can see, the content of this directory are the materials of this RC . This will be the base files used in this tutorial. The description of each directory of this RC can be found in the Introduction of this documentation. Download the data \u00b6 After downloading the RC , one can follow the steps presented in the introduction of the ( Data Processing ) chapter to download the files that will be used in this example. These data are described in the Example Toolkit section and are stored in the GitHub Release Assets . It will be required to use the Example Toolkit to obtain them. This tool downloads and organizes the data in the structure required by the processing scripts . To execute it, first create a Docker network, in which the Containers created in this tutorial will be associated to. To do that, in your terminal, use the following command: docker network create research_processing_network #> fdaa46b4fe70bd34b6cb0e59734376234d801599a1fb1cbe1d9fd66a8f5044b1 Now, through your GNU Make , execute the following command example_download_data : make example_download_data Pro tip During the GNU Make execution, if you have any problema regarding permissions on the execution of the setenv.sh file, use the following command before executing the GNU Make again: chmod +x setenv.sh This command will use a Docker Compose to download the example data. After executing, the data download will start and a message similar to the one bellow will be presented (A few fields are omitted here so the documentation can be more readable): Creating example-minimal-download-data ... done Attaching to example-minimal-download-data ( omitted ) | 2022 -05-02 01 :16:20.078 | INFO | ( omitted ) - Downloading minimal-example_landsat8_data.zip ( omitted ) ( omitted ) | 2022 -05-02 01 :21:09.345 | INFO | ( omitted ) - Downloading minimal-example_lasrc_auxiliary_data.zip ( omitted ) ( omitted ) | 2022 -05-02 01 :22:35.845 | INFO | ( omitted ) - Downloading minimal-example_scene_id_list.zip ( omitted ) ( omitted ) | 2022 -05-02 01 :22:36.510 | INFO | ( omitted ) - Downloading minimal-example_sentinel2_data.zip ( omitted ) ( omitted ) | 2022 -05-02 01 :25:14.653 | INFO | ( omitted ) - All files are downloaded. example-minimal-download-data exited with code 0 Data download If you are interested in obtaining more information regarding the data download , please, consult the reference section Auxiliary scripts . Following the RC organization , the downloaded data were stored in the directory analysis/data/examples/minimal_example/raw_data/ : ls -ls analysis/data/examples/minimal_example/raw_data/ #> total 16 #> 4 drwxrwxrwx 4 root root 4096 May 2 01:25 landsat8_data #> 4 drwxrwxrwx 5 root root 4096 May 2 01:22 lasrc_auxiliary_data #> 4 drwxrwxrwx 2 root root 4096 May 2 01:22 scene_id_list #> 4 drwxrwxrwx 4 root root 4096 May 2 01:25 sentinel2_data Data organization The data organization in the directory analysis/data/examples/minimal_example/raw_data/ follow the processing scripts required pattern. For more information, please, consult the reference section Data Directory . Processing data through Jupyter Notebook \u00b6 Continuing the processing chain presented in the Chapter Indroduction , a first way to execute the processings described in the article associated with this RC , is through a Jupyter Notebook. This document has the detailed description of each step of the processing chain. In this example, the notebooks will be the first approach. Jupyter Notebook everywhere For more information regarding this RC Jupyter Notebooks, consult the section Processing scripts - Jupyter Notebook . To use the notebooks and process the data, you can use the command example_notebook through the GNU Make . This command will configure the Container in order to you execute the Jupyter Notebook through an interface Jupyter Lab : make example_notebook #> (omitted) | [C 2022-05-02 02:09:19.813 ServerApp] #> (omitted) | #> (omitted) | To access the server, open this file in a browser: #> (omitted) | file:///home/jovyan/.local/share/jupyter/runtime/jpserver-7-open.html #> (omitted) | Or copy and paste one of these URLs: #> (omitted) | http://7bed3d1c3851:8888/lab?token=e6ad88f2a1b6358e1de88ea5a99ba3fd0b872293d3c9e845 #> (omitted) | or http://127.0.0.1:8888/lab?token=e6ad88f2a1b6358e1de88ea5a99ba3fd0b872293d3c9e845 Once this command is executed, your terminal will show the address to access the Jupyter Lab through a web interface. Use your browser to access the following address: firefox http://127.0.0.1:8888/lab?token = e6ad88f2a1b6358e1de88ea5a99ba3fd0b872293d3c9e845 After accessing it, access the processing file that is in the following directory structure: analysis > notebook > research-processing.ipynb . To exemplify this process, bellow, there is a video of the mentioned steps: Jupyter Notebook configuration to process the data After the execution, the generated products will be available on the directory analysis/data/derived_data Removing data After the tests, if you wish, you can remove the input/output data, so it does not use your disk volume. Through the GNU Make execute the command example_cleanup_data : make example_cleanup_data This will erase all the data. Doing this, the next execution will require to download again the files. Processing through Dagster \u00b6 The second approach to process the downloaded data is using Dagster. With this tool, the processing of the data is performed as batch . More Dagster For more informationg regarding this RC Dagster, consult the section Processing scripts - Dagster . To use Dagster and process the data, use the command example_pipeline through the GNU Make . This command will configure the Container so you can access DagIt through interface DagIt : make example_pipeline #> (omitted) #> (omitted) | Welcome to Dagster! #> (omitted) | #> (omitted) | If you have any questions or would like to engage with the Dagster team, please join us on Slack #> (omitted) | (https://bit.ly/39dvSsF). #> (omitted) | #> (omitted) | Serving on http://0.0.0.0:3000 in process 1 Once this command is executed, your terminal will show the adress to access Dagster. Use your browser to navegate and access the following address: firefox http://127.0.0.1:3000 Pro tip The address presented in the terminal is 0.0.0.0 and the access is through the address 127.0.0.1 in the above example. This is possible since, 0.0.0.0 means that any address can access the created service. Accessing the address you will be at DagIt and can start processing data. The Figure bellow shows an example of the DagIt you will see: DagIt example interface. To start processing data ib DagIt, select the option Playground : DagIt's interface Playground Option. Acessing the Playground tab you will see a field containing the configuration definitions that should be considered during processing. This configuration is used to determine which will be the inputs, auxiliary data and also the output in which data will be written. These options must be defined in order to consider the downloaded data. The Figure Bellow presents a field in which the configurations are set. DagIt configuration field. To auxiliate in this configuration, during the data download (Subsection Data Download ), the script Example toolkit also generated the Dagster configuration required to use the data. This file is available at the directory analysis/pipeline/ , with the name config.yaml . Dagster Configuration and Example toolkit For more information regarding the Dagster configuration format and how it can be adapted to your context, please, see the reference section Dagster - Configuration . If you wish to understand how Example toolkit works, consult the reference section Auxiliary scripts . Copy the content of the config.yaml file and paste it on the DagIt configuration interface field: DagIt configuration field (Filled). Once this, start the processing by clicking Launch Execution . To exemplify each of these steps, bellow you can find a video with each of the configuration steps and use of Dagster: Dagster configuration to process the data After the execution, the generated products will be available on the directory analysis/data/derived_data . Removing Data After the tests, if you wish, you can remove the input/output data, so it does not use your disk volume. Through the GNU Make execute the command example_cleanup_data : make example_cleanup_data This will erase all the data. Doing this, the next execution will require to download again the files.","title":"Minimal example"},{"location":"en/reproducible-research/minimal-example/#minimal-example","text":"Requirements Before starting this example, certify that the requirements are installed in your work environment. Alongside this RC , there is an article in which the harmonization experiments were conducted using Landsat-8/OLI and Sentinel-2/MSI images. As presented in the Reference Chapter , the materials and tools of this RC , represent the effort of generating the mentioned article results. In this section, these materials and a reproducible example are presented, going through all the processing chain used in the article to generate the harmonized products. This minimal example consists in a small subset containing 4 scenes ( 2x Landsat-8/OLI and 2x Sentinel-2/MSI ) extracted from the original dataset. With this minimal example it is expected to allow researchers to explore the produced material and the chain implementation of the article. Reference This is a practical section, in which the RC materials are used. If you need more info regarding the used tools, please consult the Reference Chapter .","title":"Minimal example"},{"location":"en/reproducible-research/minimal-example/#research-compendium-download","text":"This example first step is to download this RC and all its materials. To do this, in a terminal, use git to clone : git clone https://github.com/brazil-data-cube/compendium-harmonization After the clone , a new directory will be created in your current directory. Its name is compendium-harmonization : ls -ls . #> 4 drwxrwxr-x 3 ubuntu ubuntu 4096 May 2 00:44 compendium-harmonization Now, access the directory compendium-harmonization and list its content: Changing directory cd compendium-harmonization Listing its content ls -ls . #> total 76K #> drwxrwxr-x 9 ubuntu ubuntu 4.0K May 1 23:29 . #> drwxrwxr-x 4 ubuntu ubuntu 4.0K May 2 00:44 .. #> drwxrwxr-x 5 ubuntu ubuntu 4.0K Apr 14 17:00 analysis #> -rw-rw-r-- 1 ubuntu ubuntu 1.4K May 1 16:36 bootstrap.sh #> drwxrwxr-x 4 ubuntu ubuntu 4.0K Apr 14 17:00 composes #> drwxrwxr-x 4 ubuntu ubuntu 4.0K Apr 14 17:00 docker #> -rw-rw-r-- 1 ubuntu ubuntu 375 May 1 16:36 .dockerignore #> drwxrwxr-x 3 ubuntu ubuntu 4.0K May 1 16:44 docs #> drwxrwxr-x 7 ubuntu ubuntu 4.0K Apr 14 17:00 .git #> drwxrwxr-x 3 ubuntu ubuntu 4.0K Apr 15 22:53 .github #> -rw-rw-r-- 1 ubuntu ubuntu 4.6K May 1 16:36 .gitignore #> -rw-rw-r-- 1 ubuntu ubuntu 1.1K May 1 16:35 LICENSE #> -rw-rw-r-- 1 ubuntu ubuntu 2.7K May 1 16:36 Makefile #> -rw-rw-r-- 1 ubuntu ubuntu 4.5K Apr 9 20:01 README.md #> -rw-rw-r-- 1 ubuntu ubuntu 392 May 1 16:36 setenv.sh #> drwxrwxr-x 6 ubuntu ubuntu 4.0K Apr 14 17:00 tools #> -rw-rw-r-- 1 ubuntu ubuntu 3.4K May 1 16:36 Vagrantfile As you can see, the content of this directory are the materials of this RC . This will be the base files used in this tutorial. The description of each directory of this RC can be found in the Introduction of this documentation.","title":"Research Compendium Download"},{"location":"en/reproducible-research/minimal-example/#download-the-data","text":"After downloading the RC , one can follow the steps presented in the introduction of the ( Data Processing ) chapter to download the files that will be used in this example. These data are described in the Example Toolkit section and are stored in the GitHub Release Assets . It will be required to use the Example Toolkit to obtain them. This tool downloads and organizes the data in the structure required by the processing scripts . To execute it, first create a Docker network, in which the Containers created in this tutorial will be associated to. To do that, in your terminal, use the following command: docker network create research_processing_network #> fdaa46b4fe70bd34b6cb0e59734376234d801599a1fb1cbe1d9fd66a8f5044b1 Now, through your GNU Make , execute the following command example_download_data : make example_download_data Pro tip During the GNU Make execution, if you have any problema regarding permissions on the execution of the setenv.sh file, use the following command before executing the GNU Make again: chmod +x setenv.sh This command will use a Docker Compose to download the example data. After executing, the data download will start and a message similar to the one bellow will be presented (A few fields are omitted here so the documentation can be more readable): Creating example-minimal-download-data ... done Attaching to example-minimal-download-data ( omitted ) | 2022 -05-02 01 :16:20.078 | INFO | ( omitted ) - Downloading minimal-example_landsat8_data.zip ( omitted ) ( omitted ) | 2022 -05-02 01 :21:09.345 | INFO | ( omitted ) - Downloading minimal-example_lasrc_auxiliary_data.zip ( omitted ) ( omitted ) | 2022 -05-02 01 :22:35.845 | INFO | ( omitted ) - Downloading minimal-example_scene_id_list.zip ( omitted ) ( omitted ) | 2022 -05-02 01 :22:36.510 | INFO | ( omitted ) - Downloading minimal-example_sentinel2_data.zip ( omitted ) ( omitted ) | 2022 -05-02 01 :25:14.653 | INFO | ( omitted ) - All files are downloaded. example-minimal-download-data exited with code 0 Data download If you are interested in obtaining more information regarding the data download , please, consult the reference section Auxiliary scripts . Following the RC organization , the downloaded data were stored in the directory analysis/data/examples/minimal_example/raw_data/ : ls -ls analysis/data/examples/minimal_example/raw_data/ #> total 16 #> 4 drwxrwxrwx 4 root root 4096 May 2 01:25 landsat8_data #> 4 drwxrwxrwx 5 root root 4096 May 2 01:22 lasrc_auxiliary_data #> 4 drwxrwxrwx 2 root root 4096 May 2 01:22 scene_id_list #> 4 drwxrwxrwx 4 root root 4096 May 2 01:25 sentinel2_data Data organization The data organization in the directory analysis/data/examples/minimal_example/raw_data/ follow the processing scripts required pattern. For more information, please, consult the reference section Data Directory .","title":"Download the data"},{"location":"en/reproducible-research/minimal-example/#processing-data-through-jupyter-notebook","text":"Continuing the processing chain presented in the Chapter Indroduction , a first way to execute the processings described in the article associated with this RC , is through a Jupyter Notebook. This document has the detailed description of each step of the processing chain. In this example, the notebooks will be the first approach. Jupyter Notebook everywhere For more information regarding this RC Jupyter Notebooks, consult the section Processing scripts - Jupyter Notebook . To use the notebooks and process the data, you can use the command example_notebook through the GNU Make . This command will configure the Container in order to you execute the Jupyter Notebook through an interface Jupyter Lab : make example_notebook #> (omitted) | [C 2022-05-02 02:09:19.813 ServerApp] #> (omitted) | #> (omitted) | To access the server, open this file in a browser: #> (omitted) | file:///home/jovyan/.local/share/jupyter/runtime/jpserver-7-open.html #> (omitted) | Or copy and paste one of these URLs: #> (omitted) | http://7bed3d1c3851:8888/lab?token=e6ad88f2a1b6358e1de88ea5a99ba3fd0b872293d3c9e845 #> (omitted) | or http://127.0.0.1:8888/lab?token=e6ad88f2a1b6358e1de88ea5a99ba3fd0b872293d3c9e845 Once this command is executed, your terminal will show the address to access the Jupyter Lab through a web interface. Use your browser to access the following address: firefox http://127.0.0.1:8888/lab?token = e6ad88f2a1b6358e1de88ea5a99ba3fd0b872293d3c9e845 After accessing it, access the processing file that is in the following directory structure: analysis > notebook > research-processing.ipynb . To exemplify this process, bellow, there is a video of the mentioned steps: Jupyter Notebook configuration to process the data After the execution, the generated products will be available on the directory analysis/data/derived_data Removing data After the tests, if you wish, you can remove the input/output data, so it does not use your disk volume. Through the GNU Make execute the command example_cleanup_data : make example_cleanup_data This will erase all the data. Doing this, the next execution will require to download again the files.","title":"Processing data through Jupyter Notebook"},{"location":"en/reproducible-research/minimal-example/#processing-through-dagster","text":"The second approach to process the downloaded data is using Dagster. With this tool, the processing of the data is performed as batch . More Dagster For more informationg regarding this RC Dagster, consult the section Processing scripts - Dagster . To use Dagster and process the data, use the command example_pipeline through the GNU Make . This command will configure the Container so you can access DagIt through interface DagIt : make example_pipeline #> (omitted) #> (omitted) | Welcome to Dagster! #> (omitted) | #> (omitted) | If you have any questions or would like to engage with the Dagster team, please join us on Slack #> (omitted) | (https://bit.ly/39dvSsF). #> (omitted) | #> (omitted) | Serving on http://0.0.0.0:3000 in process 1 Once this command is executed, your terminal will show the adress to access Dagster. Use your browser to navegate and access the following address: firefox http://127.0.0.1:3000 Pro tip The address presented in the terminal is 0.0.0.0 and the access is through the address 127.0.0.1 in the above example. This is possible since, 0.0.0.0 means that any address can access the created service. Accessing the address you will be at DagIt and can start processing data. The Figure bellow shows an example of the DagIt you will see: DagIt example interface. To start processing data ib DagIt, select the option Playground : DagIt's interface Playground Option. Acessing the Playground tab you will see a field containing the configuration definitions that should be considered during processing. This configuration is used to determine which will be the inputs, auxiliary data and also the output in which data will be written. These options must be defined in order to consider the downloaded data. The Figure Bellow presents a field in which the configurations are set. DagIt configuration field. To auxiliate in this configuration, during the data download (Subsection Data Download ), the script Example toolkit also generated the Dagster configuration required to use the data. This file is available at the directory analysis/pipeline/ , with the name config.yaml . Dagster Configuration and Example toolkit For more information regarding the Dagster configuration format and how it can be adapted to your context, please, see the reference section Dagster - Configuration . If you wish to understand how Example toolkit works, consult the reference section Auxiliary scripts . Copy the content of the config.yaml file and paste it on the DagIt configuration interface field: DagIt configuration field (Filled). Once this, start the processing by clicking Launch Execution . To exemplify each of these steps, bellow you can find a video with each of the configuration steps and use of Dagster: Dagster configuration to process the data After the execution, the generated products will be available on the directory analysis/data/derived_data . Removing Data After the tests, if you wish, you can remove the input/output data, so it does not use your disk volume. Through the GNU Make execute the command example_cleanup_data : make example_cleanup_data This will erase all the data. Doing this, the next execution will require to download again the files.","title":"Processing through Dagster"},{"location":"en/reproducible-research/replication-example/","text":"Replica\u00e7\u00e3o example \u00b6 requirements Before starting this example, verify if all the requirements are installed in your work environment. Base experiment This is an example that uses the RC materials to process data from a region that isn't considered in the original article. The objective is to show that the tool has characteristics that makes it reproducible and replicable. If you haven't performed the first example ( Minimal exemple ), it is recommended to do it before starting this one. Download the Research Compendium \u00b6 To execute this example first you should download this RC and its materials. To do that, in a terminal, use git to clone the repository in which this RC is stored: git clone https://github.com/brazil-data-cube/compendium-harmonization After the clone , a new directory will be created in your current directory. The name of this new directory is compendium-harmonization : ls -ls . #> 4 drwxrwxr-x 3 ubuntu ubuntu 4096 May 2 00:44 compendium-harmonization Now, access the directory compendium-harmonization and list its content: Changing directory cd compendium-harmonization Listing directory content ls -ls . #> total 76K #> drwxrwxr-x 9 ubuntu ubuntu 4.0K May 1 23:29 . #> drwxrwxr-x 4 ubuntu ubuntu 4.0K May 2 00:44 .. #> drwxrwxr-x 5 ubuntu ubuntu 4.0K Apr 14 17:00 analysis #> -rw-rw-r-- 1 ubuntu ubuntu 1.4K May 1 16:36 bootstrap.sh #> drwxrwxr-x 4 ubuntu ubuntu 4.0K Apr 14 17:00 composes #> drwxrwxr-x 4 ubuntu ubuntu 4.0K Apr 14 17:00 docker #> -rw-rw-r-- 1 ubuntu ubuntu 375 May 1 16:36 .dockerignore #> drwxrwxr-x 3 ubuntu ubuntu 4.0K May 1 16:44 docs #> drwxrwxr-x 7 ubuntu ubuntu 4.0K Apr 14 17:00 .git #> drwxrwxr-x 3 ubuntu ubuntu 4.0K Apr 15 22:53 .github #> -rw-rw-r-- 1 ubuntu ubuntu 4.6K May 1 16:36 .gitignore #> -rw-rw-r-- 1 ubuntu ubuntu 1.1K May 1 16:35 LICENSE #> -rw-rw-r-- 1 ubuntu ubuntu 2.7K May 1 16:36 Makefile #> -rw-rw-r-- 1 ubuntu ubuntu 4.5K Apr 9 20:01 README.md #> -rw-rw-r-- 1 ubuntu ubuntu 392 May 1 16:36 setenv.sh #> drwxrwxr-x 6 ubuntu ubuntu 4.0K Apr 14 17:00 tools #> -rw-rw-r-- 1 ubuntu ubuntu 3.4K May 1 16:36 Vagrantfile As you can see, the content of the directory represents all the materials found on this RC . This will be the content used to execute this tutorial. Data Download \u00b6 To download the replication example data you can use the following command: make replication_ download_data Verify if the data were downloaded at the directory analysis/data/examples/replication_example/ . It must be similar to the following: . \u251c\u2500\u2500 landsat8_data \u251c\u2500\u2500 lasrc_auxiliary_data \u251c\u2500\u2500 scene_id_list \u2514\u2500\u2500 sentinel2_data Data Processing \u00b6 The data processing can be performed through Jupyter Notebook or Dagster . For each approach, automatic commands can be used through the Makefile . You can check bellow how to start it from each of the environments: Jupyter Notebook make replication_notebook Changes required As the replication example uses other data from that presented in the minimal example, you should modify some Jupyter Notebook parameters \u2014 specifically, the day_diff parameter. This parameter defines the number of days difference between images. This change should only be made for tests where only Sentinel-2 is used. So, you must change the following sections of the Jupyter Notebook: 4.3.1: Searching for image pairs 4.4.1: Searching for image pairs 4.5.1: Searching for image pairs 4.6.1: Searching for image pairs In all of them, you must change day_diff to 15 days : validation_funcs . search_pairs_s2 ( sentinel2_sceneid_list , day_diff = 15 ) Dagster make replication_pipeline Changes required As the replication example uses other data from that presented in the minimal example, you should modify some Jupyter Notebook parameters \u2014 specifically, the day_diff parameter. This parameter defines the number of days difference between images. This change should only be made for tests where only Sentinel-2 is used. So, you must add the following changes to the solids section of your Dagster configuration file : solids : validation_sr_s2_sen2cor : config : day_difference : 15 validation_sr_s2_lasrc : config : day_difference : 15 validation_nbar_s2_sen2cor : config : day_difference : 15 validation_nbar_s2_lasrc : config : day_difference : 15 Once the environment is chosen, the processing and data analysis can be performed.","title":"Replication example"},{"location":"en/reproducible-research/replication-example/#replicacao-example","text":"requirements Before starting this example, verify if all the requirements are installed in your work environment. Base experiment This is an example that uses the RC materials to process data from a region that isn't considered in the original article. The objective is to show that the tool has characteristics that makes it reproducible and replicable. If you haven't performed the first example ( Minimal exemple ), it is recommended to do it before starting this one.","title":"Replica\u00e7\u00e3o example"},{"location":"en/reproducible-research/replication-example/#download-the-research-compendium","text":"To execute this example first you should download this RC and its materials. To do that, in a terminal, use git to clone the repository in which this RC is stored: git clone https://github.com/brazil-data-cube/compendium-harmonization After the clone , a new directory will be created in your current directory. The name of this new directory is compendium-harmonization : ls -ls . #> 4 drwxrwxr-x 3 ubuntu ubuntu 4096 May 2 00:44 compendium-harmonization Now, access the directory compendium-harmonization and list its content: Changing directory cd compendium-harmonization Listing directory content ls -ls . #> total 76K #> drwxrwxr-x 9 ubuntu ubuntu 4.0K May 1 23:29 . #> drwxrwxr-x 4 ubuntu ubuntu 4.0K May 2 00:44 .. #> drwxrwxr-x 5 ubuntu ubuntu 4.0K Apr 14 17:00 analysis #> -rw-rw-r-- 1 ubuntu ubuntu 1.4K May 1 16:36 bootstrap.sh #> drwxrwxr-x 4 ubuntu ubuntu 4.0K Apr 14 17:00 composes #> drwxrwxr-x 4 ubuntu ubuntu 4.0K Apr 14 17:00 docker #> -rw-rw-r-- 1 ubuntu ubuntu 375 May 1 16:36 .dockerignore #> drwxrwxr-x 3 ubuntu ubuntu 4.0K May 1 16:44 docs #> drwxrwxr-x 7 ubuntu ubuntu 4.0K Apr 14 17:00 .git #> drwxrwxr-x 3 ubuntu ubuntu 4.0K Apr 15 22:53 .github #> -rw-rw-r-- 1 ubuntu ubuntu 4.6K May 1 16:36 .gitignore #> -rw-rw-r-- 1 ubuntu ubuntu 1.1K May 1 16:35 LICENSE #> -rw-rw-r-- 1 ubuntu ubuntu 2.7K May 1 16:36 Makefile #> -rw-rw-r-- 1 ubuntu ubuntu 4.5K Apr 9 20:01 README.md #> -rw-rw-r-- 1 ubuntu ubuntu 392 May 1 16:36 setenv.sh #> drwxrwxr-x 6 ubuntu ubuntu 4.0K Apr 14 17:00 tools #> -rw-rw-r-- 1 ubuntu ubuntu 3.4K May 1 16:36 Vagrantfile As you can see, the content of the directory represents all the materials found on this RC . This will be the content used to execute this tutorial.","title":"Download the Research Compendium"},{"location":"en/reproducible-research/replication-example/#data-download","text":"To download the replication example data you can use the following command: make replication_ download_data Verify if the data were downloaded at the directory analysis/data/examples/replication_example/ . It must be similar to the following: . \u251c\u2500\u2500 landsat8_data \u251c\u2500\u2500 lasrc_auxiliary_data \u251c\u2500\u2500 scene_id_list \u2514\u2500\u2500 sentinel2_data","title":"Data Download"},{"location":"en/reproducible-research/replication-example/#data-processing","text":"The data processing can be performed through Jupyter Notebook or Dagster . For each approach, automatic commands can be used through the Makefile . You can check bellow how to start it from each of the environments: Jupyter Notebook make replication_notebook Changes required As the replication example uses other data from that presented in the minimal example, you should modify some Jupyter Notebook parameters \u2014 specifically, the day_diff parameter. This parameter defines the number of days difference between images. This change should only be made for tests where only Sentinel-2 is used. So, you must change the following sections of the Jupyter Notebook: 4.3.1: Searching for image pairs 4.4.1: Searching for image pairs 4.5.1: Searching for image pairs 4.6.1: Searching for image pairs In all of them, you must change day_diff to 15 days : validation_funcs . search_pairs_s2 ( sentinel2_sceneid_list , day_diff = 15 ) Dagster make replication_pipeline Changes required As the replication example uses other data from that presented in the minimal example, you should modify some Jupyter Notebook parameters \u2014 specifically, the day_diff parameter. This parameter defines the number of days difference between images. This change should only be made for tests where only Sentinel-2 is used. So, you must add the following changes to the solids section of your Dagster configuration file : solids : validation_sr_s2_sen2cor : config : day_difference : 15 validation_sr_s2_lasrc : config : day_difference : 15 validation_nbar_s2_sen2cor : config : day_difference : 15 validation_nbar_s2_lasrc : config : day_difference : 15 Once the environment is chosen, the processing and data analysis can be performed.","title":"Data Processing"},{"location":"en/tools/","text":"Compendium Reference \u00b6 This is the Reference Chapter of this RC . In this Chapter, the properties of the RC materials are presented in detail. The user can explore the content to learn how to use this RC content outside the context of the original work and also to reproduce the original article results. Recommendation We recommended that you use this Chapter in conjunction with running through the examples provided in the Chapter Data Processing . This way, you can see the application of the materials. So, when you feel the need for further clarification, you can refer to the content available in this reference Chapter.","title":"Introduction"},{"location":"en/tools/#compendium-reference","text":"This is the Reference Chapter of this RC . In this Chapter, the properties of the RC materials are presented in detail. The user can explore the content to learn how to use this RC content outside the context of the original work and also to reproduce the original article results. Recommendation We recommended that you use this Chapter in conjunction with running through the examples provided in the Chapter Data Processing . This way, you can see the application of the materials. So, when you feel the need for further clarification, you can refer to the content available in this reference Chapter.","title":"Compendium Reference"},{"location":"en/tools/environment/","text":"Computational Environments \u00b6 Behind each step on the processing scripts , as presented in the previous sections, there are several tools and software libraries being used. Some of these tools use special technologies to execute their operations, as is the case for the research-processing library , that uses Docker Containers to execute the processing functions in isolated environments. Other tools only use their own environment, as for the auxiliary script Example toolkit . In this case, it is required that its own environment to be configured to execute the script . In both presented scenaries, there are specific challenges on managing the used computational environment. For instance, specific configurations may be required on the software to operate alongside the research-processing library , while specific configurations may be required during the Example toolkit use. To solve these problems and avoud that the configuration used to interfere with reproductibility and replicability of the processing scripts created in this RC , all the environments required for using the tools were organized in Docker images. These, represent \"environment packages\" ready to use, in which all dependencies and required configurations are already set. In this section each Docker Image is presented, its characteristics, configuration and use. Note that these images were not created for a specific environment, as long as it supports Docker, any operational system can be used. However, in this documentation, Linux Ubuntu 20.04 syntax was adopted. Thus, changes can be required on the commands if you wish to use a different operational system, e.g. Windows. Changes between operational systems Although we believe that the commands and hints in this document can be used without trouble in Linux operational systems (e.g., Ubuntu, Debian) and MacOS, there is no warranty that this will always be true. Besides, for those who use Windows, changes in the commands may be required. To enable the use of the materials produced here even in those environments, we also created a Ubuntu 20.04 Virtual Machine, containing all required dependencies (e.g., Docker , Docker Compose ) to enable that all the commands presented here could be used. If you need to use this Virtual Machine, please, consult the Section Virtual Machine with Vagrant . Docker Images \u00b6 In this RC , there are different types of environments that are being configured within the Docker Images, which can be categorized in two types: Executable Command Line Interfaces ( CLI ) are simple and direct to use, allowing automation during the processing steps. The Executable Docker Images are the images created to store a script that can be executed as a CLI . For that, this type of Docker Image has the following properties: Each Docker Image execution represents an individual execution of the tool it is associated; Parameters can be passed during a Docker Image execution. These parameters are used to configure the executed tool; Docker Volumes and environmental variables, also can be used to configure a Docker Image, being used to determine the inputs, outputs and configurations of the executed tool. Environment Different from the Executable Docker Images, these Docker Images are created to serve a complete environment that will be used to execute the tool, as a Jupyter Notebook or a Dagster Web Interface. The main difference between these two types of Docker Images created in this RC is its goals. While the Executables represent the executable tools, the Environment represent their environment, used to execute these specific tools. In the following Subsections, the Docker Images created in this RC are presented. Sen2Cor 2.9.0 \u00b6 Sen2cor is an atmospherical correction processor developed for Sentinel-2 products. As inputs it uses Top of Atmosphere (ToA) radiance Sentinel-2 products, also called Level-1C (L1C) and generates \"Bottom of the atmosphere (BOA) reflectance\" products, also providing a Scene Classification Layer (SCL), that has several classes, including cloud, cloud shadow and snow. More about Sen2Cor For more information regarding Sen2Cor, consult the oficial User Manual . We prepared a Docker Image with sen2cor installed to allow executions of it to be reproducible and reusable. This Docker Image, named sen2cor , has all its dependencies and configurations required in order to execute the Sen2Cor. Sen2cor Versions The Sen2Cor , is a software maintained by E uropean S pace A gency (ESA) and continues in development with new versions being released. In this RC , the Sen2cor version 2.9.0 was used. The following topics, present the main characterisctis of this Docker Image, as volumes, auxiliary data and how to use it. Auxiliary Data To execute the sen2cor , it is required to obtain some auxiliary data. The ESACCI-LC for Sen2Cor data package , which is used to identify clouds and classify a scene. It can be obtained following the steps listed bellow: Access the address: http://maps.elie.ucl.ac.be/CCI/viewer/download.php ; Sign in; Search for the ESACCI-LC for Sen2Cor data package ; Download this package ( zip file); Extract its content in a directory. It is recommended to name it CCI4SEN2COR . After extracting the files, the directory will contain the following files: ESACCI-LC-L4-WB-Map-150m-P13Y-2000-v4.0.tif (GeoTIFF); ESACCI-LC-L4-LCCS-Map-300m-P1Y-2015-v2.0.7.tif (GeoTIFF); ESACCI-LC-L4-Snow-Cond-500m-P13Y7D-2000-2012-v2.0 (Direct\u00f3rio). Volumes To use the Sen2cor , it is required to define a few Docker Volumes. These volumes specify the input, output, configuration files and auxiliary files used by the tool. Bellow these volumes are listed and descripted: Input data (Required) Directory containing the input files. This volume should map a directory in the local machine to the Container /mnt/input_dir directory. It is recommended to use a read-only volume, to ensure no modification to be made on the input data. Output directory (Required) Directory in which the output products will be stored. This volume should map a directory in the local machine to the Container /mnt/output_dir directory. Auxiliary data (Required) Directory containing the auxiliary files, required by Sen2cor. This volume should map a directory in the local machine to the Container /mnt/sen2cor-aux/CCI4SEN2COR . Configuration file (Optional) Volume that defines a configuration file ( L2A_GIPP.xml ). This volume should map a L2A_GIPP.xml file in the local machine to the Container /opt/sen2cor/2.9.0/cfg/L2A_GIPP.xml . SRTM data (Opcional) Volume to store SRTM data used by Sen2cor. This volume should map a directory in the local machine to the Container /mnt/sen2cor-aux/srtm . Use example (Docker CLI ) The following code presents, through Docker CLI , an example on how to use the Docker Image sen2cor to process a single scene. Image name On the following command, the Docker Image sen2cor is identified as marujore/sen2cor:2.9.0 stored in the user marujore on DockerHub, being the chosen version 2.9.0 . Command format The following command was created to be didatic. If you desire to use it, don't forguet to replace its values and remove the blank spaces between lines.O comando abaixo \u00e9 criado para ser dit\u00e1dico. docker run --rm \\ # Volume: Input data --volume /path/to/input_dir:/mnt/input_dir:ro \\ # Volume: Output Data --volume /path/to/output_dir:/mnt/output_dir:rw \\ # Auxiliary Data: Diret\u00f3rio CCI4SEN2COR --volume /path/to/CCI4SEN2COR:/mnt/aux_data \\ # Configuration file: L2A_GIPP.xml (Opcional) --volume /path/to/L2A_GIPP.xml:/opt/sen2cor/2.9.0/cfg/L2A_GIPP.xml \\ # SRTM Data (Opcional) --volume /path/to/srtm:/root/sen2cor/2.9/dem/srtm \\ # Docker Image and scene to be processed marujore/sen2cor:2.9.0 S2A_MSIL1C_20210903T140021_N0301_R067_T21KVR_20210903T172609.SAFE The execution of the command presented above will create a sen2cor Docker Container. This Docker Container will process the S2A_MSIL1C_20210903T140021_N0301_R067_T21KVR_20210903T172609.SAFE scene. Note that in this command, the input directory ( /path/to/input_dir ) must contain a subdirectory with the scene S2A_MSIL1C_20210903T140021_N0301_R067_T21KVR_20210903T172609.SAFE . For more information, consult the GitHub Repository , where the versioning of the changes in this Docker Image ( sen2cor ) has been keep. LaSRC 2.0.1 \u00b6 LaSRC is an atmospheric correction processor, originally proposed for the Landsat-8 Collection 1, and posteriorly adapted to also correct Sentinel-2 products. It uses as input Landsat-8 Digital Number products (DN) or Sentinel-2 Top of Atmosphere (ToA) radiance products, also called Level-1C (L1C). As results this tool generates surface reflectance (SR) products. To facilitate the use of LaSRC in this RC , and ensure the executions to be reproducible and reusable, we created a Docker image for the LaSRC, called lasrc . The lasrc , has all dependencies and configuration required to execute the LaSRC processor. The following topics present the main characteristics of this Docker Image, as volumes, auxiliary data and how to use it. Auxiliary data To execute lasrc , it is required to define a few auxiliary data. To obtain then see the following steps: Access: https://edclpdsftp.cr.usgs.gov/downloads/auxiliaries/lasrc_auxiliary/L8/ ; Download all the available contents, except the LADS folder. LADS data is also required to use the LaSRC. However, this directory contains daily files since 2013 until nowadays, which is a huge volume of data. To perform this RC experiment, you can download the LADS accordingly to the dates of the data you will process. LADS files selection Each LADS file links to a day of the year. Thus, to process the image from January first 2017, one can obtain the LADS L8ANC2017001.hdf_fused in 2017, which is represented by 001 in the day of the year format. At the end of the acquisition, the auxiliary data directory must follow this structure: . \u251c\u2500\u2500 CMGDEM.hdf \u251c\u2500\u2500 LADS \u251c\u2500\u2500 LDCMLUT \u251c\u2500\u2500 MSILUT \u2514\u2500\u2500 ratiomapndwiexp.hdf Volumes To use the lasrc , it is required to define a few Docker Volumes. These volumes, specify the input data, output data and auxiliary files used by the tool during processing. Bellow, is descripted each volume that must be created during the execution of LaSRC Docker Image: Input data (Required) Directory containing the input data. This volume should map a directory in the local machine to the Container /mnt/input_dir directory. It is recommended to use a read-only volume, to ensure no modification to be made on the input data. Output data (Required) Directory containing the output data. This volume should map a directory in the local machine to the Container /mnt/output_dir directory. Auxiliary data (Required) Directory containing the auxiliary data required by LaSRC. The created volume, should map a directory in the local machine to the Container /mnt/atmcor_aux/lasrc/L8 directory. Use example (Docker CLI ) The codes bellow present two examples on how to use lasrc , through the Docker CLI . On the first example, a Landsat-8/OLI scene is processed, while on the second a Sentinel-2/MSI image is processed. Image name On the following commands, a lasrc Docker Image is identified as marujore/lasrc:2.0.1 , stored in the profile marujore on DockerHub. Command format The following command was created to be didatic. If you desire to use it, don't forguet to replace its values and remove the blank spaces between lines.O comando abaixo \u00e9 criado para ser dit\u00e1dico. LaSRC Landsat-8/OLI example docker run --rm \\ # Volume: Input data --volume /path/to/input/:/mnt/input-dir:rw \\ # Volume: Output data --volume /path/to/output:/mnt/output-dir:rw \\ # auxiliary data (data L8/LADS) --volume /path/to/lasrc_auxiliaries/L8:/mnt/atmcor_aux/lasrc/L8:ro \\ # Docker Image and scene to be processed --tty brazildatacube/lasrc:2.0.1 LC08_L1TP_220069_20190112_20190131_01_T1 LaSRC Sentinel-2/MSI example docker run --rm \\ # Volume: Input data --volume /path/to/input/:/mnt/input-dir:rw \\ # Volume: Output data --volume /path/to/output:/mnt/output-dir:rw \\ # auxiliary data (data L8/LADS) --volume /path/to/lasrc_auxiliaries/L8:/mnt/atmcor_aux/lasrc/L8:ro \\ # Docker Image and scene to be processed --tty brazildatacube/lasrc:2.0.1 S2A_MSIL1C_20190105T132231_N0207_R038_T23LLF_20190105T145859.SAFE As can be noted, the difference on using lasrc for different satellite-sensors relly only on the sceneid name. It is important to note that, for both cases, the input directory ( /path/to/input/ ) must contain the scenes to be processed. In this case LC08_L1TP_220069_20190112_20190131_01_T1 and S2A_MSIL1C_20190105T132231_N0207_R038_T23LLF_20190105T145859.SAFE . For more information, consult the Github repository , where the versioning of the changes in this Docker Image ( lasrc ) has been keep. L8Angs \u00b6 Landsat Ang Tool is a tool developed and maintained by U nited S tates G eological S urvey . This tool uses the ANG.txt that are provided alongside Landsat-8 images and use them to generate per pixel angle bands. The angle bands are solar azimuth angle ( SAA ), solar zenithal angle ( SZA ), view azimuth angle ( VAA ) and view zenithal angle ( VZA ). The bands are generated with the same resolution as OLI soectral bands (30m). More about Landsat Ang Tool For more information regarding Landsat Ang Tool, consult the USGS official page about the tool . In this RC , the Landsat-8/OLI (Collection-2) images were obtained already processed to Surface Reflectance Level (L2). However, for further processing we use the angle bands, using the Landsat Ang Tool . The installation and configuration of the Landsat Ang Tool can make hard to replicate and reproduce this experiment in the future. Due to that, in this RC we created a Docker Image, named l8angs , containing the tool. The following topics present the main characteristics of this Docker Image, as volumes and auxiliary data to its execution, as well as Docker CLI execution examples. Volumes To use l8angs , you must provide the following volume during the execution: Input data (Required) The created volume, should map a directory in the local machine to the Container /mnt/input-dir directory. The angle bands are generated on the same input directory, this is the standard behavior of the tool. Use example (Docker CLI ) The code bellow, presents a use example of the l8angs through Docker CLI . Image name In the following commands, the l8angs Docker image, identified as marujore/landsat-angles:latest on DockerHub. Command format The following command was created to be didatic. If you desire to use it, don't forguet to replace its values and remove the blank spaces between lines.O comando abaixo \u00e9 criado para ser dit\u00e1dico. docker run --rm \\ # Volume: Input data -v /path/to/input/:/mnt/input-dir:rw \\ # Docker Image and scene to be processed marujore/landsat-angles:latest LC08_L2SP_222081_20190502_20200829_02_T1 The execution of the command presented above will create a l8angs Docker Container. This Docker Container will process the LC08_L2SP_222081_20190502_20200829_02_T1 scene. In this command, the input dir ( /path/to/input/ ) must contain a subdirectory named LC08_L2SP_222081_20190502_20200829_02_T1 , which is the scene being processed. For more information, consult the GitHub repository , which contains the versioning and changes made on l8angs . Sensor Harm \u00b6 In this RC , the Landsat-8 Collection-2 images were already obtained as surface reflectance products (L2) while Sentinel-2 images were obtained as L1C and processed using Sen2cor or LaSRC. Both Landsat-8 and Sentinel-2 images are harmonized using the sensor-harm library. To allow reproductibility and replicability of this tool use, we created a Docker Image called sensor-harm . In this image, all dependencies and configurations to execute sensor-harm are already prepared. The following topics present the main characteristics of this Docker Image, the volumes it requires and use examples. Volumes To use the sensor-harm , it is required to define some Docker Volumes. These volumes specify the input and auxiliary data used by sensor-harm. Bellow a list of these volumes is listed and descripted: Input data (Required) Directory containing the input data. This volume should map a directory in the local machine to the Container /mnt/input_dir directory. It is recommended to use a read-only volume, to ensure no modification to be made on the input data. Output data (Required) Directory containing the output data. This volume should map a directory in the local machine to the Container /mnt/output_dir directory. Angle directory (Required only for Landsat-8/OLI) Directory containing the angles of the scene that will be processed. The created volume, should map a directory in the local machine to the Container /mnt/angles-dir directory. It is recommended to use a read-only volume, to ensure no modification to be made on the input data. Use example (Docker CLI ) The codes bellow present two examples on how to use sensor-harm through Docker CLI . In the first example, the processing is performed on a Landsat-8/OLI scene, while on the second a Sentinel-2/MSI scene is used. Image name On the following commands, a sensor-harm Docker Image is identified as marujore/sensor-harm:latest , stored in the profile marujore on DockerHub. Command format The following command was created to be didatic. If you desire to use it, don't forguet to replace its values and remove the blank spaces between lines.O comando abaixo \u00e9 criado para ser dit\u00e1dico. Landsat-8/OLI example docker run --rm \\ # Volume: Input data --volume /path/to/input/:/mnt/input-dir:ro \\ # Volume: Output data --volume /path/to/output:/mnt/output-dir:rw \\ # Angle directory (Only for Landsat-8/OLI) --volume /path/to/angles:/mnt/angles-dir:ro \\ # Docker Image and scene to be processed --tty marujore/sensor-harm:latest LC08_L1TP_220069_20190112_20190131_01_T1 Sentinel-2/MSI example docker run --rm \\ # Volume: Input data --volume /path/to/input/:/mnt/input-dir:ro \\ # Volume: Output data --volume /path/to/output:/mnt/output-dir:rw \\ # Docker Image and scene to be processed --tty brazildatacube/sensor-harm:latest S2A_MSIL1C_20190105T132231_N0207_R038_T23LLF_20190105T145859.SAFE As can be noted, the difference on using sensor-harm for different satellite-sensors relly only on the sceneid name. It is important to note that, for both cases, the input directory ( /path/to/input/ ) must contain the scenes to be processed. In this case LC08_L1TP_220069_20190112_20190131_01_T1 and S2A_MSIL1C_20190105T132231_N0207_R038_T23LLF_20190105T145859.SAFE For more information, consult the Github repository , where the versioning of the changes in this Docker Image ( sensor-harm ) has been keep. Processing scripts Docker Images \u00b6 As presented in the Processing scripts section, two environments were created to execute this RC methodology experiments. Oe uses Jupyter Notebook , which is useful for interactive processing of the codes. The second, using Dagster , which is useful for batch execution and error control. To facilitate the use of both approaches, Docker Images were created containing the required environments of each execution. This avoids dependencies to be installed or configured to execute the processing scripts . The following topics present the Docker Images characteristics, required volumes, configuration and use example. Scripts behavior If you wish to reuse these Docker Images, it is recomended first to read about how is the processing scripts behavior , as well as the software libraries used by these scripts . Jupyter Notebook \u00b6 To execute the Jupyter Notebook version, we created the research-processing-jupyter Docker Image. This Docker Image brings a JupyterLab , with the required Python dependencies to execute the scripts . Besides, in this Docker Image, Docker is also installed, allowing the scripts to operate and create other processing Docker Containers. Environment Base The creation of research-processing-jupyter was made using the jupyter/minimal-notebook Docker Image, made available by the Jupyter development team. So, all the environment variables and configurations available on the jupyter/minimal-notebook Docker Image are also applyable to research-processing-jupyter . On the following topics the required configuration to use this Docker image is demonstrated. Examples on how to use the Docker CLI and Docker Compose are also presented. Environment variables To use the research-processing-jupyter , it is required to define the following environment variable: DATA_DIRECTORY (Required) Environmental variable that determines the directory, in the local machine, in which the downloaded data will be saved. Volumes The execution of the research-processing-jupyter requires two volumes to be mounted: Data Volume (Required) Volume containing the data. Following the processing function execution model used in the scripts , this volume will be used by functions inside the container ( Local ) or in other containers ( Containerized ). Thus, the volume mount must attend 2 requirements: The volume must be a Bind Mount ; The volume mapping ( Bind Mount ) must have, in the local machine and in the Container, the same path defined in DATA_DIRECTORY . With these definitions, the volume will be visible within the research-processing-jupyter Container and also by the auxiliary processing Containers generated during the processing script execution. Daemon Socket Volume (Required) To allow the scripts to generate processing Docker Containers, it is required to define the Daemon Socket as a volume. Doing this, the Docker within the container created with the research-processing-jupyter Image is capable of interacting with the local machine Docker, allowing processing Containers to be created. User definition Complementing the Daemon Socket volume definition, to execute the research-processing-jupyter , it is required to specify the user ( UID ) and group ( GID ) on the local machine that has permission to interact with the o Docker Daemon. These values will be applied to the Container standard user so Docker can allow it to also interact with the Docker Daemon of the local machine. Docker permission If you are intereseted in understanding the detail behind this user definition, we recommend that you consult the oficial Docker documentation . To define the user duering research-processing-jupyter execution, you can use the parameter --user . If you with to use the Docker Compose, the field user can be used for this definition. Use example (Docker CLI ) Bellow is presented an example of how to use the research-processing-jupyter Docker Image through the Docker CLI : Command format The following command was created to be didatic. If you desire to use it, don't forguet to replace its values and remove the blank spaces between lines. docker run \\ --name research-processing-jupyter \\ # User definition --user ${ UID } : ${ GID } \\ # Environment variable --env JUPYTER_ENABLE_LAB = yes \\ # Activating JupyterLab --env DATA_DIRECTORY = /my-data-dir \\ # Volume: Data Volume --volume /my-data-dir:/my-data-dir \\ # Volume: Daemon Socket Volume --volume /var/run/docker.sock:/var/run/docker.sock:ro \\ # Network port in which the service will be accessed --publish 8888 :8888 \\ # Docker Image brazildatacube/research-processing-jupyter:latest User definition To define a user, using the environment variables ( ${UID} e ${GID} ), as in the previous command, before executing the Docker command use the following commands: export UID = ` id -u $USER ` export GID = ` cut -d: -f3 < < ( getent group docker ) ` After the execution of the above command, a result simillar to the bellow should be presented: # (Omitted) [ I 2022 -04-30 19 :22:50.684 ServerApp ] Use Control-C to stop this server and shut down all kernels ( twice to skip confirmation ) . [ C 2022 -04-30 19 :22:50.694 ServerApp ] To access the server, open this file in a browser: file:///home/jovyan/.local/share/jupyter/runtime/jpserver-7-open.html Or copy and paste one of these URLs: http://ae88466ccb18:8888/lab?token = 0497e15e042d52cfb498a2edf3d2c6e5874e79b4808ca860 or http://127.0.0.1:8888/lab?token = 0497e15e042d52cfb498a2edf3d2c6e5874e79b4808ca860 After executing this command, using a web browser and accessing the presented JupyterLab address (Replace the address bellow by what has been showed in your terminal): firefox http://127.0.0.1:8888/lab?token = 0497e15e042d52cfb498a2edf3d2c6e5874e79b4808ca860 Use example (Docker Compose) Bellow, the same example performed using Docker CLI is presented using Docker Compose . First, the docker-compose.yml file was created: docker-compose.yml version: '3.2' services: my-notebook: # User definition user: ${UID}:${GID} image: brazildatacube/research-processing-jupyter:latest environment: # Environment variable - JUPYTER_ENABLE_LAB=yes - DATA_DIRECTORY=/my-data-dir volumes: # Volume: Data volume - type: bind source: /my-data-dir target: /my-data-dir # Volume: Daemon Socket Volume - type: bind source: /var/run/docker.sock target: /var/run/docker.sock ports: # Network port in which the service will be accessed - \"8888:8888\" User definition To define a user, using the environment variables ( ${UID} e ${GID} ), as presented in the docker-compose.yml , before executing the Docker command use the following commands: export UID = ` id -u $USER ` export GID = ` cut -d: -f3 < < ( getent group docker ) ` With the file created, the compose can be executed: docker-compose -f docker-compose.yml up The output of the above command should be similar to: # (Omitted) [ I 2022 -04-30 19 :23:57.260 ServerApp ] http://afd0fe2755a7:8888/lab?token = 6d37701a6787bd58a7f92f29f33709970df11a742bf3b79b [ I 2022 -04-30 19 :23:57.260 ServerApp ] or http://127.0.0.1:8888/lab?token = 6d37701a6787bd58a7f92f29f33709970df11a742bf3b79b [ I 2022 -04-30 19 :23:57.260 ServerApp ] Use Control-C to stop this server and shut down all kernels ( twice to skip confirmation ) . [ C 2022 -04-30 19 :23:57.264 ServerApp ] To access the server, open this file in a browser: file:///home/jovyan/.local/share/jupyter/runtime/jpserver-8-open.html Or copy and paste one of these URLs: http://afd0fe2755a7:8888/lab?token = 6d37701a6787bd58a7f92f29f33709970df11a742bf3b79b or http://127.0.0.1:8888/lab?token = 6d37701a6787bd58a7f92f29f33709970df11a742bf3b79b With this information, the JupyterLab can be accesse in the browser. For that open the link displayed in your terminal on your browser: firefox http://127.0.0.1:8888/lab?token = 6d37701a6787bd58a7f92f29f33709970df11a742bf3b79b Dagster \u00b6 To execute the Dagster version, the research-processing-dagster Docker Image was created. This Docker Image has Dagster (version 0.12.15 ), alongside DagIt , a web interface to configure and interact with Dagster. It also has a Docker installed, to allow scripts to create and operate other processing Docker Containers. Environment variables To use the research-processing-dagster , it is required to define the following environmental variable: DATA_DIRECTORY (Required) Environmental variable that determines the directory, in the local machine, in which the downloaded data will be saved. Volumes The execution of the research-processing-dagster requires the definition of the following Docker volumes : Data volume (Required) Volume containing the data. Following the processing function execution model used in the scripts , this volume will be used by functions inside the container ( Local ) or in other containers ( Containerized ). Thus, the volume mount must attend 2 requirements: The volume must be a Bind Mount ; The volume mapping ( Bind Mount ) must have, in the local machine and in the Container, the same path defined in DATA_DIRECTORY . With these definitions, the volume will be visible within the research-processing-jupyter Container and also by the auxiliary processing Containers generated during the processing script execution. Daemon Socket Volume (Required) To allow the scripts to generate processing Docker Containers, it is required to define the Daemon Socket as a volume. Doing this, the Docker within the container created with the research-processing-jupyter Image is capable of interacting with the local machine Docker, allowing processing Containers to be created. Use example (Docker CLI ) Bellow is presented an example of how to use the research-processing-dagster Docker Image through the Docker CLI : Command format The following command was created to be didatic. If you desire to use it, don't forguet to replace its values and remove the blank spaces between lines. docker run \\ --name research-processing-dagster \\ # Environment variable --env DATA_DIRECTORY = /my-data-dir \\ # Volume: Data Volume --volume /my-data-dir:/my-data-dir \\ # Volume: Daemon Socket Volume --volume /var/run/docker.sock:/var/run/docker.sock:ro \\ # Network port in which the service will be accessed --publish 3000 :3000 \\ # Docker Image brazildatacube/research-processing-dagster:latest After executing the above command, a result similar to the showed bellow will be displayed: # (Omitted) Welcome to Dagster! If you have any questions or would like to engage with the Dagster team, please join us on Slack ( https://bit.ly/39dvSsF ) . Serving on http://0.0.0.0:3000 in process 1 After executting this command, use a web browser to access the DagIt adress presented: firefox http://127.0.0.1:3000 Use example (Docker Compose) Bellow, the same example performed using Docker CLI is presented using Docker Compose . First, the docker-compose.yml file was created: docker-compose.yml version: '3.2' services: my-dagster: image: brazildatacube/research-processing-dagster:latest environment: # Environmnet variables - DATA_DIRECTORY=/my-data-dir volumes: # Volume: Data volume - type: bind source: /my-data-dir target: /my-data-dir # Volume: Daemon Socket Volume - type: bind source: /var/run/docker.sock target: /var/run/docker.sock ports: # Network port in which the service will be accessed - \"3000:3000\" Once the file is created, the compose execution can be done: docker-compose -f docker-compose.yml up The output of the above command should look like: # (Omitted) Welcome to Dagster! If you have any questions or would like to engage with the Dagster team, please join us on Slack ( https://bit.ly/39dvSsF ) . Serving on http://0.0.0.0:3000 in process 1 Now accessing the base address http://0.0.0.0:3000 displayed on the screen, access Dagster through your browser: firefox http://127.0.0.1:3000 Example toolkit environment \u00b6 To easily use the Example toolkit , we created a example-toolkit-docker Docker Image . This Docker Image, has all the required depencies to execute the Example toolkit . On the following topics the required configuration to use this Docker image is demonstrated. Examples on how to use the Docker CLI and Docker Compose are also presented. Environment variables When using the Example toolkit , all the configurations are performed through environment variables. On the example-toolkit-docker , the same pattern was adopted. Thus, before executing this image, it is required to set a few environment variables. The same environment variables used for the Example toolkit are also valid for the example-toolkit-docker . To verify the complete list of the Example toolkit environment variables and their explanation, consults Section Example toolkit - Usage . Volumes The execution of the example-toolkit-docker , requires the definition of a few Docker Volumes. These volumes, specify the input, output, configuration and auxiliary data. Bellow these volumes are presented: Data volume (Required) Directory in which downloaded data will be stored. This volume must be created on the same directory defined by the DOWNLOAD_OUTPUT_DIRECTORY environment variable ( Example toolkit configuration). Dagster configuration volume (Required) Directory where the generated Dagster configuration file will be saved. This volume mus be created on the same directory defined by the PIPELINE_DIR environment variable ( Example toolkit configuration). Download configuration volume (Required) Configuration file containing information regarding the data that will be downloaded. The file defined in this volume must be the same defined by the DOWNLOAD_REFERENCE_FILE ( Example toolkit configuration). Use example (Docker CLI ) Bellow the Docker Image tagged as example-toolkit-docker is executed: Command format The following command was created to be didatic. If you desire to use it, don't forguet to replace its values and remove the blank spaces between lines. docker run \\ --name example-toolkit-docker \\ # Environment variable --env RAW_DATA_DIR = /compendium/data/raw_data \\ --env DERIVED_DATA_DIR = /compendium/data/derived_data \\ --env PIPELINE_DIR = /compendium/config \\ --env DOWNLOAD_OUTPUT_DIRECTORY = /compendium/data \\ --env DOWNLOAD_REFERENCE_FILE = /compendium/config/example-toolkit.json \\ # Volume: Data Volume --volume /my-data/dir:/compendium/data \\ # Volume: Dagster configuration volume --volume /my-dagster/dir:/compendium/config \\ # Volume: download configuration volume --volume /my-toolkit/config.json:/compendium/config/example-toolkit.json \\ # Docker Image brazildatacube/example-toolkit-docker:latest After executing the above command, a result similar to the showed bellow will be displayed: # (Omitted) 2022 -04-30 14 :59:16.525 | INFO | pipeline_steps:download_data_files_from_github:59 - Downloading minimal-example_landsat8_data.zip ( Sometimes the progress bar may not appear. Please, be patient. ) 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 2 .05G/2.05G [ 03 :43< 00 :00, 9 .17MB/s ] 2022 -04-30 15 :03:32.059 | INFO | pipeline_steps:download_data_files_from_github:59 - Downloading minimal-example_lasrc_auxiliary_data.zip ( Sometimes the progress bar may not appear. Please, be patient. ) 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 341M/341M [ 00 :35< 00 :00, 9 .57MB/s ] 2022 -04-30 15 :04:44.977 | INFO | pipeline_steps:download_data_files_from_github:59 - Downloading minimal-example_scene_id_list.zip ( Sometimes the progress bar may not appear. Please, be patient. ) 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 2 .17k/2.17k [ 00 :00< 00 :00, 1 .16MB/s ] 2022 -04-30 15 :04:45.690 | INFO | pipeline_steps:download_data_files_from_github:59 - Downloading minimal-example_sentinel2_data.zip ( Sometimes the progress bar may not appear. Please, be patient. ) 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 1 .14G/1.14G [ 02 :12< 00 :00, 8 .59MB/s ] 2022 -04-30 15 :07:15.765 | INFO | pipeline_steps:download_data_files_from_github:92 - All files are downloaded. Use example (Docker Compose) Bellow, the same example performed using Docker CLI is presented using Docker Compose . First, the docker-compose.yml file was created: docker-compose.yml version: '3.2' services: my-dagster: # Docker Image specification image: brazildatacube/example-toolkit-docker:latest environment: # VEnvironmnet variables - RAW_DATA_DIR=/compendium/data/raw_data - DERIVED_DATA_DIR=/compendium/data/derived_data - PIPELINE_DIR=/compendium/config - DOWNLOAD_OUTPUT_DIRECTORY=/compendium/data - DOWNLOAD_REFERENCE_FILE=/compendium/config/example-toolkit.json volumes: # Volume: Data volume - type: bind source: /my-data/dir target: /compendium/data # Volume: Dagster configuration volume - type: bind source: /my-dagster/dir target: /compendium/config # Volume: Download configuration volume - type: bind source: /my-toolkit/config.json target: /compendium/config/example-toolkit.json ports: # Network port in which the service will be accessed - \"3000:3000\" Once the file is created, the compose execution can be done: docker-compose -f docker-compose.yml up The output of the above command should look like: # (Omitted) Welcome to Dagster! If you have any questions or would like to engage with the Dagster team, please join us on Slack ( https://bit.ly/39dvSsF ) . Serving on http://0.0.0.0:3000 in process 1 Vagrant Virtual Machine \u00b6 This RC resources were developed, teste and used in Linux environment. Specifically, Ubuntu 20.04 . Tests using Ubuntu 20.10 were also performed. In theory, the executed codes can be adapted and used in other operational systems, e.g. Windows and MacOS . However, it is important to note that there is no warranty all commands, configurations and dependencies will be available for other environments. Even using Docker, specific characteristics, as Daemon Socket may not be available. To solve this and avoid that operational systems became a barrier to reproduce and replicate this RC material, we created a Vitrual Machine (VM). Using a VM, different from Docker, the whole system is virtualized. This VM was created using Vagrant , a tool for provisioning and managing VMs, developed by Hashicorp . Vagrant is available for Windows, Linux, MacOS and other operational systems. With this tool, one can use a description file ( Vagrantfile ), and specify a complete virtual machine, considering elements such as: RAM quantity; CPU quantity; Operational System; Network; Installed packages. Besides, many other configurations are available. Using these characteristics, in this RC we created a Vagrantfile that specifies a Ubuntu 20.04 VM, already prepared with the main dependencies required to use this RC 's materials (e.g., Docker, Docker Compose). The machine is created with 12 GB of RAM and 8 CPUs by default. VM Resources The ammount of resources define for the VM was considered using a machine with 24 GB of RAM and 12 CPUs as base. If necessary, the Vagrantfile can be used to change these values. To do that change the following properties in the file: vb.memory = \"12288\" # 12 GB vb.cpus = \"8\" Vagrant supports several Providers , which are tools to create the VMs. In this RC , we used the Open Source Provider VirtualBox . Vagrant Installation \u00b6 To use the VM through Vagrant, first you need to install Vagrant. For that it is recommended to use the official documentation . Using the VM through Vagrant \u00b6 Once Vagrant is installed in your system, to create the VM, the first step consinsts in clonning the repository that contains this RC 's materials: git clone https://github.com/brazil-data-cube/compendium-harmonization After clonning, enter the directory compendium-harmonization : cd compendium-harmonization And you will be able to see the RC 's materials: ls -lha #> -rwxrwxrwx 1 felipe felipe 368 Apr 9 20:01 .dockerignore #> drwxrwxrwx 1 felipe felipe 512 Apr 14 17:00 .git #> drwxrwxrwx 1 felipe felipe 512 Apr 15 22:53 .github #> -rwxrwxrwx 1 felipe felipe 4.3K Apr 10 08:42 .gitignore #> -rwxrwxrwx 1 felipe felipe 1.1K Apr 9 20:01 LICENSE #> -rwxrwxrwx 1 felipe felipe 2.7K Apr 30 18:38 Makefile #> -rwxrwxrwx 1 felipe felipe 4.5K Apr 9 20:01 README.md #> -rwxrwxrwx 1 felipe felipe 3.4K Apr 15 22:53 Vagrantfile #> drwxrwxrwx 1 felipe felipe 512 Apr 14 17:00 analysis #> -rwxrwxrwx 1 felipe felipe 1.4K Apr 10 08:19 bootstrap.sh #> drwxrwxrwx 1 felipe felipe 512 Apr 14 17:00 composes #> drwxrwxrwx 1 felipe felipe 512 Apr 14 17:00 docker #> -rwxrwxrwx 1 felipe felipe 383 Apr 10 07:39 setenv.sh #> drwxrwxrwx 1 felipe felipe 512 Apr 14 17:00 tools In these files, note that there is a Vagrantfile available. This file, has the specifications to create the VM. To create the VM with this file, use the following command: vagrant up #> Bringing machine 'default' up with 'virtualbox' provider... #> ==> default: Checking if box 'alvistack/ubuntu-20.04' version '20220415.1.1' is up to date... #> ==> default: A newer version of the box 'alvistack/ubuntu-20.04' for provider 'virtualbox' is #> ==> default: available! You currently have version '20220415.1.1'. The latest is version #> ==> default: '20220430.1.2'. Run `vagrant box update` to update. #> ==> default: Resuming suspended VM... #> ==> default: Booting VM... #> ==> default: Waiting for machine to boot. This may take a few minutes... #> default: SSH address: 127.0.0.1:2222 #> default: SSH username: vagrant #> default: SSH auth method: private key After this execution, the VM will be created and ready to be used. In this case, to access the VM, use the command: vagrant ssh #> Welcome to Ubuntu 20.04.4 LTS (GNU/Linux 5.13.0-39-generic x86_64) #> * Documentation: https://help.ubuntu.com #> * Management: https://landscape.canonical.com #> * Support: https://ubuntu.com/advantage # (Omitted) #> vagrant@ubuntu:~$ Once the environment is accessed, you will be ready to use this RC 's materials. For instance, if you desire to access the materials you made the clone to create the VM, you can access the /compendium directory: Changing directory cd /compendium Listing files ls -lha #> drwxrwxrwx 1 vagrant vagrant 0 Apr 14 20:00 analysis #> -rwxrwxrwx 1 vagrant vagrant 1.4K Apr 10 11:19 bootstrap.sh #> drwxrwxrwx 1 vagrant vagrant 0 Apr 14 20:00 composes #> drwxrwxrwx 1 vagrant vagrant 0 Apr 14 20:00 docker #> -rwxrwxrwx 1 vagrant vagrant 368 Apr 9 23:01 .dockerignore #> -rwxrwxrwx 1 vagrant vagrant 17 Apr 10 11:25 .env #> drwxrwxrwx 1 vagrant vagrant 0 Apr 16 01:53 .github #> -rwxrwxrwx 1 vagrant vagrant 4.3K Apr 10 11:42 .gitignore #> -rwxrwxrwx 1 vagrant vagrant 1.1K Apr 9 23:01 LICENSE #> -rwxrwxrwx 1 vagrant vagrant 2.7K Apr 30 21:38 Makefile #> -rwxrwxrwx 1 vagrant vagrant 4.5K Apr 9 23:01 README.md #> -rwxrwxrwx 1 vagrant vagrant 383 Apr 10 10:39 setenv.sh #> drwxrwxrwx 1 vagrant vagrant 4.0K Apr 14 20:00 tools #> drwxrwxrwx 1 vagrant vagrant 0 May 1 19:16 .vagrant #> -rwxrwxrwx 1 vagrant vagrant 3.4K Apr 16 01:53 Vagrantfile","title":"Computational environments"},{"location":"en/tools/environment/#computational-environments","text":"Behind each step on the processing scripts , as presented in the previous sections, there are several tools and software libraries being used. Some of these tools use special technologies to execute their operations, as is the case for the research-processing library , that uses Docker Containers to execute the processing functions in isolated environments. Other tools only use their own environment, as for the auxiliary script Example toolkit . In this case, it is required that its own environment to be configured to execute the script . In both presented scenaries, there are specific challenges on managing the used computational environment. For instance, specific configurations may be required on the software to operate alongside the research-processing library , while specific configurations may be required during the Example toolkit use. To solve these problems and avoud that the configuration used to interfere with reproductibility and replicability of the processing scripts created in this RC , all the environments required for using the tools were organized in Docker images. These, represent \"environment packages\" ready to use, in which all dependencies and required configurations are already set. In this section each Docker Image is presented, its characteristics, configuration and use. Note that these images were not created for a specific environment, as long as it supports Docker, any operational system can be used. However, in this documentation, Linux Ubuntu 20.04 syntax was adopted. Thus, changes can be required on the commands if you wish to use a different operational system, e.g. Windows. Changes between operational systems Although we believe that the commands and hints in this document can be used without trouble in Linux operational systems (e.g., Ubuntu, Debian) and MacOS, there is no warranty that this will always be true. Besides, for those who use Windows, changes in the commands may be required. To enable the use of the materials produced here even in those environments, we also created a Ubuntu 20.04 Virtual Machine, containing all required dependencies (e.g., Docker , Docker Compose ) to enable that all the commands presented here could be used. If you need to use this Virtual Machine, please, consult the Section Virtual Machine with Vagrant .","title":"Computational Environments"},{"location":"en/tools/environment/#docker-images","text":"In this RC , there are different types of environments that are being configured within the Docker Images, which can be categorized in two types: Executable Command Line Interfaces ( CLI ) are simple and direct to use, allowing automation during the processing steps. The Executable Docker Images are the images created to store a script that can be executed as a CLI . For that, this type of Docker Image has the following properties: Each Docker Image execution represents an individual execution of the tool it is associated; Parameters can be passed during a Docker Image execution. These parameters are used to configure the executed tool; Docker Volumes and environmental variables, also can be used to configure a Docker Image, being used to determine the inputs, outputs and configurations of the executed tool. Environment Different from the Executable Docker Images, these Docker Images are created to serve a complete environment that will be used to execute the tool, as a Jupyter Notebook or a Dagster Web Interface. The main difference between these two types of Docker Images created in this RC is its goals. While the Executables represent the executable tools, the Environment represent their environment, used to execute these specific tools. In the following Subsections, the Docker Images created in this RC are presented.","title":"Docker Images "},{"location":"en/tools/environment/#sen2cor-290","text":"Sen2cor is an atmospherical correction processor developed for Sentinel-2 products. As inputs it uses Top of Atmosphere (ToA) radiance Sentinel-2 products, also called Level-1C (L1C) and generates \"Bottom of the atmosphere (BOA) reflectance\" products, also providing a Scene Classification Layer (SCL), that has several classes, including cloud, cloud shadow and snow. More about Sen2Cor For more information regarding Sen2Cor, consult the oficial User Manual . We prepared a Docker Image with sen2cor installed to allow executions of it to be reproducible and reusable. This Docker Image, named sen2cor , has all its dependencies and configurations required in order to execute the Sen2Cor. Sen2cor Versions The Sen2Cor , is a software maintained by E uropean S pace A gency (ESA) and continues in development with new versions being released. In this RC , the Sen2cor version 2.9.0 was used. The following topics, present the main characterisctis of this Docker Image, as volumes, auxiliary data and how to use it. Auxiliary Data To execute the sen2cor , it is required to obtain some auxiliary data. The ESACCI-LC for Sen2Cor data package , which is used to identify clouds and classify a scene. It can be obtained following the steps listed bellow: Access the address: http://maps.elie.ucl.ac.be/CCI/viewer/download.php ; Sign in; Search for the ESACCI-LC for Sen2Cor data package ; Download this package ( zip file); Extract its content in a directory. It is recommended to name it CCI4SEN2COR . After extracting the files, the directory will contain the following files: ESACCI-LC-L4-WB-Map-150m-P13Y-2000-v4.0.tif (GeoTIFF); ESACCI-LC-L4-LCCS-Map-300m-P1Y-2015-v2.0.7.tif (GeoTIFF); ESACCI-LC-L4-Snow-Cond-500m-P13Y7D-2000-2012-v2.0 (Direct\u00f3rio). Volumes To use the Sen2cor , it is required to define a few Docker Volumes. These volumes specify the input, output, configuration files and auxiliary files used by the tool. Bellow these volumes are listed and descripted: Input data (Required) Directory containing the input files. This volume should map a directory in the local machine to the Container /mnt/input_dir directory. It is recommended to use a read-only volume, to ensure no modification to be made on the input data. Output directory (Required) Directory in which the output products will be stored. This volume should map a directory in the local machine to the Container /mnt/output_dir directory. Auxiliary data (Required) Directory containing the auxiliary files, required by Sen2cor. This volume should map a directory in the local machine to the Container /mnt/sen2cor-aux/CCI4SEN2COR . Configuration file (Optional) Volume that defines a configuration file ( L2A_GIPP.xml ). This volume should map a L2A_GIPP.xml file in the local machine to the Container /opt/sen2cor/2.9.0/cfg/L2A_GIPP.xml . SRTM data (Opcional) Volume to store SRTM data used by Sen2cor. This volume should map a directory in the local machine to the Container /mnt/sen2cor-aux/srtm . Use example (Docker CLI ) The following code presents, through Docker CLI , an example on how to use the Docker Image sen2cor to process a single scene. Image name On the following command, the Docker Image sen2cor is identified as marujore/sen2cor:2.9.0 stored in the user marujore on DockerHub, being the chosen version 2.9.0 . Command format The following command was created to be didatic. If you desire to use it, don't forguet to replace its values and remove the blank spaces between lines.O comando abaixo \u00e9 criado para ser dit\u00e1dico. docker run --rm \\ # Volume: Input data --volume /path/to/input_dir:/mnt/input_dir:ro \\ # Volume: Output Data --volume /path/to/output_dir:/mnt/output_dir:rw \\ # Auxiliary Data: Diret\u00f3rio CCI4SEN2COR --volume /path/to/CCI4SEN2COR:/mnt/aux_data \\ # Configuration file: L2A_GIPP.xml (Opcional) --volume /path/to/L2A_GIPP.xml:/opt/sen2cor/2.9.0/cfg/L2A_GIPP.xml \\ # SRTM Data (Opcional) --volume /path/to/srtm:/root/sen2cor/2.9/dem/srtm \\ # Docker Image and scene to be processed marujore/sen2cor:2.9.0 S2A_MSIL1C_20210903T140021_N0301_R067_T21KVR_20210903T172609.SAFE The execution of the command presented above will create a sen2cor Docker Container. This Docker Container will process the S2A_MSIL1C_20210903T140021_N0301_R067_T21KVR_20210903T172609.SAFE scene. Note that in this command, the input directory ( /path/to/input_dir ) must contain a subdirectory with the scene S2A_MSIL1C_20210903T140021_N0301_R067_T21KVR_20210903T172609.SAFE . For more information, consult the GitHub Repository , where the versioning of the changes in this Docker Image ( sen2cor ) has been keep.","title":"Sen2Cor 2.9.0"},{"location":"en/tools/environment/#lasrc-201","text":"LaSRC is an atmospheric correction processor, originally proposed for the Landsat-8 Collection 1, and posteriorly adapted to also correct Sentinel-2 products. It uses as input Landsat-8 Digital Number products (DN) or Sentinel-2 Top of Atmosphere (ToA) radiance products, also called Level-1C (L1C). As results this tool generates surface reflectance (SR) products. To facilitate the use of LaSRC in this RC , and ensure the executions to be reproducible and reusable, we created a Docker image for the LaSRC, called lasrc . The lasrc , has all dependencies and configuration required to execute the LaSRC processor. The following topics present the main characteristics of this Docker Image, as volumes, auxiliary data and how to use it. Auxiliary data To execute lasrc , it is required to define a few auxiliary data. To obtain then see the following steps: Access: https://edclpdsftp.cr.usgs.gov/downloads/auxiliaries/lasrc_auxiliary/L8/ ; Download all the available contents, except the LADS folder. LADS data is also required to use the LaSRC. However, this directory contains daily files since 2013 until nowadays, which is a huge volume of data. To perform this RC experiment, you can download the LADS accordingly to the dates of the data you will process. LADS files selection Each LADS file links to a day of the year. Thus, to process the image from January first 2017, one can obtain the LADS L8ANC2017001.hdf_fused in 2017, which is represented by 001 in the day of the year format. At the end of the acquisition, the auxiliary data directory must follow this structure: . \u251c\u2500\u2500 CMGDEM.hdf \u251c\u2500\u2500 LADS \u251c\u2500\u2500 LDCMLUT \u251c\u2500\u2500 MSILUT \u2514\u2500\u2500 ratiomapndwiexp.hdf Volumes To use the lasrc , it is required to define a few Docker Volumes. These volumes, specify the input data, output data and auxiliary files used by the tool during processing. Bellow, is descripted each volume that must be created during the execution of LaSRC Docker Image: Input data (Required) Directory containing the input data. This volume should map a directory in the local machine to the Container /mnt/input_dir directory. It is recommended to use a read-only volume, to ensure no modification to be made on the input data. Output data (Required) Directory containing the output data. This volume should map a directory in the local machine to the Container /mnt/output_dir directory. Auxiliary data (Required) Directory containing the auxiliary data required by LaSRC. The created volume, should map a directory in the local machine to the Container /mnt/atmcor_aux/lasrc/L8 directory. Use example (Docker CLI ) The codes bellow present two examples on how to use lasrc , through the Docker CLI . On the first example, a Landsat-8/OLI scene is processed, while on the second a Sentinel-2/MSI image is processed. Image name On the following commands, a lasrc Docker Image is identified as marujore/lasrc:2.0.1 , stored in the profile marujore on DockerHub. Command format The following command was created to be didatic. If you desire to use it, don't forguet to replace its values and remove the blank spaces between lines.O comando abaixo \u00e9 criado para ser dit\u00e1dico. LaSRC Landsat-8/OLI example docker run --rm \\ # Volume: Input data --volume /path/to/input/:/mnt/input-dir:rw \\ # Volume: Output data --volume /path/to/output:/mnt/output-dir:rw \\ # auxiliary data (data L8/LADS) --volume /path/to/lasrc_auxiliaries/L8:/mnt/atmcor_aux/lasrc/L8:ro \\ # Docker Image and scene to be processed --tty brazildatacube/lasrc:2.0.1 LC08_L1TP_220069_20190112_20190131_01_T1 LaSRC Sentinel-2/MSI example docker run --rm \\ # Volume: Input data --volume /path/to/input/:/mnt/input-dir:rw \\ # Volume: Output data --volume /path/to/output:/mnt/output-dir:rw \\ # auxiliary data (data L8/LADS) --volume /path/to/lasrc_auxiliaries/L8:/mnt/atmcor_aux/lasrc/L8:ro \\ # Docker Image and scene to be processed --tty brazildatacube/lasrc:2.0.1 S2A_MSIL1C_20190105T132231_N0207_R038_T23LLF_20190105T145859.SAFE As can be noted, the difference on using lasrc for different satellite-sensors relly only on the sceneid name. It is important to note that, for both cases, the input directory ( /path/to/input/ ) must contain the scenes to be processed. In this case LC08_L1TP_220069_20190112_20190131_01_T1 and S2A_MSIL1C_20190105T132231_N0207_R038_T23LLF_20190105T145859.SAFE . For more information, consult the Github repository , where the versioning of the changes in this Docker Image ( lasrc ) has been keep.","title":"LaSRC 2.0.1"},{"location":"en/tools/environment/#l8angs","text":"Landsat Ang Tool is a tool developed and maintained by U nited S tates G eological S urvey . This tool uses the ANG.txt that are provided alongside Landsat-8 images and use them to generate per pixel angle bands. The angle bands are solar azimuth angle ( SAA ), solar zenithal angle ( SZA ), view azimuth angle ( VAA ) and view zenithal angle ( VZA ). The bands are generated with the same resolution as OLI soectral bands (30m). More about Landsat Ang Tool For more information regarding Landsat Ang Tool, consult the USGS official page about the tool . In this RC , the Landsat-8/OLI (Collection-2) images were obtained already processed to Surface Reflectance Level (L2). However, for further processing we use the angle bands, using the Landsat Ang Tool . The installation and configuration of the Landsat Ang Tool can make hard to replicate and reproduce this experiment in the future. Due to that, in this RC we created a Docker Image, named l8angs , containing the tool. The following topics present the main characteristics of this Docker Image, as volumes and auxiliary data to its execution, as well as Docker CLI execution examples. Volumes To use l8angs , you must provide the following volume during the execution: Input data (Required) The created volume, should map a directory in the local machine to the Container /mnt/input-dir directory. The angle bands are generated on the same input directory, this is the standard behavior of the tool. Use example (Docker CLI ) The code bellow, presents a use example of the l8angs through Docker CLI . Image name In the following commands, the l8angs Docker image, identified as marujore/landsat-angles:latest on DockerHub. Command format The following command was created to be didatic. If you desire to use it, don't forguet to replace its values and remove the blank spaces between lines.O comando abaixo \u00e9 criado para ser dit\u00e1dico. docker run --rm \\ # Volume: Input data -v /path/to/input/:/mnt/input-dir:rw \\ # Docker Image and scene to be processed marujore/landsat-angles:latest LC08_L2SP_222081_20190502_20200829_02_T1 The execution of the command presented above will create a l8angs Docker Container. This Docker Container will process the LC08_L2SP_222081_20190502_20200829_02_T1 scene. In this command, the input dir ( /path/to/input/ ) must contain a subdirectory named LC08_L2SP_222081_20190502_20200829_02_T1 , which is the scene being processed. For more information, consult the GitHub repository , which contains the versioning and changes made on l8angs .","title":"L8Angs"},{"location":"en/tools/environment/#sensor-harm","text":"In this RC , the Landsat-8 Collection-2 images were already obtained as surface reflectance products (L2) while Sentinel-2 images were obtained as L1C and processed using Sen2cor or LaSRC. Both Landsat-8 and Sentinel-2 images are harmonized using the sensor-harm library. To allow reproductibility and replicability of this tool use, we created a Docker Image called sensor-harm . In this image, all dependencies and configurations to execute sensor-harm are already prepared. The following topics present the main characteristics of this Docker Image, the volumes it requires and use examples. Volumes To use the sensor-harm , it is required to define some Docker Volumes. These volumes specify the input and auxiliary data used by sensor-harm. Bellow a list of these volumes is listed and descripted: Input data (Required) Directory containing the input data. This volume should map a directory in the local machine to the Container /mnt/input_dir directory. It is recommended to use a read-only volume, to ensure no modification to be made on the input data. Output data (Required) Directory containing the output data. This volume should map a directory in the local machine to the Container /mnt/output_dir directory. Angle directory (Required only for Landsat-8/OLI) Directory containing the angles of the scene that will be processed. The created volume, should map a directory in the local machine to the Container /mnt/angles-dir directory. It is recommended to use a read-only volume, to ensure no modification to be made on the input data. Use example (Docker CLI ) The codes bellow present two examples on how to use sensor-harm through Docker CLI . In the first example, the processing is performed on a Landsat-8/OLI scene, while on the second a Sentinel-2/MSI scene is used. Image name On the following commands, a sensor-harm Docker Image is identified as marujore/sensor-harm:latest , stored in the profile marujore on DockerHub. Command format The following command was created to be didatic. If you desire to use it, don't forguet to replace its values and remove the blank spaces between lines.O comando abaixo \u00e9 criado para ser dit\u00e1dico. Landsat-8/OLI example docker run --rm \\ # Volume: Input data --volume /path/to/input/:/mnt/input-dir:ro \\ # Volume: Output data --volume /path/to/output:/mnt/output-dir:rw \\ # Angle directory (Only for Landsat-8/OLI) --volume /path/to/angles:/mnt/angles-dir:ro \\ # Docker Image and scene to be processed --tty marujore/sensor-harm:latest LC08_L1TP_220069_20190112_20190131_01_T1 Sentinel-2/MSI example docker run --rm \\ # Volume: Input data --volume /path/to/input/:/mnt/input-dir:ro \\ # Volume: Output data --volume /path/to/output:/mnt/output-dir:rw \\ # Docker Image and scene to be processed --tty brazildatacube/sensor-harm:latest S2A_MSIL1C_20190105T132231_N0207_R038_T23LLF_20190105T145859.SAFE As can be noted, the difference on using sensor-harm for different satellite-sensors relly only on the sceneid name. It is important to note that, for both cases, the input directory ( /path/to/input/ ) must contain the scenes to be processed. In this case LC08_L1TP_220069_20190112_20190131_01_T1 and S2A_MSIL1C_20190105T132231_N0207_R038_T23LLF_20190105T145859.SAFE For more information, consult the Github repository , where the versioning of the changes in this Docker Image ( sensor-harm ) has been keep.","title":"Sensor Harm"},{"location":"en/tools/environment/#processing-scripts-docker-images","text":"As presented in the Processing scripts section, two environments were created to execute this RC methodology experiments. Oe uses Jupyter Notebook , which is useful for interactive processing of the codes. The second, using Dagster , which is useful for batch execution and error control. To facilitate the use of both approaches, Docker Images were created containing the required environments of each execution. This avoids dependencies to be installed or configured to execute the processing scripts . The following topics present the Docker Images characteristics, required volumes, configuration and use example. Scripts behavior If you wish to reuse these Docker Images, it is recomended first to read about how is the processing scripts behavior , as well as the software libraries used by these scripts .","title":"Processing scripts Docker Images"},{"location":"en/tools/environment/#jupyter-notebook","text":"To execute the Jupyter Notebook version, we created the research-processing-jupyter Docker Image. This Docker Image brings a JupyterLab , with the required Python dependencies to execute the scripts . Besides, in this Docker Image, Docker is also installed, allowing the scripts to operate and create other processing Docker Containers. Environment Base The creation of research-processing-jupyter was made using the jupyter/minimal-notebook Docker Image, made available by the Jupyter development team. So, all the environment variables and configurations available on the jupyter/minimal-notebook Docker Image are also applyable to research-processing-jupyter . On the following topics the required configuration to use this Docker image is demonstrated. Examples on how to use the Docker CLI and Docker Compose are also presented. Environment variables To use the research-processing-jupyter , it is required to define the following environment variable: DATA_DIRECTORY (Required) Environmental variable that determines the directory, in the local machine, in which the downloaded data will be saved. Volumes The execution of the research-processing-jupyter requires two volumes to be mounted: Data Volume (Required) Volume containing the data. Following the processing function execution model used in the scripts , this volume will be used by functions inside the container ( Local ) or in other containers ( Containerized ). Thus, the volume mount must attend 2 requirements: The volume must be a Bind Mount ; The volume mapping ( Bind Mount ) must have, in the local machine and in the Container, the same path defined in DATA_DIRECTORY . With these definitions, the volume will be visible within the research-processing-jupyter Container and also by the auxiliary processing Containers generated during the processing script execution. Daemon Socket Volume (Required) To allow the scripts to generate processing Docker Containers, it is required to define the Daemon Socket as a volume. Doing this, the Docker within the container created with the research-processing-jupyter Image is capable of interacting with the local machine Docker, allowing processing Containers to be created. User definition Complementing the Daemon Socket volume definition, to execute the research-processing-jupyter , it is required to specify the user ( UID ) and group ( GID ) on the local machine that has permission to interact with the o Docker Daemon. These values will be applied to the Container standard user so Docker can allow it to also interact with the Docker Daemon of the local machine. Docker permission If you are intereseted in understanding the detail behind this user definition, we recommend that you consult the oficial Docker documentation . To define the user duering research-processing-jupyter execution, you can use the parameter --user . If you with to use the Docker Compose, the field user can be used for this definition. Use example (Docker CLI ) Bellow is presented an example of how to use the research-processing-jupyter Docker Image through the Docker CLI : Command format The following command was created to be didatic. If you desire to use it, don't forguet to replace its values and remove the blank spaces between lines. docker run \\ --name research-processing-jupyter \\ # User definition --user ${ UID } : ${ GID } \\ # Environment variable --env JUPYTER_ENABLE_LAB = yes \\ # Activating JupyterLab --env DATA_DIRECTORY = /my-data-dir \\ # Volume: Data Volume --volume /my-data-dir:/my-data-dir \\ # Volume: Daemon Socket Volume --volume /var/run/docker.sock:/var/run/docker.sock:ro \\ # Network port in which the service will be accessed --publish 8888 :8888 \\ # Docker Image brazildatacube/research-processing-jupyter:latest User definition To define a user, using the environment variables ( ${UID} e ${GID} ), as in the previous command, before executing the Docker command use the following commands: export UID = ` id -u $USER ` export GID = ` cut -d: -f3 < < ( getent group docker ) ` After the execution of the above command, a result simillar to the bellow should be presented: # (Omitted) [ I 2022 -04-30 19 :22:50.684 ServerApp ] Use Control-C to stop this server and shut down all kernels ( twice to skip confirmation ) . [ C 2022 -04-30 19 :22:50.694 ServerApp ] To access the server, open this file in a browser: file:///home/jovyan/.local/share/jupyter/runtime/jpserver-7-open.html Or copy and paste one of these URLs: http://ae88466ccb18:8888/lab?token = 0497e15e042d52cfb498a2edf3d2c6e5874e79b4808ca860 or http://127.0.0.1:8888/lab?token = 0497e15e042d52cfb498a2edf3d2c6e5874e79b4808ca860 After executing this command, using a web browser and accessing the presented JupyterLab address (Replace the address bellow by what has been showed in your terminal): firefox http://127.0.0.1:8888/lab?token = 0497e15e042d52cfb498a2edf3d2c6e5874e79b4808ca860 Use example (Docker Compose) Bellow, the same example performed using Docker CLI is presented using Docker Compose . First, the docker-compose.yml file was created: docker-compose.yml version: '3.2' services: my-notebook: # User definition user: ${UID}:${GID} image: brazildatacube/research-processing-jupyter:latest environment: # Environment variable - JUPYTER_ENABLE_LAB=yes - DATA_DIRECTORY=/my-data-dir volumes: # Volume: Data volume - type: bind source: /my-data-dir target: /my-data-dir # Volume: Daemon Socket Volume - type: bind source: /var/run/docker.sock target: /var/run/docker.sock ports: # Network port in which the service will be accessed - \"8888:8888\" User definition To define a user, using the environment variables ( ${UID} e ${GID} ), as presented in the docker-compose.yml , before executing the Docker command use the following commands: export UID = ` id -u $USER ` export GID = ` cut -d: -f3 < < ( getent group docker ) ` With the file created, the compose can be executed: docker-compose -f docker-compose.yml up The output of the above command should be similar to: # (Omitted) [ I 2022 -04-30 19 :23:57.260 ServerApp ] http://afd0fe2755a7:8888/lab?token = 6d37701a6787bd58a7f92f29f33709970df11a742bf3b79b [ I 2022 -04-30 19 :23:57.260 ServerApp ] or http://127.0.0.1:8888/lab?token = 6d37701a6787bd58a7f92f29f33709970df11a742bf3b79b [ I 2022 -04-30 19 :23:57.260 ServerApp ] Use Control-C to stop this server and shut down all kernels ( twice to skip confirmation ) . [ C 2022 -04-30 19 :23:57.264 ServerApp ] To access the server, open this file in a browser: file:///home/jovyan/.local/share/jupyter/runtime/jpserver-8-open.html Or copy and paste one of these URLs: http://afd0fe2755a7:8888/lab?token = 6d37701a6787bd58a7f92f29f33709970df11a742bf3b79b or http://127.0.0.1:8888/lab?token = 6d37701a6787bd58a7f92f29f33709970df11a742bf3b79b With this information, the JupyterLab can be accesse in the browser. For that open the link displayed in your terminal on your browser: firefox http://127.0.0.1:8888/lab?token = 6d37701a6787bd58a7f92f29f33709970df11a742bf3b79b","title":"Jupyter Notebook"},{"location":"en/tools/environment/#dagster","text":"To execute the Dagster version, the research-processing-dagster Docker Image was created. This Docker Image has Dagster (version 0.12.15 ), alongside DagIt , a web interface to configure and interact with Dagster. It also has a Docker installed, to allow scripts to create and operate other processing Docker Containers. Environment variables To use the research-processing-dagster , it is required to define the following environmental variable: DATA_DIRECTORY (Required) Environmental variable that determines the directory, in the local machine, in which the downloaded data will be saved. Volumes The execution of the research-processing-dagster requires the definition of the following Docker volumes : Data volume (Required) Volume containing the data. Following the processing function execution model used in the scripts , this volume will be used by functions inside the container ( Local ) or in other containers ( Containerized ). Thus, the volume mount must attend 2 requirements: The volume must be a Bind Mount ; The volume mapping ( Bind Mount ) must have, in the local machine and in the Container, the same path defined in DATA_DIRECTORY . With these definitions, the volume will be visible within the research-processing-jupyter Container and also by the auxiliary processing Containers generated during the processing script execution. Daemon Socket Volume (Required) To allow the scripts to generate processing Docker Containers, it is required to define the Daemon Socket as a volume. Doing this, the Docker within the container created with the research-processing-jupyter Image is capable of interacting with the local machine Docker, allowing processing Containers to be created. Use example (Docker CLI ) Bellow is presented an example of how to use the research-processing-dagster Docker Image through the Docker CLI : Command format The following command was created to be didatic. If you desire to use it, don't forguet to replace its values and remove the blank spaces between lines. docker run \\ --name research-processing-dagster \\ # Environment variable --env DATA_DIRECTORY = /my-data-dir \\ # Volume: Data Volume --volume /my-data-dir:/my-data-dir \\ # Volume: Daemon Socket Volume --volume /var/run/docker.sock:/var/run/docker.sock:ro \\ # Network port in which the service will be accessed --publish 3000 :3000 \\ # Docker Image brazildatacube/research-processing-dagster:latest After executing the above command, a result similar to the showed bellow will be displayed: # (Omitted) Welcome to Dagster! If you have any questions or would like to engage with the Dagster team, please join us on Slack ( https://bit.ly/39dvSsF ) . Serving on http://0.0.0.0:3000 in process 1 After executting this command, use a web browser to access the DagIt adress presented: firefox http://127.0.0.1:3000 Use example (Docker Compose) Bellow, the same example performed using Docker CLI is presented using Docker Compose . First, the docker-compose.yml file was created: docker-compose.yml version: '3.2' services: my-dagster: image: brazildatacube/research-processing-dagster:latest environment: # Environmnet variables - DATA_DIRECTORY=/my-data-dir volumes: # Volume: Data volume - type: bind source: /my-data-dir target: /my-data-dir # Volume: Daemon Socket Volume - type: bind source: /var/run/docker.sock target: /var/run/docker.sock ports: # Network port in which the service will be accessed - \"3000:3000\" Once the file is created, the compose execution can be done: docker-compose -f docker-compose.yml up The output of the above command should look like: # (Omitted) Welcome to Dagster! If you have any questions or would like to engage with the Dagster team, please join us on Slack ( https://bit.ly/39dvSsF ) . Serving on http://0.0.0.0:3000 in process 1 Now accessing the base address http://0.0.0.0:3000 displayed on the screen, access Dagster through your browser: firefox http://127.0.0.1:3000","title":"Dagster"},{"location":"en/tools/environment/#example-toolkit-environment","text":"To easily use the Example toolkit , we created a example-toolkit-docker Docker Image . This Docker Image, has all the required depencies to execute the Example toolkit . On the following topics the required configuration to use this Docker image is demonstrated. Examples on how to use the Docker CLI and Docker Compose are also presented. Environment variables When using the Example toolkit , all the configurations are performed through environment variables. On the example-toolkit-docker , the same pattern was adopted. Thus, before executing this image, it is required to set a few environment variables. The same environment variables used for the Example toolkit are also valid for the example-toolkit-docker . To verify the complete list of the Example toolkit environment variables and their explanation, consults Section Example toolkit - Usage . Volumes The execution of the example-toolkit-docker , requires the definition of a few Docker Volumes. These volumes, specify the input, output, configuration and auxiliary data. Bellow these volumes are presented: Data volume (Required) Directory in which downloaded data will be stored. This volume must be created on the same directory defined by the DOWNLOAD_OUTPUT_DIRECTORY environment variable ( Example toolkit configuration). Dagster configuration volume (Required) Directory where the generated Dagster configuration file will be saved. This volume mus be created on the same directory defined by the PIPELINE_DIR environment variable ( Example toolkit configuration). Download configuration volume (Required) Configuration file containing information regarding the data that will be downloaded. The file defined in this volume must be the same defined by the DOWNLOAD_REFERENCE_FILE ( Example toolkit configuration). Use example (Docker CLI ) Bellow the Docker Image tagged as example-toolkit-docker is executed: Command format The following command was created to be didatic. If you desire to use it, don't forguet to replace its values and remove the blank spaces between lines. docker run \\ --name example-toolkit-docker \\ # Environment variable --env RAW_DATA_DIR = /compendium/data/raw_data \\ --env DERIVED_DATA_DIR = /compendium/data/derived_data \\ --env PIPELINE_DIR = /compendium/config \\ --env DOWNLOAD_OUTPUT_DIRECTORY = /compendium/data \\ --env DOWNLOAD_REFERENCE_FILE = /compendium/config/example-toolkit.json \\ # Volume: Data Volume --volume /my-data/dir:/compendium/data \\ # Volume: Dagster configuration volume --volume /my-dagster/dir:/compendium/config \\ # Volume: download configuration volume --volume /my-toolkit/config.json:/compendium/config/example-toolkit.json \\ # Docker Image brazildatacube/example-toolkit-docker:latest After executing the above command, a result similar to the showed bellow will be displayed: # (Omitted) 2022 -04-30 14 :59:16.525 | INFO | pipeline_steps:download_data_files_from_github:59 - Downloading minimal-example_landsat8_data.zip ( Sometimes the progress bar may not appear. Please, be patient. ) 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 2 .05G/2.05G [ 03 :43< 00 :00, 9 .17MB/s ] 2022 -04-30 15 :03:32.059 | INFO | pipeline_steps:download_data_files_from_github:59 - Downloading minimal-example_lasrc_auxiliary_data.zip ( Sometimes the progress bar may not appear. Please, be patient. ) 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 341M/341M [ 00 :35< 00 :00, 9 .57MB/s ] 2022 -04-30 15 :04:44.977 | INFO | pipeline_steps:download_data_files_from_github:59 - Downloading minimal-example_scene_id_list.zip ( Sometimes the progress bar may not appear. Please, be patient. ) 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 2 .17k/2.17k [ 00 :00< 00 :00, 1 .16MB/s ] 2022 -04-30 15 :04:45.690 | INFO | pipeline_steps:download_data_files_from_github:59 - Downloading minimal-example_sentinel2_data.zip ( Sometimes the progress bar may not appear. Please, be patient. ) 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 1 .14G/1.14G [ 02 :12< 00 :00, 8 .59MB/s ] 2022 -04-30 15 :07:15.765 | INFO | pipeline_steps:download_data_files_from_github:92 - All files are downloaded. Use example (Docker Compose) Bellow, the same example performed using Docker CLI is presented using Docker Compose . First, the docker-compose.yml file was created: docker-compose.yml version: '3.2' services: my-dagster: # Docker Image specification image: brazildatacube/example-toolkit-docker:latest environment: # VEnvironmnet variables - RAW_DATA_DIR=/compendium/data/raw_data - DERIVED_DATA_DIR=/compendium/data/derived_data - PIPELINE_DIR=/compendium/config - DOWNLOAD_OUTPUT_DIRECTORY=/compendium/data - DOWNLOAD_REFERENCE_FILE=/compendium/config/example-toolkit.json volumes: # Volume: Data volume - type: bind source: /my-data/dir target: /compendium/data # Volume: Dagster configuration volume - type: bind source: /my-dagster/dir target: /compendium/config # Volume: Download configuration volume - type: bind source: /my-toolkit/config.json target: /compendium/config/example-toolkit.json ports: # Network port in which the service will be accessed - \"3000:3000\" Once the file is created, the compose execution can be done: docker-compose -f docker-compose.yml up The output of the above command should look like: # (Omitted) Welcome to Dagster! If you have any questions or would like to engage with the Dagster team, please join us on Slack ( https://bit.ly/39dvSsF ) . Serving on http://0.0.0.0:3000 in process 1","title":"Example toolkit environment"},{"location":"en/tools/environment/#vagrant-virtual-machine","text":"This RC resources were developed, teste and used in Linux environment. Specifically, Ubuntu 20.04 . Tests using Ubuntu 20.10 were also performed. In theory, the executed codes can be adapted and used in other operational systems, e.g. Windows and MacOS . However, it is important to note that there is no warranty all commands, configurations and dependencies will be available for other environments. Even using Docker, specific characteristics, as Daemon Socket may not be available. To solve this and avoid that operational systems became a barrier to reproduce and replicate this RC material, we created a Vitrual Machine (VM). Using a VM, different from Docker, the whole system is virtualized. This VM was created using Vagrant , a tool for provisioning and managing VMs, developed by Hashicorp . Vagrant is available for Windows, Linux, MacOS and other operational systems. With this tool, one can use a description file ( Vagrantfile ), and specify a complete virtual machine, considering elements such as: RAM quantity; CPU quantity; Operational System; Network; Installed packages. Besides, many other configurations are available. Using these characteristics, in this RC we created a Vagrantfile that specifies a Ubuntu 20.04 VM, already prepared with the main dependencies required to use this RC 's materials (e.g., Docker, Docker Compose). The machine is created with 12 GB of RAM and 8 CPUs by default. VM Resources The ammount of resources define for the VM was considered using a machine with 24 GB of RAM and 12 CPUs as base. If necessary, the Vagrantfile can be used to change these values. To do that change the following properties in the file: vb.memory = \"12288\" # 12 GB vb.cpus = \"8\" Vagrant supports several Providers , which are tools to create the VMs. In this RC , we used the Open Source Provider VirtualBox .","title":"Vagrant Virtual Machine "},{"location":"en/tools/environment/#vagrant-installation","text":"To use the VM through Vagrant, first you need to install Vagrant. For that it is recommended to use the official documentation .","title":"Vagrant Installation"},{"location":"en/tools/environment/#using-the-vm-through-vagrant","text":"Once Vagrant is installed in your system, to create the VM, the first step consinsts in clonning the repository that contains this RC 's materials: git clone https://github.com/brazil-data-cube/compendium-harmonization After clonning, enter the directory compendium-harmonization : cd compendium-harmonization And you will be able to see the RC 's materials: ls -lha #> -rwxrwxrwx 1 felipe felipe 368 Apr 9 20:01 .dockerignore #> drwxrwxrwx 1 felipe felipe 512 Apr 14 17:00 .git #> drwxrwxrwx 1 felipe felipe 512 Apr 15 22:53 .github #> -rwxrwxrwx 1 felipe felipe 4.3K Apr 10 08:42 .gitignore #> -rwxrwxrwx 1 felipe felipe 1.1K Apr 9 20:01 LICENSE #> -rwxrwxrwx 1 felipe felipe 2.7K Apr 30 18:38 Makefile #> -rwxrwxrwx 1 felipe felipe 4.5K Apr 9 20:01 README.md #> -rwxrwxrwx 1 felipe felipe 3.4K Apr 15 22:53 Vagrantfile #> drwxrwxrwx 1 felipe felipe 512 Apr 14 17:00 analysis #> -rwxrwxrwx 1 felipe felipe 1.4K Apr 10 08:19 bootstrap.sh #> drwxrwxrwx 1 felipe felipe 512 Apr 14 17:00 composes #> drwxrwxrwx 1 felipe felipe 512 Apr 14 17:00 docker #> -rwxrwxrwx 1 felipe felipe 383 Apr 10 07:39 setenv.sh #> drwxrwxrwx 1 felipe felipe 512 Apr 14 17:00 tools In these files, note that there is a Vagrantfile available. This file, has the specifications to create the VM. To create the VM with this file, use the following command: vagrant up #> Bringing machine 'default' up with 'virtualbox' provider... #> ==> default: Checking if box 'alvistack/ubuntu-20.04' version '20220415.1.1' is up to date... #> ==> default: A newer version of the box 'alvistack/ubuntu-20.04' for provider 'virtualbox' is #> ==> default: available! You currently have version '20220415.1.1'. The latest is version #> ==> default: '20220430.1.2'. Run `vagrant box update` to update. #> ==> default: Resuming suspended VM... #> ==> default: Booting VM... #> ==> default: Waiting for machine to boot. This may take a few minutes... #> default: SSH address: 127.0.0.1:2222 #> default: SSH username: vagrant #> default: SSH auth method: private key After this execution, the VM will be created and ready to be used. In this case, to access the VM, use the command: vagrant ssh #> Welcome to Ubuntu 20.04.4 LTS (GNU/Linux 5.13.0-39-generic x86_64) #> * Documentation: https://help.ubuntu.com #> * Management: https://landscape.canonical.com #> * Support: https://ubuntu.com/advantage # (Omitted) #> vagrant@ubuntu:~$ Once the environment is accessed, you will be ready to use this RC 's materials. For instance, if you desire to access the materials you made the clone to create the VM, you can access the /compendium directory: Changing directory cd /compendium Listing files ls -lha #> drwxrwxrwx 1 vagrant vagrant 0 Apr 14 20:00 analysis #> -rwxrwxrwx 1 vagrant vagrant 1.4K Apr 10 11:19 bootstrap.sh #> drwxrwxrwx 1 vagrant vagrant 0 Apr 14 20:00 composes #> drwxrwxrwx 1 vagrant vagrant 0 Apr 14 20:00 docker #> -rwxrwxrwx 1 vagrant vagrant 368 Apr 9 23:01 .dockerignore #> -rwxrwxrwx 1 vagrant vagrant 17 Apr 10 11:25 .env #> drwxrwxrwx 1 vagrant vagrant 0 Apr 16 01:53 .github #> -rwxrwxrwx 1 vagrant vagrant 4.3K Apr 10 11:42 .gitignore #> -rwxrwxrwx 1 vagrant vagrant 1.1K Apr 9 23:01 LICENSE #> -rwxrwxrwx 1 vagrant vagrant 2.7K Apr 30 21:38 Makefile #> -rwxrwxrwx 1 vagrant vagrant 4.5K Apr 9 23:01 README.md #> -rwxrwxrwx 1 vagrant vagrant 383 Apr 10 10:39 setenv.sh #> drwxrwxrwx 1 vagrant vagrant 4.0K Apr 14 20:00 tools #> drwxrwxrwx 1 vagrant vagrant 0 May 1 19:16 .vagrant #> -rwxrwxrwx 1 vagrant vagrant 3.4K Apr 16 01:53 Vagrantfile","title":"Using the VM through Vagrant"},{"location":"en/tools/libraries/","text":"Software Libraries \u00b6 Reproducibility allows us to retrace the path that was initially taken to obtain the results of scientific research. With this, other researchers and even our future selves can benefit and understand what we have done. The reproducibility also enables us to detect and correct possible errors in the results. Among the many actions that can be taken to achieve reproducibility is the ability to automate the steps already performed. This brings several benefits: Avoids possible \"mistakes\" by reading incorrect results that are no longer valid for the current run; It allows the verification of the entire processing flow (which also helps in the writing of the report/article); Decreased overhead of running the job. A few years ago, this feature was limited in research due to the use of software with graphical interfaces that ended up not allowing the automation of the work done through \"buttons and clicks\" on the screen. Nowadays, with the increase and dissemination of high-level languages for research development, such as R and Python , this has completely changed. We can automate tasks more efficiently. Also, from the moment a processing script is created, all the logic applied to the data is described clearly and directly. In this RC , several Python code libraries were created so that all steps could be modeled through scripts. Each library created has a unique responsibility, which helped us keep the work organized during its development. At the same time, this approach facilitates the reuse of the code created. Those who wish to reuse the work developed can import these libraries into their Python project. Available code libraries \u00b6 To adopt this development approach, based on single responsibility libraries that could be reused, it was necessary to adopt some criteria that would help us keep everything organized and reusable. Also, our vision for creating multiple libraries for the composition of the scripts for processing the work is that these should be modeled in a way that they can be used together. Like blocks that can be put together to build a wall. Considering these characteristics, initially, we defined that in this RC , we would develop two types of libraries: Base Libraries that provide the base features and functionality for performing an action (e.g., Production of angle bands) Application Libraries that, through the union of Base libraries, provide functionalities that different methodologies and processing flows (e.g., Generation of harmonized products). Based on these definitions, for the production of this RC and the generation of its results, we developed two Base libraries: s2-angs Library create to provide functionality for generating angle bands from Sentinel-2/MSI data; sensor-harm Library create to generate harmonized produts from Sentinel-2/MSI and Landsat-8/OLI data; These libraries are available and can be installed like any other Python language library and used in different projects. So, if you are interested, you can, for example, install the sensor-harm library in your Python project and utilize the functionality provided for generating harmonized products. For both libraries, the only restriction is to respect the input formats expected by the library functions. If followed correctly, you should have no problem using them. Based on these two libraries, we created an Application library: research-processing Library created to provide functionality that allows the application of the data processing methodology used to generate the results of this RC . Part of its functionality is built upon the s2-angs and sensor-harm libraries. The relationship between these libraries is summarized in the Figure below. Libraries organization Details of the operation and functionality of each of these libraries are presented in the following sections. Libraries in the processing methodology \u00b6 The figure below gives you an overview of where the libraries presented above are used in the processing methodology: Processing workflow with libraries Libraries specification \u00b6 In this section, as a complement to the overview presented so far, the features and usage example of each of the aforementioned libraries are specified. Sentinel-2 Angle Generator Python Library (s2-angs) \u00b6 The s2-angs library, as mentioned earlier, is responsible for generating angle bands for Sentinel-2 imagery. These bands contain per-pixel information for solar azimuthal ( SAA ), solar zenithal ( SZA ), azimuthal sensor ( VAA ), and zenithal sensor ( VZA ) angles. This information is extracted from the Sentinel-2 image metadata. Initially, this data is provided as a 23x23 (rows X columns) matrix, i.e., at a spatial resolution of about 5000 meters. However, this information needs to be at a spatial resolution equivalent to that of the spectral bands of the sensor ( 10 , 20 or 60 meters) to take advantage of per-pixel corrections. Thus, the s2-angs library can estimate angles and save them as .tif files, either at their original spatial resolution or resampled to the spatial resolution of the sensor bands. So, we can list as main features of this library: Generation of angle bands ( SAA , SZA , VAA and VZA ); Resampling the angle bands to the sensor band resolution. Operations available \u00b6 The table below gives a summary of the operations that are available in the s2-angs library. Function Description s2_angs.gen_s2_ang Function to generate the Sentinel-2 Angle bands Usage example \u00b6 Using the s2_angs library, use the gen_s2_ang function to perform angle band generation. This function accepts as input .zip , .SAFE directory or directory with Sentinel-2 images . In the code block below is an example where a .zip file is used as input to the function: s2-angs example code 1 2 3 4 5 import s2_angs s2_angs . gen_s2_ang ( \"S2B_MSIL1C_20191223T131239_N0208_R138_T23KMR_20191223T135458.zip\" ) The above code will generate the angle bands from the image defined in the input. Examples of the results can be seen in the figures below: Intermediary results (matrix 23x23) Solar Azimuth Solar Zenith View Azimuth View Zenith Solar Azimuth intermediary result Solar Zenith intermediary result View Azimuth intermediary result View Zenith intermediary result Resampled results Solar Azimuth Solar Zenith View Azimuth View Zenith Solar Azimuth resampled result Solar Zenith resampled result View Azimuth resampled result View Zenith resampled result To learn more about the library, please, see the official s2-angs library repository on GitHub. Sensor Harmonization Python Library (sensor-harm) \u00b6 In this RC , part of the results consists of harmonized products, i.e., surface reflectance level products with correction for Bi-Directional Reflectance Distribution Function (BRDF) effects and spectral adjustments. For this purpose, the sensor-harm library was created. The BRDF correction is done using the c-factor method to generate Nadir BRDF-Adjusted Reflectance (NBAR) products in this library. In contrast, the spectral adjustment is made using bandpass adopting Landsat-8 images as the reference. Using this library, these methods can be applied to images from Landsat-4/TM, Landsat-5/TM, Landsat-7/ETM+, Landsat-8/OLI, and Sentinel-2/MSI sensor satellites. They can be harmonized between these different data. The library features two main functions, one for harmonizing images from sensors onboard the Landsat satellites and one for images from sensors onboard the Sentinel-2 satellites. Operations available \u00b6 The table below summarizes the operations available in the sensor-harm library. Function Description sensor_harm.landsat.landsat_harmonize Function to harmonize Landsat data sensor_harm.sentinel2.sentinel_harmonize Function to harmonize Sentinel-2 data Usage example \u00b6 To perform data harmonization, be it Landsat-4/TM, Landsat-5/TM, Landsat-7/ETM+, Landsat-8/OLI, or Sentinel-2, it is necessary to define the directory where the input data is stored, as well as the output directory. To exemplify the use of this function, the code block below is an example of how we can use the sensor-harm library for Sentinel-2 data harmonization: sensor-harm example code 1 2 3 4 5 6 from sensor_harm.sentinel2 import sentinel_harmonize sentinel2_entry = '/path/to/S2/SR/images/' target_dir = '/path/to/output/NBAR/' sentinel_harmonize ( sentinel2_entry , target_dir , apply_bandpass = True ) The above code will generate the angle bands from the image defined in the input. Examples of the results can be seen in the figures below: Sentinel-2/MSI Harmonized Data NBAR Band 02 (10m) NBAR Band 03 (10m) NBAR Band 04 (10m) NBAR Band 08 (10m) NBAR Band 08A (20m) NBAR Band 11 (20m) NBAR Band 12 (20m) Sentinel-2/MSI Image (B02) NBAR 10m Sentinel-2/MSI Image (B03) NBAR 10m Sentinel-2/MSI Image (B04) NBAR 10m Sentinel-2/MSI Image (B08) NBAR 10m Sentinel-2/MSI Image (B8A) NBAR 20m Sentinel-2/MSI Image (B11) NBAR 20m Sentinel-2/MSI Image (B12) NBAR 20m To learn more about the library, please, see the official sensor-harm library repository on GitHub. Research Processing Python Library (research-processing) \u00b6 As presented in the previous sections, the processing methodology performed in this RC has several steps, which depend on different software tools. Consequently, the completion of the entire processing flow may require: installation of specific software dependencies for each processing step; Specific configurations in the software environment for each processing step. With these requirements, the execution and reproduction of the processing flow could be problematic for us, and those who wish to reproduce or even apply the work developed. To avoid these possible problems and facilitate the materialization of this RC processing methodology, we developed the research-processing library. In this library, all the methodology steps are modeled as Python functions that can be easily used to build any processing flow. Additionally, part of the operations that require environment-specific configurations is executed in Docker Containers , transparently to the users of the library. All in all, you have research-processing , functionalities for performing actions such as: Preprocessing Sentinel-2/MSI atmospheric correction (Sen2Cor and LaSRC); Angle band generation (Landsat-8/OLI and Sentinel-2/MSI); Generation of NBAR products (Sentinel-2/MSI and Landsat-8/OLI). Data analysis Routines for validating the products created in the processing flow. Function Execution Approach \u00b6 As mentioned earlier, the research-processing features are modeled as reusable functions with play-friendly execution. To ensure that these characteristics could be guaranteed, we adopted the execution model concept. This concept determines where/how the function will be executed transparently to the user. Each of the functions implemented in the library has an execution model , allowing different technologies and approaches to be used as a basis to execute the function. Based on the needs of the RC functions, two `execution models' were used in the implementations, namely: Local Functions implemented with the Local execution model are simple Python functions. These functions do not have any particular execution format and are executed directly in the environment/interpreter that invokes them. Functions with this format depending on the surrounding environment, which requires all dependencies to be installed and properly configured. Containerized Unlike Local functions, Containerized functions do not depend on the surrounding environment. This is because functions implemented with this format, when executed, create a Docker Container , with the complete environment necessary for the execution of the operation associated with the function. The function is executed within the created environment. Both execution models are transparent to the user at runtime. So regardless of how the implementation was done, in the end, for the users, we have a call to a Python function. The difference lies in the demands that each type of function will make on the environment where it is being executed. As mentioned, for Local functions, you will need to install all the dependencies and settings to run the function. Meanwhile, Containerized functions will require Docker to be available in the user's environment. Containerized - User Permissions It is important to note that besides having Docker installed, it is also necessary for the user who is running it to have the appropriate permissions to use Docker. The figure below is a general representation of each of these models used. Note that Local functions work with the Python interpreter, while Containerized functions create Docker Containers where execution will take place. Research Processing Execution Models Based on these two modes of operation, the library implements and provides processing and analysis operations on the data. The Containerized mode of operation is used for processing operations, which use third-party tools like Sen2Cor and LaSRC . The Local mode of operation is used for validation operations, which uses only Python libraries as dependencies. Function communication approach \u00b6 The research-processing library features can be used together to build a processing flow that allows the materialization of the experiment methodology followed in this RC . In this context, one point to consider is how the inputs and outputs of these functions are used together. In research-processing , the implemented operations are performed in a way that avoids data movement (Inputs and Outputs). To do this, the processing functions operate based on the data path . This gives the functions the path where the data is stored and where the results should be saved. This mode of operation allows better definitions regarding where the data will be loaded and saved, avoiding unnecessary movement and storage in places that can present problems with space limitation and inadequate performance. This mode of operation is represented in the figure below. Function communication model Based on this mode of operation, functions are chained together. The outputs of one function, representing the path to where the data was saved, are used as input in other functions. Inputs as volumes For the Containerized functions, you should note that this data path information is used for creating Docker Bind Mount Volumes . Thus, the auxiliary Containers process, created by the research-processing library, has access to the processed data. Order of Operation With this input/output format of functions, it is assumed that the output of one function will be understood by the following function. Thus, the functions available in the research-processing library must be chained together in a strict order, in this case, the order described in the methodology. For more details on how this can be implemented, see the section Processing scripts . Operations available \u00b6 To support the creation of the entire operation flow implemented in this RC , the research-processing library provides several auxiliary functions and operations, which are listed in the table below: Function Description Execution model research_processing.surface_reflectance.sen2cor Sen2Cor Atmospheric Correction for Sentinel-2/MSI Containerized research_processing.surface_reflectance.lasrc LaSRC Atmospheric Correction for Sentinel-2/MSI Containerized research_processing.nbar.s2_sen2cor_nbar NBAR product generator for Sentinel-2/MSI surface reflectance data (Sen2Cor) Containerized research_processing.nbar.s2_lasrc_nbar NBAR product generator for Sentinel-2/MSI surface reflectance data (LaSRC) Containerized research_processing.nbar.lc8_nbar NBAR product generator for Landsat-8/OLI surface reflectance data Containerized research_processing.nbar.lc8_generate_angles Angle Generator for Landsat-8/OLI data Containerized research_processing.validation.validation_routines Results Analysis (Module with multiple functions) Local Among these functions, some details need to be considered to fully understand the rationale behind each of the execution models chosen for the functions. In the subtopics below, these details are presented: Sentinel-2/MSI atmospheric correction (Sen2Cor and LaSRC) The processing flow requires that the images used have an atmospheric correction. For Landsat-8/OLI images, there is no need to correct since the products are already available ready for use, with the proper geometric and radiometric corrections performed. However, for Sentinel-2/MSI data, this is not true. Thus, during the development of the article, it was necessary to perform the atmospheric correction of these data. For this, we adopted the tools Sen2Cor and LaSRC . These tools have their specific needs in the environment where they will be used, like dependencies and configurations. In the research-processing library, we created functions that operate these tools in an environment already configured and ready to use. This environment runs in a Docker Container . From the functions presented in the table above, the ones listed below are used to apply atmospheric correction to the data: research_processing.surface_reflectance.lasrc : Function that applies Atmospheric Correction to Sentinel-2/MSI data using the LaSRC tool. All processing is performed inside a Docker Container transparently to the user; research_processing.surface_reflectance.sen2cor : Function that applies Atmospheric Correction to Sentinel-2/MSI data using the Sen2Cor tool. All processing is done inside a Docker Container transparently to the user. Understanding the Containers Creating a Docker Container depends on a Docker Image that defines the environment and its settings. This is no different in research-processing . For the creation of Atmospheric Correction Docker Containers , the following Docker Images are used: Atmospheric correction with LaSRC: LaSRC 2.0.1 Docker Image Atmospheric correction with Sen2Cor: Sen2Cor 2.9.0 Docker Image These Docker Images were created to run in this RC . For more information, see the Computational Environments section. Angle band generation (Landsat-8/OLI and Sentinel-2/MSI) To generate NBAR products, angle bands (e.g., SAA , SZA , VAA , and VZA ) must be calculated. These bands are explicitly developed for each data/sensor being worked on. This requires the use of specialized tools for each sensor: Landsat-8/OLI: The generation of angle bands for Landsat-8/OLI is done using the Landsat 8 Angles Creation Tools tool. This tool has its own dependencies and also requires specific settings in the environment where it will be run; Sentinel-2/MSI: The angles from Sentinel-2/MSI data are generated with the Sentinel-2 Angle Generator Python Library (s2-angs) library, developed in this RC . Considering these characteristics, you have the following function to perform these operations: research_processing.nbar.lc8_generate_angles : A Function uses the Landsat 8 Angles Creation Tools tool to perform the calculation of angle bands for Landsat-8/OLI data. The processing performed by this function is done inside a Docker Container . In the above list of functions, there is no function specific to Sentinel-2/MSI. During the research-processing library implementation, we decided that the generation of angles for Sentinel-2/MSI data would be an integrated operation with the generation of NBAR products. Thus, the calculation of the angles required for the generation of NBAR products with Sentinel-2/MSI data, done with the s2-angs library, are part of the following functions: research_processing.nbar.s2_sen2cor_nbar research_processing.nbar.s2_lasrc_nbar The processing performed by both functions listed above is also done inside a Docker Container . Understanding the Containers Creating a Docker Container depends on a Docker Image that defines the environment and its settings. This is no different in research-processing . To create the Docker Container angle band generation, the following Docker Image is used: Angle band generation for Landsat-8/OLI data: L8Angs Docker Image The L8Angs Docker Image was created to run in this RC . For more information, see the Computational Environments section. NBAR products generation (Sentinel-2/MSI and Landsat-8/OLI) The basis for generating NBAR products in this RC , as presented in the previous sections, is the sensor-harm library. This library allows the generation of NBAR products for Sentinel-2/MSI and Landsat-8/OLI data. As a way to facilitate the use of the sensor-harm library and to avoid users having to do specific installations and configurations, in the research-processing library, the implementation of the following functions was performed: research_processing.nbar.lc8_nbar : A function to generate NBAR products for Landsat-8/OLI data; research_processing.nbar.s2_lasrc_nbar : A function to generate NBAR products for Sentinel-2/MSI data with the atmospheric correction made with the LaSRC tool; research_processing.nbar.s2_sen2cor_nbar : A function to generate NBAR products for Sentinel-2/MSI data with the atmospheric correction made with the Sen2Cor tool; These functions are implemented with the execution model Containerized. Thus, when the user executes these functions, a Docker Container with the appropriate dependencies is created to perform the function. Understanding the Containers Creating a Docker Container depends on a Docker Image that defines the environment and its settings. This is no different in research-processing . To create the NBAR product generation Docker Container (all functions), the following Docker Image is used: NBAR product generation (Sentinel-2/MSI e Landsat-8/OLI): NBAR Docker Image The NBAR Docker Image was created to run in this RC . For more information, see the Computational Environments section. Routines for validating the products created in the processing flow There is the corrections validation module to evaluate the products generated with the processing flow. This module ( research_processing.validation.validation_routines ), is responsible for all the comparisons and calculations used to evaluate the results generated in this RC . It is implemented with the Local execution model to make debugging more straightforward. Thus, the user, who uses the functions of this module, must configure the environment where the execution will be performed. In short, one only has to install the research-processing library's dependencies. Then the environment is ready to execute these functions. Usage example \u00b6 To exemplify how the `research-processing' library is used, below is how the library performs atmospheric correction of Sentinel-2/MSI images, using the Sen2Cor tool. Atmospheric correction Sen2Cor Sen2Cor atmosphere correction example with research-processign library 1 2 3 4 5 6 7 8 9 10 11 12 13 from research_processing.surface_reflectance import sen2cor # sen2cor( # input_dir = \"<path to directory where .safe is>\", # output_dir = \"<path where result will be saved>\" , # scene_ids = [\"<scene ids of `input_dir` that will be processed>\"] #) # For example: sen2cor ( input_dir = \"/data/input\" , output_dir = \"/data/output\" , scene_ids = [ \"S2B_MSIL1C_20171119T133209_N0206_R081_T22JBM_20171120T175608.SAFE\" ] ) The above code will generate images with atmospheric correction. Examples of results can be seen in the figures below: Atmospheric Corrected Images with Sen2Cor 10m resolution 20m resolution 60m resolution Sentinel-2/MSI Image with Atmospheric Correction (10m resolution) Sentinel-2/MSI Image with Atmospheric Correction (20m resolution) Sentinel-2/MSI Image with Atmospheric Correction (60m resolution)","title":"Software libraries"},{"location":"en/tools/libraries/#software-libraries","text":"Reproducibility allows us to retrace the path that was initially taken to obtain the results of scientific research. With this, other researchers and even our future selves can benefit and understand what we have done. The reproducibility also enables us to detect and correct possible errors in the results. Among the many actions that can be taken to achieve reproducibility is the ability to automate the steps already performed. This brings several benefits: Avoids possible \"mistakes\" by reading incorrect results that are no longer valid for the current run; It allows the verification of the entire processing flow (which also helps in the writing of the report/article); Decreased overhead of running the job. A few years ago, this feature was limited in research due to the use of software with graphical interfaces that ended up not allowing the automation of the work done through \"buttons and clicks\" on the screen. Nowadays, with the increase and dissemination of high-level languages for research development, such as R and Python , this has completely changed. We can automate tasks more efficiently. Also, from the moment a processing script is created, all the logic applied to the data is described clearly and directly. In this RC , several Python code libraries were created so that all steps could be modeled through scripts. Each library created has a unique responsibility, which helped us keep the work organized during its development. At the same time, this approach facilitates the reuse of the code created. Those who wish to reuse the work developed can import these libraries into their Python project.","title":"Software Libraries"},{"location":"en/tools/libraries/#available-code-libraries","text":"To adopt this development approach, based on single responsibility libraries that could be reused, it was necessary to adopt some criteria that would help us keep everything organized and reusable. Also, our vision for creating multiple libraries for the composition of the scripts for processing the work is that these should be modeled in a way that they can be used together. Like blocks that can be put together to build a wall. Considering these characteristics, initially, we defined that in this RC , we would develop two types of libraries: Base Libraries that provide the base features and functionality for performing an action (e.g., Production of angle bands) Application Libraries that, through the union of Base libraries, provide functionalities that different methodologies and processing flows (e.g., Generation of harmonized products). Based on these definitions, for the production of this RC and the generation of its results, we developed two Base libraries: s2-angs Library create to provide functionality for generating angle bands from Sentinel-2/MSI data; sensor-harm Library create to generate harmonized produts from Sentinel-2/MSI and Landsat-8/OLI data; These libraries are available and can be installed like any other Python language library and used in different projects. So, if you are interested, you can, for example, install the sensor-harm library in your Python project and utilize the functionality provided for generating harmonized products. For both libraries, the only restriction is to respect the input formats expected by the library functions. If followed correctly, you should have no problem using them. Based on these two libraries, we created an Application library: research-processing Library created to provide functionality that allows the application of the data processing methodology used to generate the results of this RC . Part of its functionality is built upon the s2-angs and sensor-harm libraries. The relationship between these libraries is summarized in the Figure below. Libraries organization Details of the operation and functionality of each of these libraries are presented in the following sections.","title":"Available code libraries"},{"location":"en/tools/libraries/#libraries-in-the-processing-methodology","text":"The figure below gives you an overview of where the libraries presented above are used in the processing methodology: Processing workflow with libraries","title":"Libraries in the processing methodology"},{"location":"en/tools/libraries/#libraries-specification","text":"In this section, as a complement to the overview presented so far, the features and usage example of each of the aforementioned libraries are specified.","title":"Libraries specification"},{"location":"en/tools/libraries/#sentinel-2-angle-generator-python-library-s2-angs","text":"The s2-angs library, as mentioned earlier, is responsible for generating angle bands for Sentinel-2 imagery. These bands contain per-pixel information for solar azimuthal ( SAA ), solar zenithal ( SZA ), azimuthal sensor ( VAA ), and zenithal sensor ( VZA ) angles. This information is extracted from the Sentinel-2 image metadata. Initially, this data is provided as a 23x23 (rows X columns) matrix, i.e., at a spatial resolution of about 5000 meters. However, this information needs to be at a spatial resolution equivalent to that of the spectral bands of the sensor ( 10 , 20 or 60 meters) to take advantage of per-pixel corrections. Thus, the s2-angs library can estimate angles and save them as .tif files, either at their original spatial resolution or resampled to the spatial resolution of the sensor bands. So, we can list as main features of this library: Generation of angle bands ( SAA , SZA , VAA and VZA ); Resampling the angle bands to the sensor band resolution.","title":"Sentinel-2 Angle Generator Python Library (s2-angs)"},{"location":"en/tools/libraries/#operations-available","text":"The table below gives a summary of the operations that are available in the s2-angs library. Function Description s2_angs.gen_s2_ang Function to generate the Sentinel-2 Angle bands","title":"Operations available"},{"location":"en/tools/libraries/#usage-example","text":"Using the s2_angs library, use the gen_s2_ang function to perform angle band generation. This function accepts as input .zip , .SAFE directory or directory with Sentinel-2 images . In the code block below is an example where a .zip file is used as input to the function: s2-angs example code 1 2 3 4 5 import s2_angs s2_angs . gen_s2_ang ( \"S2B_MSIL1C_20191223T131239_N0208_R138_T23KMR_20191223T135458.zip\" ) The above code will generate the angle bands from the image defined in the input. Examples of the results can be seen in the figures below: Intermediary results (matrix 23x23) Solar Azimuth Solar Zenith View Azimuth View Zenith Solar Azimuth intermediary result Solar Zenith intermediary result View Azimuth intermediary result View Zenith intermediary result Resampled results Solar Azimuth Solar Zenith View Azimuth View Zenith Solar Azimuth resampled result Solar Zenith resampled result View Azimuth resampled result View Zenith resampled result To learn more about the library, please, see the official s2-angs library repository on GitHub.","title":"Usage example"},{"location":"en/tools/libraries/#sensor-harmonization-python-library-sensor-harm","text":"In this RC , part of the results consists of harmonized products, i.e., surface reflectance level products with correction for Bi-Directional Reflectance Distribution Function (BRDF) effects and spectral adjustments. For this purpose, the sensor-harm library was created. The BRDF correction is done using the c-factor method to generate Nadir BRDF-Adjusted Reflectance (NBAR) products in this library. In contrast, the spectral adjustment is made using bandpass adopting Landsat-8 images as the reference. Using this library, these methods can be applied to images from Landsat-4/TM, Landsat-5/TM, Landsat-7/ETM+, Landsat-8/OLI, and Sentinel-2/MSI sensor satellites. They can be harmonized between these different data. The library features two main functions, one for harmonizing images from sensors onboard the Landsat satellites and one for images from sensors onboard the Sentinel-2 satellites.","title":"Sensor Harmonization Python Library (sensor-harm)"},{"location":"en/tools/libraries/#operations-available_1","text":"The table below summarizes the operations available in the sensor-harm library. Function Description sensor_harm.landsat.landsat_harmonize Function to harmonize Landsat data sensor_harm.sentinel2.sentinel_harmonize Function to harmonize Sentinel-2 data","title":"Operations available"},{"location":"en/tools/libraries/#usage-example_1","text":"To perform data harmonization, be it Landsat-4/TM, Landsat-5/TM, Landsat-7/ETM+, Landsat-8/OLI, or Sentinel-2, it is necessary to define the directory where the input data is stored, as well as the output directory. To exemplify the use of this function, the code block below is an example of how we can use the sensor-harm library for Sentinel-2 data harmonization: sensor-harm example code 1 2 3 4 5 6 from sensor_harm.sentinel2 import sentinel_harmonize sentinel2_entry = '/path/to/S2/SR/images/' target_dir = '/path/to/output/NBAR/' sentinel_harmonize ( sentinel2_entry , target_dir , apply_bandpass = True ) The above code will generate the angle bands from the image defined in the input. Examples of the results can be seen in the figures below: Sentinel-2/MSI Harmonized Data NBAR Band 02 (10m) NBAR Band 03 (10m) NBAR Band 04 (10m) NBAR Band 08 (10m) NBAR Band 08A (20m) NBAR Band 11 (20m) NBAR Band 12 (20m) Sentinel-2/MSI Image (B02) NBAR 10m Sentinel-2/MSI Image (B03) NBAR 10m Sentinel-2/MSI Image (B04) NBAR 10m Sentinel-2/MSI Image (B08) NBAR 10m Sentinel-2/MSI Image (B8A) NBAR 20m Sentinel-2/MSI Image (B11) NBAR 20m Sentinel-2/MSI Image (B12) NBAR 20m To learn more about the library, please, see the official sensor-harm library repository on GitHub.","title":"Usage example"},{"location":"en/tools/libraries/#research-processing-python-library-research-processing","text":"As presented in the previous sections, the processing methodology performed in this RC has several steps, which depend on different software tools. Consequently, the completion of the entire processing flow may require: installation of specific software dependencies for each processing step; Specific configurations in the software environment for each processing step. With these requirements, the execution and reproduction of the processing flow could be problematic for us, and those who wish to reproduce or even apply the work developed. To avoid these possible problems and facilitate the materialization of this RC processing methodology, we developed the research-processing library. In this library, all the methodology steps are modeled as Python functions that can be easily used to build any processing flow. Additionally, part of the operations that require environment-specific configurations is executed in Docker Containers , transparently to the users of the library. All in all, you have research-processing , functionalities for performing actions such as: Preprocessing Sentinel-2/MSI atmospheric correction (Sen2Cor and LaSRC); Angle band generation (Landsat-8/OLI and Sentinel-2/MSI); Generation of NBAR products (Sentinel-2/MSI and Landsat-8/OLI). Data analysis Routines for validating the products created in the processing flow.","title":"Research Processing Python Library (research-processing)"},{"location":"en/tools/libraries/#function-execution-approach","text":"As mentioned earlier, the research-processing features are modeled as reusable functions with play-friendly execution. To ensure that these characteristics could be guaranteed, we adopted the execution model concept. This concept determines where/how the function will be executed transparently to the user. Each of the functions implemented in the library has an execution model , allowing different technologies and approaches to be used as a basis to execute the function. Based on the needs of the RC functions, two `execution models' were used in the implementations, namely: Local Functions implemented with the Local execution model are simple Python functions. These functions do not have any particular execution format and are executed directly in the environment/interpreter that invokes them. Functions with this format depending on the surrounding environment, which requires all dependencies to be installed and properly configured. Containerized Unlike Local functions, Containerized functions do not depend on the surrounding environment. This is because functions implemented with this format, when executed, create a Docker Container , with the complete environment necessary for the execution of the operation associated with the function. The function is executed within the created environment. Both execution models are transparent to the user at runtime. So regardless of how the implementation was done, in the end, for the users, we have a call to a Python function. The difference lies in the demands that each type of function will make on the environment where it is being executed. As mentioned, for Local functions, you will need to install all the dependencies and settings to run the function. Meanwhile, Containerized functions will require Docker to be available in the user's environment. Containerized - User Permissions It is important to note that besides having Docker installed, it is also necessary for the user who is running it to have the appropriate permissions to use Docker. The figure below is a general representation of each of these models used. Note that Local functions work with the Python interpreter, while Containerized functions create Docker Containers where execution will take place. Research Processing Execution Models Based on these two modes of operation, the library implements and provides processing and analysis operations on the data. The Containerized mode of operation is used for processing operations, which use third-party tools like Sen2Cor and LaSRC . The Local mode of operation is used for validation operations, which uses only Python libraries as dependencies.","title":"Function Execution Approach"},{"location":"en/tools/libraries/#function-communication-approach","text":"The research-processing library features can be used together to build a processing flow that allows the materialization of the experiment methodology followed in this RC . In this context, one point to consider is how the inputs and outputs of these functions are used together. In research-processing , the implemented operations are performed in a way that avoids data movement (Inputs and Outputs). To do this, the processing functions operate based on the data path . This gives the functions the path where the data is stored and where the results should be saved. This mode of operation allows better definitions regarding where the data will be loaded and saved, avoiding unnecessary movement and storage in places that can present problems with space limitation and inadequate performance. This mode of operation is represented in the figure below. Function communication model Based on this mode of operation, functions are chained together. The outputs of one function, representing the path to where the data was saved, are used as input in other functions. Inputs as volumes For the Containerized functions, you should note that this data path information is used for creating Docker Bind Mount Volumes . Thus, the auxiliary Containers process, created by the research-processing library, has access to the processed data. Order of Operation With this input/output format of functions, it is assumed that the output of one function will be understood by the following function. Thus, the functions available in the research-processing library must be chained together in a strict order, in this case, the order described in the methodology. For more details on how this can be implemented, see the section Processing scripts .","title":"Function communication approach"},{"location":"en/tools/libraries/#operations-available_2","text":"To support the creation of the entire operation flow implemented in this RC , the research-processing library provides several auxiliary functions and operations, which are listed in the table below: Function Description Execution model research_processing.surface_reflectance.sen2cor Sen2Cor Atmospheric Correction for Sentinel-2/MSI Containerized research_processing.surface_reflectance.lasrc LaSRC Atmospheric Correction for Sentinel-2/MSI Containerized research_processing.nbar.s2_sen2cor_nbar NBAR product generator for Sentinel-2/MSI surface reflectance data (Sen2Cor) Containerized research_processing.nbar.s2_lasrc_nbar NBAR product generator for Sentinel-2/MSI surface reflectance data (LaSRC) Containerized research_processing.nbar.lc8_nbar NBAR product generator for Landsat-8/OLI surface reflectance data Containerized research_processing.nbar.lc8_generate_angles Angle Generator for Landsat-8/OLI data Containerized research_processing.validation.validation_routines Results Analysis (Module with multiple functions) Local Among these functions, some details need to be considered to fully understand the rationale behind each of the execution models chosen for the functions. In the subtopics below, these details are presented: Sentinel-2/MSI atmospheric correction (Sen2Cor and LaSRC) The processing flow requires that the images used have an atmospheric correction. For Landsat-8/OLI images, there is no need to correct since the products are already available ready for use, with the proper geometric and radiometric corrections performed. However, for Sentinel-2/MSI data, this is not true. Thus, during the development of the article, it was necessary to perform the atmospheric correction of these data. For this, we adopted the tools Sen2Cor and LaSRC . These tools have their specific needs in the environment where they will be used, like dependencies and configurations. In the research-processing library, we created functions that operate these tools in an environment already configured and ready to use. This environment runs in a Docker Container . From the functions presented in the table above, the ones listed below are used to apply atmospheric correction to the data: research_processing.surface_reflectance.lasrc : Function that applies Atmospheric Correction to Sentinel-2/MSI data using the LaSRC tool. All processing is performed inside a Docker Container transparently to the user; research_processing.surface_reflectance.sen2cor : Function that applies Atmospheric Correction to Sentinel-2/MSI data using the Sen2Cor tool. All processing is done inside a Docker Container transparently to the user. Understanding the Containers Creating a Docker Container depends on a Docker Image that defines the environment and its settings. This is no different in research-processing . For the creation of Atmospheric Correction Docker Containers , the following Docker Images are used: Atmospheric correction with LaSRC: LaSRC 2.0.1 Docker Image Atmospheric correction with Sen2Cor: Sen2Cor 2.9.0 Docker Image These Docker Images were created to run in this RC . For more information, see the Computational Environments section. Angle band generation (Landsat-8/OLI and Sentinel-2/MSI) To generate NBAR products, angle bands (e.g., SAA , SZA , VAA , and VZA ) must be calculated. These bands are explicitly developed for each data/sensor being worked on. This requires the use of specialized tools for each sensor: Landsat-8/OLI: The generation of angle bands for Landsat-8/OLI is done using the Landsat 8 Angles Creation Tools tool. This tool has its own dependencies and also requires specific settings in the environment where it will be run; Sentinel-2/MSI: The angles from Sentinel-2/MSI data are generated with the Sentinel-2 Angle Generator Python Library (s2-angs) library, developed in this RC . Considering these characteristics, you have the following function to perform these operations: research_processing.nbar.lc8_generate_angles : A Function uses the Landsat 8 Angles Creation Tools tool to perform the calculation of angle bands for Landsat-8/OLI data. The processing performed by this function is done inside a Docker Container . In the above list of functions, there is no function specific to Sentinel-2/MSI. During the research-processing library implementation, we decided that the generation of angles for Sentinel-2/MSI data would be an integrated operation with the generation of NBAR products. Thus, the calculation of the angles required for the generation of NBAR products with Sentinel-2/MSI data, done with the s2-angs library, are part of the following functions: research_processing.nbar.s2_sen2cor_nbar research_processing.nbar.s2_lasrc_nbar The processing performed by both functions listed above is also done inside a Docker Container . Understanding the Containers Creating a Docker Container depends on a Docker Image that defines the environment and its settings. This is no different in research-processing . To create the Docker Container angle band generation, the following Docker Image is used: Angle band generation for Landsat-8/OLI data: L8Angs Docker Image The L8Angs Docker Image was created to run in this RC . For more information, see the Computational Environments section. NBAR products generation (Sentinel-2/MSI and Landsat-8/OLI) The basis for generating NBAR products in this RC , as presented in the previous sections, is the sensor-harm library. This library allows the generation of NBAR products for Sentinel-2/MSI and Landsat-8/OLI data. As a way to facilitate the use of the sensor-harm library and to avoid users having to do specific installations and configurations, in the research-processing library, the implementation of the following functions was performed: research_processing.nbar.lc8_nbar : A function to generate NBAR products for Landsat-8/OLI data; research_processing.nbar.s2_lasrc_nbar : A function to generate NBAR products for Sentinel-2/MSI data with the atmospheric correction made with the LaSRC tool; research_processing.nbar.s2_sen2cor_nbar : A function to generate NBAR products for Sentinel-2/MSI data with the atmospheric correction made with the Sen2Cor tool; These functions are implemented with the execution model Containerized. Thus, when the user executes these functions, a Docker Container with the appropriate dependencies is created to perform the function. Understanding the Containers Creating a Docker Container depends on a Docker Image that defines the environment and its settings. This is no different in research-processing . To create the NBAR product generation Docker Container (all functions), the following Docker Image is used: NBAR product generation (Sentinel-2/MSI e Landsat-8/OLI): NBAR Docker Image The NBAR Docker Image was created to run in this RC . For more information, see the Computational Environments section. Routines for validating the products created in the processing flow There is the corrections validation module to evaluate the products generated with the processing flow. This module ( research_processing.validation.validation_routines ), is responsible for all the comparisons and calculations used to evaluate the results generated in this RC . It is implemented with the Local execution model to make debugging more straightforward. Thus, the user, who uses the functions of this module, must configure the environment where the execution will be performed. In short, one only has to install the research-processing library's dependencies. Then the environment is ready to execute these functions.","title":"Operations available"},{"location":"en/tools/libraries/#usage-example_2","text":"To exemplify how the `research-processing' library is used, below is how the library performs atmospheric correction of Sentinel-2/MSI images, using the Sen2Cor tool. Atmospheric correction Sen2Cor Sen2Cor atmosphere correction example with research-processign library 1 2 3 4 5 6 7 8 9 10 11 12 13 from research_processing.surface_reflectance import sen2cor # sen2cor( # input_dir = \"<path to directory where .safe is>\", # output_dir = \"<path where result will be saved>\" , # scene_ids = [\"<scene ids of `input_dir` that will be processed>\"] #) # For example: sen2cor ( input_dir = \"/data/input\" , output_dir = \"/data/output\" , scene_ids = [ \"S2B_MSIL1C_20171119T133209_N0206_R081_T22JBM_20171120T175608.SAFE\" ] ) The above code will generate images with atmospheric correction. Examples of results can be seen in the figures below: Atmospheric Corrected Images with Sen2Cor 10m resolution 20m resolution 60m resolution Sentinel-2/MSI Image with Atmospheric Correction (10m resolution) Sentinel-2/MSI Image with Atmospheric Correction (20m resolution) Sentinel-2/MSI Image with Atmospheric Correction (60m resolution)","title":"Usage example"},{"location":"en/tools/processing/","text":"Processing Scripts \u00b6 To implement this RC methodology, a set of software libraries was created. Using these libraries and their functionality, we implemented the processing scripts . These scripts are the materialization of this RC methodology experiments. These scripts were processed using two tools: Jupyter Notebook : Interactive computation environment, that allows the creation of computational notebooks containing code and documentation; Dagster : Platform for creating, managing and executing processing chains and data manipulation. Both implementation use the same libraries and versions to process, ensuting reproductible and comparable results. These two options were implemented due to: Present the capabilities of the use of this RC libraries; Coverage of the scenaries of this RC methodology; Double implementation, which allowed to compare the genarated results in distinct ways, from the same tools. The connhection between these processing scripts and the software libraries developed in this RC , are presented in the following Figure: Relation between software libraries and processing scripts . Jupyter Notebook \u00b6 When exploring the implemented methodology, it is desired that the processing steps should be well documented and organized. In the implemented Jupyter Notebook, all steps are documented and described, allowing users to understand and explore what is being processed. In the following video a few parts of the notebook are presented. Jupyter Notebook processing example. Configurations \u00b6 To use this Jupyter Notebook only one configuration is required: define the environmental variable DATA_DIRECTORY . This variable determines the data directory that will be used by the notebook to load input data and write the gererated results. About the data directory Details regarding how the data, defined through the environmental variable DATA_DIRECTORY , mus be organized are presented in the section Data directory . Dagster \u00b6 About Dagster version When this RC was being developed, the dagster official version was 0.12.15 . Nowadays the versions have changed, and so have the nomenclature and definitions used by the software . To keep the content consistent, the explanations used in this documentation follow the base version 0.12.15 . Nomenclatures In Dagster, all the processing flux is called a Pipeline . In this document, to keep the explanations consistent to the ones performed in the Jupyter Notebook, the pipelines are generically treated as processing scripts . You can consult the official Dagster Documentation (v0.12.15) where you can find a detailed explanation of Pipelines . Once with an implemented methodology and ready to use, its execution alongside its big data can require several controls and actions as: Parallel execution; Failure Control; Reexecution. Based on that, the processing script using Dagster was created. With this tool, all the orchestration can be performed. Besides that, when configured, several different execution ways can be performed, for instance distributed and parallel. Another Dagester advantage is also failure control and reexecution. The processing script creation is performed through a Python Application Programming Interface (API), nonetheless, the manipulation and use of the script is performed through a web interface. This interface presents the options to execute the processing, manage it as well as consult the documentation of each step and the performed flux. The video bellow presents a this interface: Dagster interface example containing part of the processing flux. Configurations \u00b6 To execute the processing script created with Dagster, it is required to configure the tool. This configuration is performed through the definition of a few parameters in a YAML file, which specifies: Where input data are stored; Which will be the output directory; Which images and spectral bands should be used; Computational resources specification (e.g., number of available CPUs). Once this file is created, it is inserted in the Dagster interface, where it is validated and used to create a new script execution. About Dagster executions To lear more on how Dagster version 0.12.15 creates its executions, please, consult the tool official documentation In order to configure your Dagster parameters and execute your own data, the following subsection presents the YAML file used by Dagster. Dagster Configuration File \u00b6 The Dagster configuration file, is used to define the resources that will be used during the processing script execution. This file, in the YAML format, is divided in two main sections: (i) Resources; (ii) Solids. About the configuration file When using the Jupyter Notebook, all configuration is performed through a unique environmental variable ( DATA_DIRECTORY ). In Dagster, the configuration file has the same role. Thus, it is recommended to consult the Data directory section before you configure the file. In the mentioned section the directory structure, that must be followed, is described, as well as the content that must be available in each section. Resources \u00b6 In Dagster, resources are used to represent elements and computational resourses that must be available for the processing script execution. The resources represent the data that must be used during the processing. Thus, a resource , can have one or more data directories. The processing script requires two resources : lasrc_data Resource containing data related to the LaSRC tool. The defition of a lasrc_data resource requires the specification of the following configuration variable: lasrc_auxiliary_data_dir : Variable that defines the path to the directory containing the LaSRC auxiliary files, required to perform the atmospheric correction processing (used in this RC for Sentinel-2/MSI). Data directory organization For more details on how these auxiliary data must be organized, please consult the section Data directory . A complete example of the lasrc_data resource definition is presented bellow: resources : lasrc_data : config : lasrc_auxiliary_data_dir : /path/to/lasrc/auxiliary/data This block of code must be defined within the configuration file. For a complete example, please consult the dagster configuration file . repository Resource that defines the input and output of the processing script . The definition of a repository resource requires the specification of the following configuration variables: landsat8_input_dir : Directory containing the Landsat-8/OLI data, that can be used as input for the processing script ; sentinel2_input_dir : Directory containing the Sentinel-2/MSI data, that can be used as input for the processing script ; derived_data_dir : Directory in which the output data will be written. Data Directory Organization For more details on how each of these directories should be organized, please, consult the section Data directory . A complete example on the repository resource definition is presented bellow: resources : repository : config : derived_data_dir : /path/to/derived/data/dir landsat8_input_dir : /path/to/input/landsat8/data/dir sentinel2_input_dir : /path/to/input/sentinel2/data/dir This block of code must be defined within the configuration file. For a comple example, consult the dagster configuration file . Solids \u00b6 In Dagster, solids represents the working unity that will execute the operation. These elements are responsible for receiving inputs, performing the processing and generate the output. Thus, the processing script , they were used to represent each of the processing steps. The solids an require a configuration for their definition. In this RC processing script only one of them requires configuration: load_and_standardize_sceneids_input : solid responsible for receiving the orbital scenes to be processed. The solid reads, validates and pass the loaded information for the following processing steps. During this solid configuration, it is required to specify the following configuration variables: landsat8_sceneid_list : .txt file containing the name of all Landsat-8/OLI scenes that will be considered during the processing flux. These names must be the same as the ones available in the Landsat-8/OLI data directory ( landsat8_input_dir ) specified in the repository resource ; sentinel2_sceneid_list : .txt file containing the name of all Sentinel-2/MSI scenes that will be considered during the processing flux. These names must be the same as the ones available in the Sentinel-2/MSI data directory ( sentinel2_input_dir ) specified in the repository resource ; Data directory organization For more details on how the files containing the scene names should be organized, please consult the section Data directory . A complete definition of the load_and_standardize_sceneids_input solid is presented bellow: solids : load_and_standardize_sceneids_input : config : landsat8_sceneid_list : /path/to/input/landsat8/data/landsat.txt sentinel2_sceneid_list : /path/to/input/sentinel2/data/sentinel.txt Complete example \u00b6 Bellow is presented a complete Dagster configuration file, that uses all resources and solids specified in the previous topics: To exemplify all the cited elements in a configuration file, bellow is presented the minimal configuration required to execute this RC pipeline. resources : lasrc_data : config : lasrc_auxiliary_data_dir : /path/to/lasrc/auxiliary/data repository : config : derived_data_dir : /path/to/derived/data/dir landsat8_input_dir : /path/to/input/landsat8/data/dir sentinel2_input_dir : /path/to/input/sentinel2/data/dir solids : load_and_standardize_sceneids_input : config : landsat8_sceneid_list : /path/to/input/landsat8/data/landsat.txt sentinel2_sceneid_list : /path/to/input/sentinel2/data/sentinel.txt Data directory \u00b6 In both implementations of the processing scripts , in the configuration step, it is required to define the path to the input directory containing the data. This directory has a standard organization, that is used by any of the two processing approaches. In this section, the organization of the data directory will be presented. To start it is important first to understand that: The data directory, used in both processing scripts approaches, represent the input and output directories of the script . All the inputs are read from this directory only, not being able to read from a different directory. The same occurs for the output, the results produced are only stored in this directory. The logic behind this definition is organization: If a researcher needs to centralize and keep organized all mateials in a logic structure, he is spending time that could be used to produce results. To allow this directory to support all these utilities, different subdirectories are created under it. In these directories are defined the input and output. Bellow the structure of the data directory is presented: data directory \u251c\u2500\u2500 derived_data \u2514\u2500\u2500 raw_data \u251c\u2500\u2500 landsat8_data \u251c\u2500\u2500 sentinel2_data \u251c\u2500\u2500 scene_id_list \u2514\u2500\u2500 lasrc_auxiliary_data In which: raw_data Directory in which the input data should be stored. Its subdirectories are explained bellow. raw_data/landsat8_data In this directory, all the Landsat-8/OLI should be stored to be used by the processing scripts . For Landsat-8/OLI, it is expected that each scene to be separated in its own folder. As presented bellow: landsat8_data \u251c\u2500\u2500 LC08_L2SP_222081_20171120_20200902_02_T1 \u2514\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1 The organization of the images within each scene directory should follow the format used by the U nited S tates G eological S urvey (USGS) in the distribution of the L2 (atmospherically corrected) data of the Collection-2 (C2). Besides, the directory nomenclature, also must follow the USGS pattern (sceneid name). Errors may occur if this is not followed. Bellow an example of how these directories must be organized internally: landsat8_data \u2514\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1 \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ANG.txt \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_MTL.json \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_MTL.txt \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_MTL.xml \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_QA_PIXEL.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_QA_RADSAT.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_SR_B1.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_SR_B2.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_SR_B3.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_SR_B4.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_SR_B5.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_SR_B6.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_SR_B7.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_SR_QA_AEROSOL.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_SR_stac.json \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_ATRAN.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_B10.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_CDIST.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_DRAD.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_EMIS.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_EMSD.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_QA.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_stac.json \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_TRAD.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_URAD.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_thumb_large.jpeg \u2514\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_thumb_small.jpeg raw_data/sentinel2_data In this directory, all the Sentinel-2/MSI should be stored to be used by the processing scripts . For Sentinel-2/MSI, it is expected that each scene to be separated in its own folder. As presented bellow: sentinel2_data \u251c\u2500\u2500 S2B_MSIL1C_20171119T133209_N0206_R081_T22JBM_20171120T175608.SAFE \u2514\u2500\u2500 S2B_MSIL1C_20171122T134159_N0206_R124_T22JBM_20171122T200800.SAFE The organization of the images within each scene directory should follow the format used in .SAFE by ESA. Besides, the directory nomenclature, also must follow the .SAFE pattern (sceneid name). Errors may occur if this is not followed. Bellow an example of how these directories must be organized internally: sentinel2_data \u2514\u2500\u2500 S2B_MSIL1C_20171119T133209_N0206_R081_T22JBM_20171120T175608.SAFE \u251c\u2500\u2500 AUX_DATA \u251c\u2500\u2500 DATASTRIP \u251c\u2500\u2500 GRANULE \u251c\u2500\u2500 HTML \u251c\u2500\u2500 INSPIRE.xml \u251c\u2500\u2500 manifest.safe \u251c\u2500\u2500 MTD_MSIL1C.xml \u251c\u2500\u2500 rep_info \u2514\u2500\u2500 S2B_MSIL1C_20171119T133209_N0206_R081_T22JBM_20171120T175608-ql.jpg raw_data/scene_id_list This directory contains two files: landsat8_data and sentinel2_data , which contains a list of all sceneids should be considered in the processing steps. scene_ids_lc8.txt : Definition file that lists which Landsat-8/OLI on the Landsat-8 directory ( landsat8_data ) should be processed; scene_ids_s2.txt : Definition file that lists which Sentinel-2/MSI on the Sentinel-2 directory ( sentinel2_data ) should be processed; In both files, are listed the scene names that will be considered during the processing step. To exemplify its use, consider the following landsat8_data and sentinel2_data directories: landsat8_data \u251c\u2500\u2500 LC08_L2SP_222081_20171120_20200902_02_T1 \u2514\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1 sentinel2_data \u251c\u2500\u2500 S2B_MSIL1C_20171119T133209_N0206_R081_T22JBM_20171120T175608.SAFE \u2514\u2500\u2500 S2B_MSIL1C_20171122T134159_N0206_R124_T22JBM_20171122T200800.SAFE In order to use these files in the processing scripts , the definition files must be filled as follow: scene_ids_lc8.txt LC08_L2SP_222081_20171120_20200902_02_T1 LC08_L2SP_223081_20171111_20200902_02_T1 scene_ids_s2.txt S2B_MSIL1C_20171119T133209_N0206_R081_T22JBM_20171120T175608.SAFE S2B_MSIL1C_20171122T134159_N0206_R124_T22JBM_20171122T200800.SAFE The directories that are not listed in these files will not be considered during the processing step. raw_data/lasrc_auxiliary_data In this directory the auxiliary files for the LaSRC atmospheric corrections are organized. The organization follow the USGS dissemination FTP . Bellow an example of its structure: lasrc_auxiliary_data \u251c\u2500\u2500 CMGDEM.hdf \u251c\u2500\u2500 LADS \u251c\u2500\u2500 LDCMLUT \u251c\u2500\u2500 MSILUT \u2514\u2500\u2500 ratiomapndwiexp.hdf Note that for the processing, only the lasrc_auxiliary_data/LADS must be altered. This directory may contain onle the auxiliary files for the dates of the sceneids. To examplify. consider the directory sentinel2_data : sentinel2_data \u251c\u2500\u2500 S2B_MSIL1C_20171119T133209_N0206_R081_T22JBM_20171120T175608.SAFE \u2514\u2500\u2500 S2B_MSIL1C_20171122T134159_N0206_R124_T22JBM_20171122T200800.SAFE To process both images in this directory using the LaSRC by executing the scripts , in the LADS directory two files reffering to the sceneid dates must be available. In this case: lasrc_auxiliary_data \u251c\u2500\u2500 CMGDEM.hdf \u2514\u2500\u2500 LADS \u2514\u2500\u2500 2017 \u251c\u2500\u2500 L8ANC2017323.hdf_fused \u2514\u2500\u2500 L8ANC2017326.hdf_fused derived_data This directory stores the generated results. At the end of the execution of the processing script , will have the following structure: derived_data \u251c\u2500\u2500 l8 \u2502 \u251c\u2500\u2500 lc8_nbar_angles \u2502 \u2514\u2500\u2500 lc8_nbar \u2514\u2500\u2500 s2 \u251c\u2500\u2500 s2_lasrc_sr \u251c\u2500\u2500 s2_lasrc_nbar \u251c\u2500\u2500 s2_sen2cor_sr \u2514\u2500\u2500 s2_sen2cor_nbar Where: Landsat-8/OLI data l8/lc8_nbar_angles : Angle bands generated for the Landsat-8/OLI data; l8/lc8_nbar : NBAR products generated with the Landsat-8/OLI data. Sentinel-2/MSI LaSRC data s2/s2_lasrc_sr : Sentinel-2/MSI data corrected for atmoshperic effects using the LaSRC; s2/s2_lasrc_nbar : NBAR products generated using Sentinel-2/MSI data corrected by LaSRC (the same s2/s2_lasrc_sr directory). Sentinel-2/MSI Sen2Cor data s2/s2_sen2cor_sr : Sentinel-2/MSI data corected for atmospheric effects using the Sen2cor; s2/s2_sen2cor_nbar : NBAR products generated using Sentinel-2/MSI data corrected by Sen2Cor (The same s2/s2_sen2cor_sr directory).","title":"Processing scripts"},{"location":"en/tools/processing/#processing-scripts","text":"To implement this RC methodology, a set of software libraries was created. Using these libraries and their functionality, we implemented the processing scripts . These scripts are the materialization of this RC methodology experiments. These scripts were processed using two tools: Jupyter Notebook : Interactive computation environment, that allows the creation of computational notebooks containing code and documentation; Dagster : Platform for creating, managing and executing processing chains and data manipulation. Both implementation use the same libraries and versions to process, ensuting reproductible and comparable results. These two options were implemented due to: Present the capabilities of the use of this RC libraries; Coverage of the scenaries of this RC methodology; Double implementation, which allowed to compare the genarated results in distinct ways, from the same tools. The connhection between these processing scripts and the software libraries developed in this RC , are presented in the following Figure: Relation between software libraries and processing scripts .","title":"Processing Scripts"},{"location":"en/tools/processing/#jupyter-notebook","text":"When exploring the implemented methodology, it is desired that the processing steps should be well documented and organized. In the implemented Jupyter Notebook, all steps are documented and described, allowing users to understand and explore what is being processed. In the following video a few parts of the notebook are presented. Jupyter Notebook processing example.","title":"Jupyter Notebook"},{"location":"en/tools/processing/#configurations","text":"To use this Jupyter Notebook only one configuration is required: define the environmental variable DATA_DIRECTORY . This variable determines the data directory that will be used by the notebook to load input data and write the gererated results. About the data directory Details regarding how the data, defined through the environmental variable DATA_DIRECTORY , mus be organized are presented in the section Data directory .","title":"Configurations"},{"location":"en/tools/processing/#dagster","text":"About Dagster version When this RC was being developed, the dagster official version was 0.12.15 . Nowadays the versions have changed, and so have the nomenclature and definitions used by the software . To keep the content consistent, the explanations used in this documentation follow the base version 0.12.15 . Nomenclatures In Dagster, all the processing flux is called a Pipeline . In this document, to keep the explanations consistent to the ones performed in the Jupyter Notebook, the pipelines are generically treated as processing scripts . You can consult the official Dagster Documentation (v0.12.15) where you can find a detailed explanation of Pipelines . Once with an implemented methodology and ready to use, its execution alongside its big data can require several controls and actions as: Parallel execution; Failure Control; Reexecution. Based on that, the processing script using Dagster was created. With this tool, all the orchestration can be performed. Besides that, when configured, several different execution ways can be performed, for instance distributed and parallel. Another Dagester advantage is also failure control and reexecution. The processing script creation is performed through a Python Application Programming Interface (API), nonetheless, the manipulation and use of the script is performed through a web interface. This interface presents the options to execute the processing, manage it as well as consult the documentation of each step and the performed flux. The video bellow presents a this interface: Dagster interface example containing part of the processing flux.","title":"Dagster"},{"location":"en/tools/processing/#configurations_1","text":"To execute the processing script created with Dagster, it is required to configure the tool. This configuration is performed through the definition of a few parameters in a YAML file, which specifies: Where input data are stored; Which will be the output directory; Which images and spectral bands should be used; Computational resources specification (e.g., number of available CPUs). Once this file is created, it is inserted in the Dagster interface, where it is validated and used to create a new script execution. About Dagster executions To lear more on how Dagster version 0.12.15 creates its executions, please, consult the tool official documentation In order to configure your Dagster parameters and execute your own data, the following subsection presents the YAML file used by Dagster.","title":"Configurations"},{"location":"en/tools/processing/#dagster-configuration-file","text":"The Dagster configuration file, is used to define the resources that will be used during the processing script execution. This file, in the YAML format, is divided in two main sections: (i) Resources; (ii) Solids. About the configuration file When using the Jupyter Notebook, all configuration is performed through a unique environmental variable ( DATA_DIRECTORY ). In Dagster, the configuration file has the same role. Thus, it is recommended to consult the Data directory section before you configure the file. In the mentioned section the directory structure, that must be followed, is described, as well as the content that must be available in each section.","title":"Dagster Configuration File"},{"location":"en/tools/processing/#resources","text":"In Dagster, resources are used to represent elements and computational resourses that must be available for the processing script execution. The resources represent the data that must be used during the processing. Thus, a resource , can have one or more data directories. The processing script requires two resources : lasrc_data Resource containing data related to the LaSRC tool. The defition of a lasrc_data resource requires the specification of the following configuration variable: lasrc_auxiliary_data_dir : Variable that defines the path to the directory containing the LaSRC auxiliary files, required to perform the atmospheric correction processing (used in this RC for Sentinel-2/MSI). Data directory organization For more details on how these auxiliary data must be organized, please consult the section Data directory . A complete example of the lasrc_data resource definition is presented bellow: resources : lasrc_data : config : lasrc_auxiliary_data_dir : /path/to/lasrc/auxiliary/data This block of code must be defined within the configuration file. For a complete example, please consult the dagster configuration file . repository Resource that defines the input and output of the processing script . The definition of a repository resource requires the specification of the following configuration variables: landsat8_input_dir : Directory containing the Landsat-8/OLI data, that can be used as input for the processing script ; sentinel2_input_dir : Directory containing the Sentinel-2/MSI data, that can be used as input for the processing script ; derived_data_dir : Directory in which the output data will be written. Data Directory Organization For more details on how each of these directories should be organized, please, consult the section Data directory . A complete example on the repository resource definition is presented bellow: resources : repository : config : derived_data_dir : /path/to/derived/data/dir landsat8_input_dir : /path/to/input/landsat8/data/dir sentinel2_input_dir : /path/to/input/sentinel2/data/dir This block of code must be defined within the configuration file. For a comple example, consult the dagster configuration file .","title":"Resources"},{"location":"en/tools/processing/#solids","text":"In Dagster, solids represents the working unity that will execute the operation. These elements are responsible for receiving inputs, performing the processing and generate the output. Thus, the processing script , they were used to represent each of the processing steps. The solids an require a configuration for their definition. In this RC processing script only one of them requires configuration: load_and_standardize_sceneids_input : solid responsible for receiving the orbital scenes to be processed. The solid reads, validates and pass the loaded information for the following processing steps. During this solid configuration, it is required to specify the following configuration variables: landsat8_sceneid_list : .txt file containing the name of all Landsat-8/OLI scenes that will be considered during the processing flux. These names must be the same as the ones available in the Landsat-8/OLI data directory ( landsat8_input_dir ) specified in the repository resource ; sentinel2_sceneid_list : .txt file containing the name of all Sentinel-2/MSI scenes that will be considered during the processing flux. These names must be the same as the ones available in the Sentinel-2/MSI data directory ( sentinel2_input_dir ) specified in the repository resource ; Data directory organization For more details on how the files containing the scene names should be organized, please consult the section Data directory . A complete definition of the load_and_standardize_sceneids_input solid is presented bellow: solids : load_and_standardize_sceneids_input : config : landsat8_sceneid_list : /path/to/input/landsat8/data/landsat.txt sentinel2_sceneid_list : /path/to/input/sentinel2/data/sentinel.txt","title":"Solids"},{"location":"en/tools/processing/#complete-example","text":"Bellow is presented a complete Dagster configuration file, that uses all resources and solids specified in the previous topics: To exemplify all the cited elements in a configuration file, bellow is presented the minimal configuration required to execute this RC pipeline. resources : lasrc_data : config : lasrc_auxiliary_data_dir : /path/to/lasrc/auxiliary/data repository : config : derived_data_dir : /path/to/derived/data/dir landsat8_input_dir : /path/to/input/landsat8/data/dir sentinel2_input_dir : /path/to/input/sentinel2/data/dir solids : load_and_standardize_sceneids_input : config : landsat8_sceneid_list : /path/to/input/landsat8/data/landsat.txt sentinel2_sceneid_list : /path/to/input/sentinel2/data/sentinel.txt","title":"Complete example"},{"location":"en/tools/processing/#data-directory","text":"In both implementations of the processing scripts , in the configuration step, it is required to define the path to the input directory containing the data. This directory has a standard organization, that is used by any of the two processing approaches. In this section, the organization of the data directory will be presented. To start it is important first to understand that: The data directory, used in both processing scripts approaches, represent the input and output directories of the script . All the inputs are read from this directory only, not being able to read from a different directory. The same occurs for the output, the results produced are only stored in this directory. The logic behind this definition is organization: If a researcher needs to centralize and keep organized all mateials in a logic structure, he is spending time that could be used to produce results. To allow this directory to support all these utilities, different subdirectories are created under it. In these directories are defined the input and output. Bellow the structure of the data directory is presented: data directory \u251c\u2500\u2500 derived_data \u2514\u2500\u2500 raw_data \u251c\u2500\u2500 landsat8_data \u251c\u2500\u2500 sentinel2_data \u251c\u2500\u2500 scene_id_list \u2514\u2500\u2500 lasrc_auxiliary_data In which: raw_data Directory in which the input data should be stored. Its subdirectories are explained bellow. raw_data/landsat8_data In this directory, all the Landsat-8/OLI should be stored to be used by the processing scripts . For Landsat-8/OLI, it is expected that each scene to be separated in its own folder. As presented bellow: landsat8_data \u251c\u2500\u2500 LC08_L2SP_222081_20171120_20200902_02_T1 \u2514\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1 The organization of the images within each scene directory should follow the format used by the U nited S tates G eological S urvey (USGS) in the distribution of the L2 (atmospherically corrected) data of the Collection-2 (C2). Besides, the directory nomenclature, also must follow the USGS pattern (sceneid name). Errors may occur if this is not followed. Bellow an example of how these directories must be organized internally: landsat8_data \u2514\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1 \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ANG.txt \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_MTL.json \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_MTL.txt \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_MTL.xml \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_QA_PIXEL.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_QA_RADSAT.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_SR_B1.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_SR_B2.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_SR_B3.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_SR_B4.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_SR_B5.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_SR_B6.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_SR_B7.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_SR_QA_AEROSOL.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_SR_stac.json \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_ATRAN.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_B10.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_CDIST.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_DRAD.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_EMIS.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_EMSD.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_QA.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_stac.json \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_TRAD.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_ST_URAD.TIF \u251c\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_thumb_large.jpeg \u2514\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1_thumb_small.jpeg raw_data/sentinel2_data In this directory, all the Sentinel-2/MSI should be stored to be used by the processing scripts . For Sentinel-2/MSI, it is expected that each scene to be separated in its own folder. As presented bellow: sentinel2_data \u251c\u2500\u2500 S2B_MSIL1C_20171119T133209_N0206_R081_T22JBM_20171120T175608.SAFE \u2514\u2500\u2500 S2B_MSIL1C_20171122T134159_N0206_R124_T22JBM_20171122T200800.SAFE The organization of the images within each scene directory should follow the format used in .SAFE by ESA. Besides, the directory nomenclature, also must follow the .SAFE pattern (sceneid name). Errors may occur if this is not followed. Bellow an example of how these directories must be organized internally: sentinel2_data \u2514\u2500\u2500 S2B_MSIL1C_20171119T133209_N0206_R081_T22JBM_20171120T175608.SAFE \u251c\u2500\u2500 AUX_DATA \u251c\u2500\u2500 DATASTRIP \u251c\u2500\u2500 GRANULE \u251c\u2500\u2500 HTML \u251c\u2500\u2500 INSPIRE.xml \u251c\u2500\u2500 manifest.safe \u251c\u2500\u2500 MTD_MSIL1C.xml \u251c\u2500\u2500 rep_info \u2514\u2500\u2500 S2B_MSIL1C_20171119T133209_N0206_R081_T22JBM_20171120T175608-ql.jpg raw_data/scene_id_list This directory contains two files: landsat8_data and sentinel2_data , which contains a list of all sceneids should be considered in the processing steps. scene_ids_lc8.txt : Definition file that lists which Landsat-8/OLI on the Landsat-8 directory ( landsat8_data ) should be processed; scene_ids_s2.txt : Definition file that lists which Sentinel-2/MSI on the Sentinel-2 directory ( sentinel2_data ) should be processed; In both files, are listed the scene names that will be considered during the processing step. To exemplify its use, consider the following landsat8_data and sentinel2_data directories: landsat8_data \u251c\u2500\u2500 LC08_L2SP_222081_20171120_20200902_02_T1 \u2514\u2500\u2500 LC08_L2SP_223081_20171111_20200902_02_T1 sentinel2_data \u251c\u2500\u2500 S2B_MSIL1C_20171119T133209_N0206_R081_T22JBM_20171120T175608.SAFE \u2514\u2500\u2500 S2B_MSIL1C_20171122T134159_N0206_R124_T22JBM_20171122T200800.SAFE In order to use these files in the processing scripts , the definition files must be filled as follow: scene_ids_lc8.txt LC08_L2SP_222081_20171120_20200902_02_T1 LC08_L2SP_223081_20171111_20200902_02_T1 scene_ids_s2.txt S2B_MSIL1C_20171119T133209_N0206_R081_T22JBM_20171120T175608.SAFE S2B_MSIL1C_20171122T134159_N0206_R124_T22JBM_20171122T200800.SAFE The directories that are not listed in these files will not be considered during the processing step. raw_data/lasrc_auxiliary_data In this directory the auxiliary files for the LaSRC atmospheric corrections are organized. The organization follow the USGS dissemination FTP . Bellow an example of its structure: lasrc_auxiliary_data \u251c\u2500\u2500 CMGDEM.hdf \u251c\u2500\u2500 LADS \u251c\u2500\u2500 LDCMLUT \u251c\u2500\u2500 MSILUT \u2514\u2500\u2500 ratiomapndwiexp.hdf Note that for the processing, only the lasrc_auxiliary_data/LADS must be altered. This directory may contain onle the auxiliary files for the dates of the sceneids. To examplify. consider the directory sentinel2_data : sentinel2_data \u251c\u2500\u2500 S2B_MSIL1C_20171119T133209_N0206_R081_T22JBM_20171120T175608.SAFE \u2514\u2500\u2500 S2B_MSIL1C_20171122T134159_N0206_R124_T22JBM_20171122T200800.SAFE To process both images in this directory using the LaSRC by executing the scripts , in the LADS directory two files reffering to the sceneid dates must be available. In this case: lasrc_auxiliary_data \u251c\u2500\u2500 CMGDEM.hdf \u2514\u2500\u2500 LADS \u2514\u2500\u2500 2017 \u251c\u2500\u2500 L8ANC2017323.hdf_fused \u2514\u2500\u2500 L8ANC2017326.hdf_fused derived_data This directory stores the generated results. At the end of the execution of the processing script , will have the following structure: derived_data \u251c\u2500\u2500 l8 \u2502 \u251c\u2500\u2500 lc8_nbar_angles \u2502 \u2514\u2500\u2500 lc8_nbar \u2514\u2500\u2500 s2 \u251c\u2500\u2500 s2_lasrc_sr \u251c\u2500\u2500 s2_lasrc_nbar \u251c\u2500\u2500 s2_sen2cor_sr \u2514\u2500\u2500 s2_sen2cor_nbar Where: Landsat-8/OLI data l8/lc8_nbar_angles : Angle bands generated for the Landsat-8/OLI data; l8/lc8_nbar : NBAR products generated with the Landsat-8/OLI data. Sentinel-2/MSI LaSRC data s2/s2_lasrc_sr : Sentinel-2/MSI data corrected for atmoshperic effects using the LaSRC; s2/s2_lasrc_nbar : NBAR products generated using Sentinel-2/MSI data corrected by LaSRC (the same s2/s2_lasrc_sr directory). Sentinel-2/MSI Sen2Cor data s2/s2_sen2cor_sr : Sentinel-2/MSI data corected for atmospheric effects using the Sen2cor; s2/s2_sen2cor_nbar : NBAR products generated using Sentinel-2/MSI data corrected by Sen2Cor (The same s2/s2_sen2cor_sr directory).","title":"Data directory"},{"location":"en/tools/utilitary/","text":"Auxiliary scripts \u00b6 Besides the codes, in this RC that, that generates the results, a few auxiliary scripts were also developed. In this section, these scripts are detailed. Calculate Checksum and GitHub Asset Upload \u00b6 To share this RC , all its materials were made available through a GitHub repository, which contains all the historic of modification on materials, code, documentation and data. The scripts and documentation does not uses much disk volume, so it could be stored directly in GitHub. However, auxiliary data, that are used in minimal example and replication are larger and cannot be stored in a common repository. As an alternative, these files were published through the Release Assets , which supports files of up to 2 GB to be stored and maintained in the repository. To prepare and organize the data as GitHub Release Assets two auxiliary scripts were made: Calculate Checksum Calculate Checksum is a Python script that creates a BagIt and stores their files as zip . GitHub Asset Upload Once the BagIt are created, the R script GitHub Asset Upload uploads them to the GitHub servers, which is done with the package piggyback . Using these two scripts the data was made available on the same repository that contains the processing scripts and documentation. Example toolkit \u00b6 Considering that since the data is on Github and that anyone can obtain the, to execute the examples of this RC , it is necessary that the data is organized in the RC correct directories . To solve this and avoid manual download and organization, we provide a Python script , the Example toolkit , that automatically perform these steps. The only configuration a user must do is to define a directory for the download . Operation \u00b6 The Example toolkit script will execute four main steps, as illustrated on the Figure bellow: Example Toolkit Operation Flux The Example toolkit sript performs the data download (from the GitHub Release Assets ). Following, it verifies if the downloaded data haven't suffered any changes or if they are corrupted. This is performed using the downloaded BagIts. Then, the tool scrit extract the data to the correct folders. Finally, using the users definitions and directives of where the data was downloaded, it generates a Dagster configuration file , which can be used to start the processing of the downloaded data. Dagster and configuration files If you are interested in learning more on how Dagster is used in this RC and where the configuration file should be used, consult the Section Processing Scripts - Dagster . Use \u00b6 To use the Example toolkit script , only Python is required with the environment requisites. Example toolkit with Docker To use the Example toolkit with no environment configuration, consult the section Example toolkit environment for more information on how you can use the Docker version of the script. Manual configuration of the environment To manually configure the Python environment and the dependencies required to run the Example toolkit , you can use conda . With this package manager, you can create a new environment with the Example toolkit requirements. To create this environment, use the file environment.yml that can be found in the directory tools/example-toolkit : conda env create -f environment.yml To execute the Example toolkit you must define its configurations, which will be used to deremine the local to write the data and some parameters to auxiliate the script generates the Dagster configuration file. This configuration is done through environment variables, being them: DOWNLOAD_REFERENCE_FILE Environment variable to determine the absolute path of the JSON which defines where the GitHub Assets Release data will be downloaded. Examples of this file can be found in the directory tools/example-toolkit/config . Value example : /compendium/config/example-toolkit.json DOWNLOAD_OUTPUT_DIRECTORY Environment variable that determines the directory where the downloaded data will be stored. The data is organized following the format of the Data directory , required by the processing scripts . Value example : /compendium/data PIPELINE_DIR (Dagster Configuration) Environment variable that determines where the Dagster Configuration file will be saved. Value example : /compendium/config/config.yml RAW_DATA_DIR (Dagster Configuration) Environment variable that determines which directory should be considered as the machine input dir for the Dagster processing. Value example : /compendium/data/raw_data DERIVED_DATA_DIR (Dagster Configuration) Environment variable that determines which directory should be considered as the machine output dir in the Dagster configuration file. Value example : /compendium/data/derived_data Variable definition consistency The configuration variables have a logical dependency that must be followed to avoid problems. To present this dependency let's consider the following example: Suppose that you wish to download the data on a directory /opt/my-data . In this case, you will define the DOWNLOAD_OUTPUT_DIRECTORY as: DOWNLOAD_OUTPUT_DIRECTORY = /opt/my-data Knowing that your data will be organized following the pattern in Data directory , the downloaded data will be stored as follows: /opt/my-data \u251c\u2500\u2500 derived_data \u2514\u2500\u2500 raw_data Considering this organization, if you desire to generate the Dagster file to process data in /opt/my-data , it will be required to define the Dagster environment variables as: # 1. Input dir RAW_DATA_DIR = /opt/my-data/raw_data # 2. Output dir DERIVED_DATA_DIR = /opt/my-data/derived_data After defining each environment variable, the Example toolkit can be executed. To do that, the script available on the diretory tools/example-toolkit/scripts/pipeline.py must be executed. Considering that you are in the root of this RC , the execution of this script can be performed by: 1. Changing directory cd tools/example-toolkit/ 2. Execution python3 scripts/pipeline.py At the end of the execution, the output directories will appear as: Data The directory defined by the variable DOWNLOAD_OUTPUT_DIRECTORY , as mentioned, will follow the Data directory organization, required by this RC processing scripts scripts . The data will be organized as: DOWNLOAD_OUTPUT_DIRECTORY \u251c\u2500\u2500 derived_data \u2514\u2500\u2500 raw_data \u251c\u2500\u2500 landsat8_data \u251c\u2500\u2500 sentinel2_data \u251c\u2500\u2500 scene_id_list \u2514\u2500\u2500 lasrc_auxiliary_data Dagster Configuration The directory defined by the variable PIPELINE_DIR will be populated as a Dagster configuration File named config.yaml . In this file, the following content will be available: config.yaml: Arquivo de configura\u00e7\u00e3o Dagster resources : lasrc_data : config : lasrc_auxiliary_data_dir : { RAW_DATA_DIR } /lasrc_auxiliary_data repository : config : derived_data_dir : { DERIVED_DATA_DIR } landsat8_input_dir : { RAW_DATA_DIR } /landsat8_data sentinel2_input_dir : { RAW_DATA_DIR } /sentinel2_data solids : load_and_standardize_sceneids_input : config : landsat8_sceneid_list : { RAW_DATA_DIR } /scene_id_list/l8-sceneids.txt sentinel2_sceneid_list : { RAW_DATA_DIR } /scene_id_list/s2-sceneids.txt For a complete and functional example of the Example toolkit , consult the section Reproducible research - minimal example .","title":"Auxiliary scripts"},{"location":"en/tools/utilitary/#auxiliary-scripts","text":"Besides the codes, in this RC that, that generates the results, a few auxiliary scripts were also developed. In this section, these scripts are detailed.","title":"Auxiliary scripts"},{"location":"en/tools/utilitary/#calculate-checksum-and-github-asset-upload","text":"To share this RC , all its materials were made available through a GitHub repository, which contains all the historic of modification on materials, code, documentation and data. The scripts and documentation does not uses much disk volume, so it could be stored directly in GitHub. However, auxiliary data, that are used in minimal example and replication are larger and cannot be stored in a common repository. As an alternative, these files were published through the Release Assets , which supports files of up to 2 GB to be stored and maintained in the repository. To prepare and organize the data as GitHub Release Assets two auxiliary scripts were made: Calculate Checksum Calculate Checksum is a Python script that creates a BagIt and stores their files as zip . GitHub Asset Upload Once the BagIt are created, the R script GitHub Asset Upload uploads them to the GitHub servers, which is done with the package piggyback . Using these two scripts the data was made available on the same repository that contains the processing scripts and documentation.","title":"Calculate Checksum and GitHub Asset Upload"},{"location":"en/tools/utilitary/#example-toolkit","text":"Considering that since the data is on Github and that anyone can obtain the, to execute the examples of this RC , it is necessary that the data is organized in the RC correct directories . To solve this and avoid manual download and organization, we provide a Python script , the Example toolkit , that automatically perform these steps. The only configuration a user must do is to define a directory for the download .","title":"Example toolkit"},{"location":"en/tools/utilitary/#operation","text":"The Example toolkit script will execute four main steps, as illustrated on the Figure bellow: Example Toolkit Operation Flux The Example toolkit sript performs the data download (from the GitHub Release Assets ). Following, it verifies if the downloaded data haven't suffered any changes or if they are corrupted. This is performed using the downloaded BagIts. Then, the tool scrit extract the data to the correct folders. Finally, using the users definitions and directives of where the data was downloaded, it generates a Dagster configuration file , which can be used to start the processing of the downloaded data. Dagster and configuration files If you are interested in learning more on how Dagster is used in this RC and where the configuration file should be used, consult the Section Processing Scripts - Dagster .","title":"Operation"},{"location":"en/tools/utilitary/#use","text":"To use the Example toolkit script , only Python is required with the environment requisites. Example toolkit with Docker To use the Example toolkit with no environment configuration, consult the section Example toolkit environment for more information on how you can use the Docker version of the script. Manual configuration of the environment To manually configure the Python environment and the dependencies required to run the Example toolkit , you can use conda . With this package manager, you can create a new environment with the Example toolkit requirements. To create this environment, use the file environment.yml that can be found in the directory tools/example-toolkit : conda env create -f environment.yml To execute the Example toolkit you must define its configurations, which will be used to deremine the local to write the data and some parameters to auxiliate the script generates the Dagster configuration file. This configuration is done through environment variables, being them: DOWNLOAD_REFERENCE_FILE Environment variable to determine the absolute path of the JSON which defines where the GitHub Assets Release data will be downloaded. Examples of this file can be found in the directory tools/example-toolkit/config . Value example : /compendium/config/example-toolkit.json DOWNLOAD_OUTPUT_DIRECTORY Environment variable that determines the directory where the downloaded data will be stored. The data is organized following the format of the Data directory , required by the processing scripts . Value example : /compendium/data PIPELINE_DIR (Dagster Configuration) Environment variable that determines where the Dagster Configuration file will be saved. Value example : /compendium/config/config.yml RAW_DATA_DIR (Dagster Configuration) Environment variable that determines which directory should be considered as the machine input dir for the Dagster processing. Value example : /compendium/data/raw_data DERIVED_DATA_DIR (Dagster Configuration) Environment variable that determines which directory should be considered as the machine output dir in the Dagster configuration file. Value example : /compendium/data/derived_data Variable definition consistency The configuration variables have a logical dependency that must be followed to avoid problems. To present this dependency let's consider the following example: Suppose that you wish to download the data on a directory /opt/my-data . In this case, you will define the DOWNLOAD_OUTPUT_DIRECTORY as: DOWNLOAD_OUTPUT_DIRECTORY = /opt/my-data Knowing that your data will be organized following the pattern in Data directory , the downloaded data will be stored as follows: /opt/my-data \u251c\u2500\u2500 derived_data \u2514\u2500\u2500 raw_data Considering this organization, if you desire to generate the Dagster file to process data in /opt/my-data , it will be required to define the Dagster environment variables as: # 1. Input dir RAW_DATA_DIR = /opt/my-data/raw_data # 2. Output dir DERIVED_DATA_DIR = /opt/my-data/derived_data After defining each environment variable, the Example toolkit can be executed. To do that, the script available on the diretory tools/example-toolkit/scripts/pipeline.py must be executed. Considering that you are in the root of this RC , the execution of this script can be performed by: 1. Changing directory cd tools/example-toolkit/ 2. Execution python3 scripts/pipeline.py At the end of the execution, the output directories will appear as: Data The directory defined by the variable DOWNLOAD_OUTPUT_DIRECTORY , as mentioned, will follow the Data directory organization, required by this RC processing scripts scripts . The data will be organized as: DOWNLOAD_OUTPUT_DIRECTORY \u251c\u2500\u2500 derived_data \u2514\u2500\u2500 raw_data \u251c\u2500\u2500 landsat8_data \u251c\u2500\u2500 sentinel2_data \u251c\u2500\u2500 scene_id_list \u2514\u2500\u2500 lasrc_auxiliary_data Dagster Configuration The directory defined by the variable PIPELINE_DIR will be populated as a Dagster configuration File named config.yaml . In this file, the following content will be available: config.yaml: Arquivo de configura\u00e7\u00e3o Dagster resources : lasrc_data : config : lasrc_auxiliary_data_dir : { RAW_DATA_DIR } /lasrc_auxiliary_data repository : config : derived_data_dir : { DERIVED_DATA_DIR } landsat8_input_dir : { RAW_DATA_DIR } /landsat8_data sentinel2_input_dir : { RAW_DATA_DIR } /sentinel2_data solids : load_and_standardize_sceneids_input : config : landsat8_sceneid_list : { RAW_DATA_DIR } /scene_id_list/l8-sceneids.txt sentinel2_sceneid_list : { RAW_DATA_DIR } /scene_id_list/s2-sceneids.txt For a complete and functional example of the Example toolkit , consult the section Reproducible research - minimal example .","title":"Use"}]}